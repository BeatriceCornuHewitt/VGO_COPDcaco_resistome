---
title: "VGO-3 COPD case control resistome project - version 4 data"
subtitle: "35 cases, 34 control samples processing from counts, rarefied relatiely, geneLength corrected, 16-S rRNA qPCR corrected"
date: "8th June 2021"
output:
  html_notebook:
    code_folding: none
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
  bookdown::gitbook:
    config:
      theme: cerulean
      toc:
        collapse: none
        scroll_highlight: yes
  prettydoc::html_pretty:
    highlight: github
    theme: cayman
    toc: yes
    toc_depth: 6
    toc_float: yes
  pdf_document:
    df_print: paged
    toc: yes
    toc_depth: 6
  html_document:
    df_print: paged
    code_folding: none
    number_sections: yes
    theme: cerulean
    toc: yes
    toc_depth: 6
    toc_float: yes
site: bookdown::bookdown_site
toc-title: Table of Contents
editor_options: 
  chunk_output_type: inline
---
# Summary
**Resistome composition using ResCap data of oropharyngeal (OP) samples from 35 COPD patients, 34 matched controls, and 3 field blanks**
- 1: Relative 'rarefy': ARG read count data was first rarefied according to the relative correction factors per sample (phyloseq= merged.rar3)
- 2: Gene length correction: Rarefied ARG read count data was then corrected for ARG lengths (now FPK) ((read counts*1000)/ARG length)  (phyloseq= merged.rar3.fpk)
- 3: Bacterial content correction: FPK values were normalised to total bacteria (using qPCR 16-S values for each sample)
  
Since rarefaction of data before or after ARG length correction could make a limited/small (tested with artificial set) difference we decided to do it 100% correct and rarefy before ARG length correction.

# Set up 
```{r setup, include=FALSE}
# settings for controlling the appearance and behavior of code chunks when generating documents using R Markdown
knitr::opts_chunk$set(echo = TRUE)           #include source or not
knitr::opts_chunk$set(warnings = FALSE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(fig.width=14, fig.height=10, fig.align='center')
knitr::opts_knit$set(root.dir = "C:/Users/Cornu003/OneDrive - Universiteit Utrecht/Documents/PhD/Manuscripts/2. Resistome/R_scripts/VGO_COPDcaco_resistome") # set the working directory to the R project file. 
```

## Home Directory based on OS Type
```{r}
getwd()
```
## Parameters and folders 
```{r}
# Phyloseq object consists of 3 parts (i.e. it is an S4 object linking different spreadsheets (S4 objects are particularly useful for handling complex data structures in bioinformatics, statistics, and other data-intensive fields) – in our case 3 (where otu_table = ARGs in each sample, sample_data = the qPCR 16S metadata, tax_table = categorisations of genes into clusters). 

# set random seed set for reproducibility
seed <- 2202
set.seed(seed)

# project
pjbase          <- "VGO-COPD_resistome_v4" 
# input for ps object 
metafile        <- "Input_files//VGOCOPD_metadata_completed_20210429alx_correctFormulaColumnTU.xlsx.tab"
argclusterfile  <- "Input_files//ResfinderClustersFinal_Resfinder20200127_inclGeneLen+pheno_20201206b.tab"
argmappingfile  <- "Input_files//02e_TotalResfinderMapped_v3.tab"
# output
psfile          <- "Output_files//03_VGO-COPD_resistome_v4_phyloseq_object(BCH).RDS"               # filename R Data Serialization (RDS) phyloseq object
psrarfile       <- "Output_files//03_VGO-COPD_resistome_v4_phyloseq_object_rarefied(BCH).RDS"      # filename RDS phyloseq object rarefied
psfpkfile       <- "Output_files//03_VGO-COPD_resistome_v4_phyloseq_object_rarefied_FPK(BCH).RDS"  # filename RDS phyloseq object rarefied and gene length corrected
psfpkmfile    <- "Output_files//03_VGO-COPD_resistome_v4_phyloseq_object_rarefied_FPKM(BCH).RDS" # filename RDS phyloseq object rarefied, gene length corrected and qPCR 16-S corrected 
# folders
outfiles <- "Output_files"
dir.create(outfiles,showWarnings = FALSE)
```
## Packages
```{r}
.libPaths() # location of where packages are downloaded/saved on my pc
# Load required associated libraries
install.packages('Rtools')
library(Rtools) # For organising data in structure suitable for analysis
library(tidyverse) # For organising data in structure suitable for analysis
library(BiocManager) 
library(writexl) 
library(openxlsx)
library(patchwork)
library(phyloseq) # For microbiome analyses
library(vegan) # tools for microbiome analyses 
library(RColorBrewer) # Colour palettes for graphs
library(plotly) # For interactive web graphics 
library(ggplot2) #for graphs
library(cowplot) # An addition to ggplot package - helps produce high quality graphs (extra features)
library(pheatmap) # for heatmaps
library(knitr) # needed for creation of the notebook (?)
library(dplyr)# data manipulation
library(magrittr)# for pipe operators 
library(forcats) # For working with categorical variables
library(foreach) # needed for looping construction
library(graphics) # For creating simple (non ggplot) graphs
library(lattice) # Data visualisation
library(methods)
library(readr)
library(stats)
library(stringr)
library(tibble)
library(tidyr)
library(plyr)
library(utils)
library(eulerr) # for venn diagram 
# library(microbiomeSeq) # For alpha diversity plots/anovas
library(ggpubr) #for adding significance values to boxplots
library(matrixStats)
BiocManager::install(version = "3.16")
BiocManager::install("decontam")
library(decontam) # To check for data contaminants
BiocManager::install("DESeq2")
library(DESeq2) #For DESeq differntial abundance analyses
BiocManager::install("microbiome")
library(microbiome)
library(matrixStats) # needed for DESeq2 package 
library(e1071) # For support vector machine
library(microbiomeutilities) # For heatmap building
library(grid) # For ggplot2 plots for procrustes analysis
library(cooccur) # For cooccurrence network analysis 
library(visNetwork) # For cooccurrence network visualisation
library(corrplot) # For correlation plots 
library(Hmisc) # For correlation matrix creation with p-values included
library(gtools) # For quantcut function (used for exposure proxy splitting)
library(igraph) # For co-occurrence network visualisation
library(DiagrammeR)
library(rgexf) # to build graph.ml files for upload into gephi etc.
library(here)
library(car)
library(olsrr) # for cook's distance plots to look at influential data points in linear regression models
library(doBy)
```
## My environment
```{r}
sessionInfo() 
```
## Read in the raw data
```{r}
# Data were pre-processed from individual excel sheets (metadata and 'otu data') and the refined ARG clustering spreadsheet (ResfinderClustersFinal_Resfinder20200127_inclGeneLen+pheno_20201206b)
getwd()
metadata <- read.csv(metafile, header=TRUE, sep="\t", row.names=1 )
metadata

ARGclusters <- read.csv(argclusterfile, header=TRUE, sep="\t", row.names=1 )
ARGclusters

ARG.long <- read.csv(argmappingfile, header=TRUE, sep="\t")
head(ARG.long)
```
### Make matrix of 'otu_data' 
```{r}
# make matrix with samples in rows and ARGs in columns
ARGcount <- ARG.long %>%
    select(SampleName, ARGname, MappedCount) %>% 
    pivot_wider(names_from=ARGname, values_from=MappedCount, values_fill=0)

dim(ARGcount)
ARGcount
```

# Create PS objects 
- Convert ARG table to "otu"_table having taxa in columns and move SampleNames to rownames first
- Taxonomy needs to be matrix type
- Metadata should be ordered in the same way as the samples in the ps object before merging.

PS object structure:
The class structure in the phyloseq package uses 4 core data classes: (1) OTU abundance table (otu_table), a table of sample data (sample_data); (2) a table of taxonomic descriptors (taxonomyTable); and (3) a phylogenetic tree ("phylo"-class, ape package (NA in this case). 

otu_table:  represents the number and type of sequences observed in each sample. In our case the otu_table is oriented with taxa as columns (taxa_as_rows= FALSE)
sample_data: same structure as R’s data.frame, and effectively stores both categorical and numerical data about each sample. Samples are rows, and variables are columns (consistent with vegan and other packages). 
tax_table: This directly inherits the matrix class, and is oriented such that rows are taxa/OTUs and columns are taxonomic levels (e.g. Phylum).

# Non-normalised PS 
```{r}
ARGotutable <- ARGcount %>% column_to_rownames("SampleName")
merged <- phyloseq( otu_table( ARGotutable, taxa_are_rows=FALSE ), tax_table( as.matrix(ARGclusters) ) )
# somehow the apostrophes get converted from df to matrix. Therefore we import again directly the matrix into the ps object to preserve names!
merged@otu_table@.Data <- as.matrix(ARGotutable)

# align meta data and merge
metadata <- metadata[ match( sample_names(merged), rownames(metadata) ), ]

# final PS object
merged@sam_data <- sample_data(metadata)

# make num to factor
merged@sam_data$Enrich_pool_num <- as.factor(merged@sam_data$Enrich_pool_num)
```
## Save phyloseq
```{r}
saveRDS(merged, psfile)
```
## Save otu table
```{r}
otu <- as.data.frame(otu_table(merged)) %>% rownames_to_column( "SampleID" )
write.csv( otu, paste0( psfile, ".tab" ), row.names=FALSE )
```
## Basic metrics of PS object
```{r}
merged
cat("\n")
summary(sample_data(merged))
```

# Normalised PS 
We need to normalise/correct the relative ratios between samples.
- Correct for relative volume inputs (using relative rarefaction)
- Subsequently we need to correct for ARG gene length. (N.B. We tested in a small artificial example that rarefication before or after ARG length correction could make a limited/small difference (tested with artificial set...see R playground using 10 samples), we decided to do it 100% correct and rarefy before ARG length correction.
- Normalise to bacterial content (16S qPCR data) of each sample.

## Relative rarefy the raw count data
```{r}
# Based on the correction factors calculated from the wet-lab data we determined a correction factor by which each sample should be normalised to individually. We therefore calculated what the target read depth should be per sample based on inputs. Use that per-sample value to rarefy each sample individually.  
# how many samples are there in the phyloseq object?
nsamples <- length( sample_names(merged) )

# determine which number we should rarefy to (from metadata table)
metasum <- data.frame( sample_data(merged) )$RelativeCorrect_readpair_counts

#first do sample 1, then build the rest around it
i=1
m.tmp <- subset_samples( merged, sample_names(merged) == sample_names(merged)[i] )
m.tmp <- rarefy_even_depth( m.tmp, sample.size=metasum[i], trimOTUs=FALSE, rngseed=seed )
mtotal <- m.tmp

#now add remainder samples one by one after rarefy to same m4.final ps object
for( i in 2:nsamples) {
  m.tmp <- subset_samples( merged, sample_names(merged) == sample_names(merged)[i] )
  m.tmp <- rarefy_even_depth( m.tmp, sample.size=metasum[i], trimOTUs=FALSE, rngseed=seed )
  mtotal <- merge_phyloseq( mtotal, m.tmp )
}

merged.rar <- mtotal
```
### Inspect rarefied data
```{r}
merged.rar

rarefy_compare <- data.frame( "OrginalSum"=sample_sums(merged), "RarefiedSum"=sample_sums(merged.rar) )
rarefy_compare
```
## Restore taxa_names
```{r}
# At some point all apostrophes got removed from the taxa names. We need to restore them, and do this here
# Note: the functions subset_taxa and subset_samples remove the apostrophes from the taxa names (prune_ functions do not do this)
# quickly visually check to see that the order of the taxa in the ps obect has not changed (the only difference should be apostrophes). 
taxnames_compare <- data.frame("org"=taxa_names(merged),"altered.rar"=taxa_names(merged.rar))
taxnames_compare

taxa_names(merged.rar) <- taxa_names(merged)
taxnames_compare2 <- data.frame("org"=taxa_names(merged),"altered.rar"=taxa_names(merged.rar))
taxnames_compare2
```
## Cleanup zero taxa and samples
```{r}
# If any zero sum taxa now in the data we should remove them.  
# It is expected that some taxa end up below 1 count. We do not expect any samples to be dropped!
merged.rar

# note! Using the subset_taxa or subset_samples functions removes the apostrophes again from the ps objecect!!! Prune_ does not.
merged.rar2 <- prune_taxa( taxa_sums(merged.rar) > 0, merged.rar )
merged.rar2

merged.rar3 <- prune_samples( sample_sums(merged.rar2) > 0, merged.rar2 )
merged.rar3

# check if taxa names still contain apostrophes
head(taxa_names(merged.rar3))

# save this ps for later use for DA analysis
saveRDS(merged.rar3, file = "Output_files//merged.rar3.rds")
```

# Normalised and ARG gene length corrected PS

We will use the ARG gene - gene length values from the initial long ARGmapping file.

- Make a unique list of ARG and corresponding ARG lengths
- Convert OTU counts matrix to a long format dataframe.
- Correct the count value by dividing corresponding ARG length.
- Convert df long into matrix into phyloseq object again.

## ARG length table
```{r}
# unique list of ARG and ARG length (n=369)
ARGlen <- data.frame("ARGname"=ARG.long$ARGname, "ARGlen"=ARG.long$ARGlength)
ARGlen <- unique.data.frame(ARGlen)
rownames(ARGlen) <- NULL #not sure why dplyr mowns about already set rownames...
ARGlen <- ARGlen %>% column_to_rownames("ARGname")
ARGlen
```
## ARG count to FPK
```{r}
# copy object
merged.rar3.fpk <- merged.rar3
# convert otu counts directly by matrix manipulation in the ps object using ARGlen df.
merged.rar3.fpk@otu_table@.Data <- merged.rar3.fpk@otu_table@.Data * 1000 / ARGlen[ col(merged.rar3.fpk@otu_table@.Data), ] 

#checks
merged.rar3.fpk
head( taxa_names(merged.rar3.fpk) )
head( data.frame( otu_table(merged.rar3.fpk) ) )
```
## Save partially normalised data 
```{r}
saveRDS( merged.rar3, psrarfile )
saveRDS( merged.rar3.fpk, psfpkfile )
```
## Save otu table
```{r}
otu <- as.data.frame( otu_table(merged.rar3.fpk) ) %>% rownames_to_column( "SampleID" )
write.csv( otu, paste0( psfpkfile, ".tab" ), row.names=FALSE )
```
## Basic metrics of normalised/ARG length-corrected PS 
```{r}
merged.rar3.fpk
cat("\n")
summary( sample_data(merged.rar3.fpk) )
```
# Normalised & ARG length-corrected & 16S qPCR count-corrected PS 
```{r}
# create a list of sample ID and associated 16S qPCR count
qPCRcounts <- data.frame("SampleID"= unique(ARG.long$SampleName), "qPCR16S"= metadata$qPCR_16S_ngml)
rownames(qPCRcounts) <- NULL
qPCRcounts
qPCRcounts <- qPCRcounts %>% column_to_rownames("SampleID")
qPCRcounts

saveRDS(qPCRcounts, "Output_files//qPCRcounts")
```
## Correcting for 16S qPCR counts 
```{r}
#copy ps object 
merged.rar3.fpkm <- merged.rar3.fpk

# convert otu fpk values directly by matrix manipulation in the ps object using qPCRcounts data frame 
merged.rar3.fpkm@otu_table@.Data <- merged.rar3.fpkm@otu_table@.Data / qPCRcounts [ 1:73 ,]

#checks
merged.rar3.fpkm
head( taxa_names(merged.rar3.fpkm) )
```
```{r}
# Manual checking of data
view (merged.rar3.fpkm@otu_table@.Data)
view (merged.rar3.fpk@otu_table@.Data)
view (merged.rar3@otu_table@.Data) 
# Manual checks of the otu table data shows me that this correction has been successful as we can see that each value has been divided by the individual ARG lengths
```
## Save fully normalised & corrected data
```{r}
saveRDS(merged.rar3.fpkm, psfpkmfile)
```
## Save otu table
```{r}
otu_fpkm <- as.data.frame( otu_table(merged.rar3.fpkm) ) %>% rownames_to_column( "SampleID" )
write.csv( otu_fpkm, paste0( psfpkmfile, ".tab" ), row.names=FALSE )
```
## Basic metrics
```{r}
merged.rar3.fpkm
cat("\n")
summary( sample_data(merged.rar3.fpkm) )
```
## Remove duplicate sample (13674) from the dataset
```{r}
# There was an error in the choice and labelling of 1 control sample and we ended up with the same control sample being used for 2 case samples. It was mislabelled as '13674' hence was thought to be a different sample to '13764' which was already included in the dataset. Therefore we must remoce '13674' from the dataset. 
Final.ps <- prune_samples(sample_names(merged.rar3.fpkm) != "13674", merged.rar3.fpkm) 
Final.ps
# Final PS object therefore now has 72 samples in total (35 cases, 34 controls, 3 blanks)
```
# Agglomerate at 90% cluster level 
```{r}
# Using the functions created by AB, I will now agglomerate the data to AMRcluster level - i.e. merge ARGs which are in the same ARGcluster (90% identity level)  
# Clustering of ARGs at the 90% identity level is classified as 'ARGCluster' in the tax data of the Final.ps object - agglomerate at this level i.e. create a new ps object with cluster 90 as default. Automatic default is the ARG level but this may not be very accurate therefore best to use cluster90 level (ARGCluster in ps object). 

# print the available taxonomic ranks
colnames(tax_table(Final.ps))
# how many ARGs are there before agglomeration in original ps object? 
Final.ps # 233 taxa (ARGs)

# Tax_glom2 function as created by AB - save this in the global environment for later use
tax_glom2 <- function(physeq, taxrank=rank_names(physeq)[1],
                     NArm=TRUE, bad_empty=c(NA, "", " ", "\t")){
  #### This part is identical to phyloseq's tax_glom
  # Error if tax_table slot is empty
  if( is.null(access(physeq, "tax_table")) ){
    stop("The tax_glom2() function requires that physeq contain a taxonomyTable")
  }
  # Error if bad taxrank
  if( !taxrank[1] %in% rank_names(physeq) ){
    stop("Bad taxrank argument. Must be among the values of rank_names(physeq)")
  }
  # Make a vector from the taxonomic data.
  CN  <- which( rank_names(physeq) %in% taxrank[1] )
  tax <- as(access(physeq, "tax_table"), "matrix")[, CN]
  # if NArm is TRUE, remove the empty, white-space, NA values from
  if( NArm ){
    keep_species <- names(tax)[ !(tax %in% bad_empty) ]
    physeq <- prune_taxa(keep_species, physeq)
  }
  # Concatenate data up to the taxrank column, use this for agglomeration
  tax <- as(access(physeq, "tax_table"), "matrix")[, 1:CN, drop=FALSE]
  tax <- apply(tax, 1, function(i){paste(i, sep=";_;", collapse=";_;")})
  #### **Speedyseq changes start here**
  ## Make the new OTU table
  # Work with taxa as rows
  if (!taxa_are_rows(physeq)) {
    physeq <- phyloseq::t(physeq)
    # Note that we need to flip back to samples as rows at the end
    needs_flip <- TRUE
  } else {
    needs_flip <- FALSE
  }
  # Starting point is a tibble with rows as taxa, to be able to combine taxa
  # with the dplyr::summarize_*() functions
  otu <- otu_table(physeq)
  tb <- otu %>%
    as("matrix") %>%
    tibble::as_tibble(rownames = "OTU")
  # We want to name each new taxon (group of merged OTUs) by its "archetype",
  # the most abundant OTU in the group
  tb <- tb %>%
    tibble::add_column(Tax = tax, Sum = taxa_sums(physeq)) %>%
    dplyr::group_by(Tax)
  # Name new taxa by the most abundant OTU; pick the first OTU in case of
  # ties (to be consistent with phyloseq)
  new_taxa_names <- tb %>%
    dplyr::top_n(1, Sum) %>%
    dplyr::slice(1) %>%
    dplyr::select(Tax, OTU)
  # Sum abundances and rename taxa
  tb0 <- tb %>%
    dplyr::summarize_at(dplyr::vars(sample_names(physeq)), sum) %>%
    dplyr::left_join(new_taxa_names, by = "Tax") %>%
    dplyr::select(OTU, dplyr::everything(), -Tax)
  # Put back into phyloseq form
  mat <- tb0 %>%
    dplyr::select(-OTU) %>%
    as("matrix")
  rownames(mat) <- tb0$OTU
  otu0 <- otu_table(mat, taxa_are_rows = TRUE)
  ## Make the new phyloseq object
  # Replacing the original otu_table with the new, smaller table will
  # automatically prune the taxonomy, tree, and refseq to the smaller set of
  # archetypal otus
  otu_table(physeq) <- otu0
  # "Empty" the taxonomy values to the right of the rank, using
  # NA_character_.
  if (CN < length(rank_names(physeq))) {
    bad_ranks <- seq(CN + 1, length(rank_names(physeq)))
    tax_table(physeq)[, bad_ranks] <- NA_character_
  }
  ## Return.
  if (needs_flip) {
    physeq <- phyloseq::t(physeq)
  }
  return(physeq)
}


# Use function tax_glom2 - 6 is the ARGCluster level in the tax_table of Final.ps object. This creates a new ps object with ARGcluster as default 
ARGcluster.aggl <- tax_glom2(Final.ps, taxrank=rank_names(Final.ps)[6], NArm=TRUE)
taxa_names(ARGcluster.aggl)
# Confused why the 'taxa_names' are not the arg cluster names? AB responded saying: "What you see is perfectly normal. If you checkout the tax_table you will see that the lowest level is cluster. The lower ranks should be NA. That is why you use explaining Ps object names to realise some aggregation happened. The data is underneath still linked on indiv accno level but incomplete. Only one per arg cluster should be present.  If you now directly ask the taxon name it is indeed "wrong" the accession name. Just use the arg cluster name corresponding to it and you should be fine. Most functions suited for Ps object allow to mention the tax rank or the fill rank to use. They handle it for you underneath in the same way.No worries, just check above and if that matches. Just continue and be sure to use the name of the tax rank. 
# Check tax-table of ps object
tax_table(ARGcluster.aggl)

# How many taxa before/after agglomeration?
ntaxa(Final.ps); ntaxa(ARGcluster.aggl)
# 85 ARGClusters vs 233 ARGs 
ARGcluster.aggl@tax_table[,6]
head(ARGcluster.aggl@otu_table)
taxa_names(ARGcluster.aggl)

# Function created by AB to counts the number of NAs in the taxonomy table per taxonomic rank. It returns a dataframe having the NA counts and the percentage NAs of total entries per taxrank.
countTaxrankNAs <- function(  ps, filename="" )
{
  natax   <- colSums(is.na(ps@tax_table))
  tottax  <- natax + colSums(!is.na(ps@tax_table))
  perctax <- round( natax/tottax * 100, digits=2 )
  overall <- data.frame(NtaxaNAs=natax, percNAs=perctax) %>% rownames_to_column("TaxRank")
  cat( paste0("Total of ARGs = ",tottax[1],"\n"))
  if( filename > "") { write_tsv(overall, paste0(pjbase,"_TotalNAtaxaAtTaxonomicLevelsOverall.tab")) }
  return(overall)
}

countTaxrankNAs(ARGcluster.aggl) # 0 NAs in the ARG and ARG cluster columns
```

# Remove blanks from agglomerated PS objects
```{r}
# Indentify blanks and save as a new PS object
blanks <- subset_samples(ARGcluster.aggl, copdcaco == "blanc")
# Convert 'blancs' to a character vector of sample names
blank_names <- as.character(sample_names(blanks))
# Prune samples from 'ARGcluster.aggl'
ARGcluster.aggl.noblanks <- ARGcluster.aggl
ARGcluster.aggl.noblanks <- prune_samples(!sample_names(ARGcluster.aggl.noblanks)%in%blank_names, ARGcluster.aggl.noblanks)
ARGcluster.aggl.noblanks;ARGcluster.aggl # checking to ensure blanks have been removed - yes now there are 69 instead of 72 samples 

# Prune samples from 'AMRclass.aggl'
AMRclass.aggl.noblanks <- AMRclass.aggl
AMRclass.aggl.noblanks <- prune_samples(!sample_names(AMRclass.aggl.noblanks)%in%blank_names, AMRclass.aggl.noblanks)
AMRclass.aggl.noblanks;AMRclass.aggl # checking to ensure blanks have been removed - yes now there are 69 instead of 72 samples 
```
# Basic numbers 
```{r}
# ARG cluster level
# Print the number of samples in the dataset
nsamples(Final.ps)
# Number and names of all original ARGs in the dataset
ntaxa(Final.ps)
taxa_names(Final.ps)
# Number and names of all ARG clusters (90% identity) in the dataset
ntaxa(ARGcluster.aggl)
taxa_names(ARGcluster.aggl) # Names are actually one ARG name chosen for each cluster 

# Number and names of all AMR classes in the dataset
# Firstly agglomerate to the AMR class level
AMRclass.aggl <- tax_glom(Final.ps, taxrank = rank_names(Final.ps)[1])
ntaxa(AMRclass.aggl) # 8 AMR classes

taxa_names(AMRclass.aggl) # again...names are actually one ARG name chosen for each class 
# Names of the actual AMR class categories in the dataset
get_taxa_unique(Final.ps, taxonomic.rank=rank_names(Final.ps)[1], errorIfNULL=TRUE)
```
## Abundance of ARG clusters
```{r}
# Make new ps object with only COPD patients in this
COPD_ps <- subset_samples(ARGcluster.aggl, copdcaco == "1")
# Access the otu_table and tax_table from the COPD_ps phyloseq object
COPD_otu_table <- otu_table(COPD_ps)
COPD_tax_table <- tax_table(COPD_ps)
# Get the row indices of taxa with at least one non-zero value
COPD_non_zero_taxa <- which(rowSums(COPD_otu_table > 0) > 0)
# Extract the taxa names with ARGCluster information
COPD_ARGCluster_names <- as.character(COPD_tax_table[COPD_non_zero_taxa, "ARGCluster"])
# Print the ARGCluster names
print(COPD_ARGCluster_names)

# Controls 
control_ps <- subset_samples(ARGcluster.aggl, copdcaco == "0")
# Access the otu_table and tax_table from the control_ps phyloseq object
control_otu_table <- otu_table(control_ps)
control_tax_table <- tax_table(control_ps)
# Get the row indices of taxa with at least one non-zero value
control_non_zero_taxa <- which(rowSums(control_otu_table > 0) > 0)
# Extract the taxa names with ARGCluster information
control_ARGCluster_names <- as.character(control_tax_table[control_non_zero_taxa, "ARGCluster"])
# Print the ARGCluster names
print(control_ARGCluster_names)

# blanks 
blanc_ps <- subset_samples(ARGcluster.aggl, copdcaco == "blanc")
# Access the otu_table and tax_table from the COPD_ps phyloseq object
blancs_otu_table <- otu_table(blanc_ps)
blancs_tax_table <- tax_table(blanc_ps)
# Get the row indices of taxa with at least one non-zero value
blancs_non_zero_taxa <- which(rowSums(blancs_otu_table > 0) > 0)
# Extract the taxa names with ARGCluster information
blancs_ARGCluster_names <- as.character(blancs_tax_table[blancs_non_zero_taxa, "ARGCluster"])
# Print the ARGCluster names
print(blancs_ARGCluster_names)


# convert matrices to dataframes
df_copd_ARGclusters <- as.data.frame(COPD_ARGCluster_names)
df_control_ARGclusters <- as.data.frame(control_ARGCluster_names)
df_blanc_ARGclusters <- as.data.frame(blancs_ARGCluster_names)

# Export data frames to Excel files
write.xlsx(df_copd_ARGclusters, "Output_files//df_copd_ARGclusters.xlsx", rowNames = FALSE)
write.xlsx(df_control_ARGclusters, "Output_files//df_control_ARGclusters.xlsx", rowNames = FALSE)
write.xlsx(df_blanc_ARGclusters, "Output_files//df_blanc_ARGclusters.xlsx", rowNames = FALSE)
```
## 16S qPCR DNA load 
```{r}
# this ps object is clustered at the 90% identity level and blanks removed
ARGcluster.aggl.noblanks 

# Subset the phyloseq object based on 'copdcaco' metadata column
copd_data <- subset_samples(ARGcluster.aggl.noblanks, copdcaco == 1)
control_data <- subset_samples(ARGcluster.aggl.noblanks, copdcaco == 0)

# Calculate summary statistics for 'qPCR_16S_ngml' for COPD patients and controls
copd_qPCR16S_summary <- summary(copd_data@sam_data$qPCR_16S_ngml)
control_qPCR16S_summary <- summary(control_data@sam_data$qPCR_16S_ngml)

# Check distribution in each group
hist(copd_data@sam_data$qPCR_16S_ngml)
hist(control_data@sam_data$qPCR_16S_ngml)
# Distribution in both groups not normal, therefore will use Wilcoxon rank-sum test

# Perform Wilcoxon rank-sum test
wilcox_test_result <- wilcox.test(
  copd_data@sam_data$qPCR_16S_ngml,
  control_data@sam_data$qPCR_16S_ngml
)

# Extract the p-value
p_value <- wilcox_test_result$p.value

# Create boxplot comparing bacterial load between COPD and control groups
plot_data <- data.frame(
  Group = c(rep("COPD", length(copd_data@sam_data$qPCR_16S_ngml)), rep("Control", length(control_data@sam_data$qPCR_16S_ngml))),
  Bacterial_Load = c(copd_data@sam_data$qPCR_16S_ngml, control_data@sam_data$qPCR_16S_ngml)
)

# Create the boxplot with p-value annotation
Bacterial_load_comparison <- ggplot(plot_data, aes(x = Group, y = Bacterial_Load, fill = Group)) +
  geom_boxplot() +
  labs(
    title = "Bacterial load (qPCR 16S) comparison between COPD case and control groups",
    x = "Group",
    y = "Bacterial load (qPCR 16S (ngml))",
    caption = paste("Wilcoxon Rank-Sum Test p-value: ", format(p_value, digits = 3))
  ) +
  scale_fill_manual(values = c("cornflowerblue", "coral1")) +  # Reversed colors
  theme_minimal()+
  theme(plot.background = element_rect(fill = "white"))

# Save the ggplot to the output_files folder
ggsave(filename = "Output_files/Bacterial_load_comparison.png", plot = Bacterial_load_comparison, width = 8, height = 6, dpi = 300)
```
## Checking FPK values
```{r}
# FPK means not corrected for 16S qPCR values (Fragments Per Kilobase of transcript (FPK))
PS_FPK <- readRDS("Output_files//03_VGO-COPD_resistome_v4_phyloseq_object_rarefied_FPK(BCH).RDS") # this is the PS object which is not corrected for 16S qPCR values

# Remove duplicate sample (13674) from the dataset - there was an error in the choice and labelling of 1 control sample and we ended up with the same control sample being used for 2 case samples. It was mislabelled as '13674' hence was thought to be a different sample to '13764' which was already included in the dataset. Therefore we must remoce '13674' from the dataset. 
PS_FPK <- prune_samples(sample_names(PS_FPK) != "13674", PS_FPK) 
PS_FPK

# Now agglomerate the data to ARG cluster level (as above)
colnames(tax_table(PS_FPK))
# how many ARGs are there before agglomeration in original ps object? 
PS_FPK # 233 taxa (ARGs)

# Use tax_glom2 function (as above) - 6 is the ARGCluster level in the tax_table of Final.ps object. This creates a new ps object with ARGcluster as default 
ARGcluster.aggl.FPK <- tax_glom2(PS_FPK, taxrank=rank_names(PS_FPK)[6], NArm=TRUE)
ARGcluster.aggl.FPK # Now 85 taxa (as before)

# remove blanks
blanks <- c("veldbl16", "veldbl3", "veldbl5")

ARGcluster.aggl.FPK.noblanks <- ARGcluster.aggl.FPK
ARGcluster.aggl.FPK.noblanks <-  prune_samples(!sample_names(ARGcluster.aggl.FPK.noblanks)%in%blanks, ARGcluster.aggl.FPK.noblanks) # Now 69 samples as we want

# Now can compute FPK per individual 
# Subset the data into COPD and control groups
copd_data_FPK <- subset_samples(ARGcluster.aggl.FPK.noblanks, copdcaco == 1)
control_data_FPK <- subset_samples(ARGcluster.aggl.FPK.noblanks, copdcaco == 0)

# Function to calculate total FPKM for each individual
calculate_total_FPK <- function(physeq_data) {
  total_FPK <- rowSums(otu_table(physeq_data))
  return(total_FPK)
}

# Calculate total FPKM for COPD and control groups
copd_total_FPK <- calculate_total_FPK(copd_data_FPK)
control_total_FPK <- calculate_total_FPK(control_data_FPK)

summary(copd_total_FPK)
summary(control_total_FPK)

hist(copd_total_FPK)
hist(control_total_FPK)
# both not normal distibution - should test with wilcoxon rank sum test

# Create a data frame for boxplot visualization
plot_data_FPK <- data.frame(
  Group = factor(c(rep("COPD", length(copd_total_FPK)), rep("Control", length(control_total_FPK)))),
  Total_FPK = c(copd_total_FPK, control_total_FPK)
)

# Calculate summary statistics for both groups
summary_stats <- summary(plot_data$Total_FPK)

# Create boxplot with summary statistics annotation
FPK_comparison <- ggplot(plot_data_FPK, aes(x = Group, y = Total_FPK, fill = Group)) +
  geom_boxplot() +
  labs(
    title = "Total FPK Comparison between COPD and Control Groups",
    x = "Group",
    y = "Total FPK",
    caption = paste("Wilcoxon Rank-Sum Test p-value:",
                    format(wilcox.test(Total_FPK ~ Group, data = plot_data_FPK)$p.value, digits = 3))
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("COPD" = "coral1", "Control" = "cornflowerblue"))+
  theme(plot.background = element_rect(fill = "white"))

# Save the ggplot to the output_files folder
ggsave(filename = "Output_files/FPK_comparison.png", plot = FPK_comparison, width = 8, height = 6, dpi = 300)
```
## Checking FPKM values
```{r}
# Corrected for 16S qPCR (Fragments Per Kilobase of transcript per Million mapped reads (FPKM))
ARGcluster.aggl.noblanks # this is the fully corrected PS object (FPKM)

# Subset the data into COPD and control groups
copd_data <- subset_samples(ARGcluster.aggl.noblanks, copdcaco == 1)
control_data <- subset_samples(ARGcluster.aggl.noblanks, copdcaco == 0)

# Function to calculate total FPKM for each individual
calculate_total_FPKM <- function(physeq_data) {
  total_FPKM <- rowSums(otu_table(physeq_data))
  return(total_FPKM)
}

# Calculate total FPKM for COPD and control groups
copd_total_FPKM <- calculate_total_FPKM(copd_data)
control_total_FPKM <- calculate_total_FPKM(control_data)

summary(copd_total_FPKM)
summary(control_total_FPKM)

hist(copd_total_FPKM)
hist(control_total_FPKM)
# both not normal distibution - should test with wilcoxon rank sum test

# Create a data frame for boxplot visualization
plot_data_FPKM <- data.frame(
  Group = factor(c(rep("COPD", length(copd_total_FPKM)), rep("Control", length(control_total_FPKM)))),
  Total_FPKM = c(copd_total_FPKM, control_total_FPKM)
)

# Calculate summary statistics for both groups
summary_stats <- summary(plot_data_FPKM$Total_FPKM)

# Create boxplot with summary statistics annotation
FPKM_comparison <- ggplot(plot_data_FPKM, aes(x = Group, y = Total_FPKM, fill = Group)) +
  geom_boxplot() +
  labs(
    title = "Total FPKM Comparison between COPD and Control Groups",
    x = "Group",
    y = "Total FPKM",
    caption = paste("Wilcoxon Rank-Sum Test p-value:",
                    format(wilcox.test(Total_FPKM ~ Group, data = plot_data_FPKM)$p.value, digits = 3))
  ) +
  theme_minimal() +
  scale_fill_manual(values = c("COPD" = "coral1", "Control" = "cornflowerblue"))+
  theme(plot.background = element_rect(fill = "white"))

# Save the ggplot to the output_files folder
ggsave(filename = "Output_files/FPKM_comparison.png", plot = FPKM_comparison, width = 8, height = 6, dpi = 300)
```
## ARGs per AMR class
```{r}
# Show available ranks in the dataset
rank_names(Final.ps)

# create a table to show the number of ARG clusters categorised into each AMR class in this data set
table('tax_table'(ARGcluster.aggl)[, "ARG_class"], exclude = NULL)
# This shows us that all ARGs have been assigned to an AMR class (as no NAs present here so no artifacts in the data)
# We have a total of 85 genes which are categorised into 8 AMR classes 
```
## Check for contaminants in the dataset
```{r}
# Using decontam package
Final.ps 

#take a quick look at the library sizes (i.e. the number of reads) in each sample, as a function of whether that sample was a true positive sample or a negative control:
df.libsize <- as.data.frame(sample_data(Final.ps)) # Put sample_data into a ggplot-friendly data.frame
df.libsize$LibrarySize <- sample_sums(Final.ps)
df.libsize <- df.libsize[order(df.libsize$LibrarySize),]
df.libsize$Index <- seq(nrow(df.libsize))
ggplot(data=df.libsize, aes(x=Index, y=LibrarySize, color=copdcaco)) + geom_point()

# Identify Contaminants - Frequency method (as sequences from contaminating ARGs are likely to have a frequency that inversely relates with sample DNA concentration)
contamdf.freq <- isContaminant(Final.ps, method="frequency", conc = "DNA_ngul_clariostar")
head(contamdf.freq)
# This gives us a dataframe with several columns: $p tells us the probability that was used for classifying contaminants, and $contaminant tells us the TRUE/FALSE classification values - TRUE indicating that the statistical evidence that the associated sequence feature is a contaminant exceeds the user-settable threshold. As we did not specify the threshold, the default value of threshold = 0.1 was used, and $contaminant=TRUE if $p < 0.1.
# How many genes are flagged as contaminants? 
table(contamdf.freq$contaminant)
# Just 11 out of 233 ARGs are classified as contaminants
# Figure out which ARGs are flagged as the contaminants
which(contamdf.freq$contaminant) # ARG 11,16,25 etc. are classified as contaminants - not very useful not seeing the gene names though...
# View ARG names of contaminants
taxa_names(Final.ps)[sample(which(contamdf.freq$contaminant),11)]
# Now we look at what a clear non-contaminant (e.g. 1st ARG), and a clear contaminant (e.g 11th ARG) look like:
plot_frequency(Final.ps, taxa_names(Final.ps)[c(1,11)], conc="DNA_ngul_clariostar") + 
  xlab("DNA_ngul_clariostar (input DNA concentration)")
# The dashed black line shows the model of a noncontaminant sequence feature for which frequency is expected to be independent of the input DNA concentration. The red line shows the model of a contaminant sequence feature, for which frequency is expected to be inversely proportional to input DNA concentration, as contaminating DNA will make up a larger fraction of the total DNA in samples with very little total DNA. Gene 11 (aph(3')-la_5_AP004237)  fits the red contaminant model better than ARG 1 (aadD_2_M19465). 
# Inspecting more of the ARGs that were classified as contaminants - to make sure they look like what we expect: 
set.seed(100)
plot_frequency(Final.ps,taxa_names(Final.ps)[sample(which(contamdf.freq$contaminant),6)], conc="DNA_ngul_clariostar") +
    xlab("DNA_ngul_clariostar (input DNA concentration)")

# # Now that these ARGs have been identified as likely contaminants we nee dto remove them from the Final.ps object 
# Final.ps
# Final.ps.decontam.freqbased <- prune_taxa(!contamdf.freq$contaminant, Final.ps)
# Final.ps.decontam.freqbased
# # Our 'decontaminated' ps object now has 222 taxa instead of 233 as the 11 contaminants have been removed 

# Alex suggested not using DNA input as the "concentration" but instead to use the correction factor. In amplicon sequencing like 16S the DNA concentration is the major influencer. In my case I also have this input but also several other steps in the processing that were not equal for each sample. Could you try using the input DNA amounts but now corrected with the correction factor? 
# Frequency method - redone with different 'concentration' - using input DNA with correction factor 
#Create new column with DNA content with correction factor per sample - i.e. DNA_ngul_clariostar * TotalCorrectionMaxed
corrected.DNAinput <- Final.ps@sam_data$DNA_ngul_clariostar *
Final.ps@sam_data$TotalCorrectionMaxed
Final.ps@sam_data$correctedDNAinput <- corrected.DNAinput
contamdf.freq.DNAcorrfactor <- isContaminant(Final.ps, method="frequency", conc = "correctedDNAinput")
head(contamdf.freq.DNAcorrfactor)
table(contamdf.freq.DNAcorrfactor$contaminant)
# 17 out of 233 ARGs are classified as contaminants
which(contamdf.freq.DNAcorrfactor$contaminant) 
which(contamdf.freq$contaminant) 

taxa_names(Final.ps)[sample(which(contamdf.freq.DNAcorrfactor$contaminant))]
# comparing to previous frequency method used:
taxa_names(Final.ps)[sample(which(contamdf.freq$contaminant))]
taxa_names(Final.ps)[c(70,153,219)]
taxa_names(Final.ps)[sample(which(contamdf.freq.corrfactor$contaminant))];taxa_names(Final.ps)[sample(which(contamdf.freq$contaminant))]

# Now we look at what a clear non-contaminant (e.g. 1st ARG), and a clear contaminant (e.g 11th ARG) look like:
plot_frequency(Final.ps, taxa_names(Final.ps)[c(1,16)], conc="correctedDNAinput") + 
  xlab("correctedDNAinput")
# The dashed black line shows the model of a noncontaminant sequence feature for which frequency is expected to be independent of the input DNA concentration. The red line shows the model of a contaminant sequence feature, for which frequency is expected to be inversely proportional to input DNA concentration, as contaminating DNA will make up a larger fraction of the total DNA in samples with very little total DNA. Gene 16  fits the red contaminant model better than ARG 1 (aadD_2_M19465). 
# Inspecting more of the ARGs that were classified as contaminants - to make sure they look like what we expect: 
set.seed(100)
plot_frequency(Final.ps,taxa_names(Final.ps)[sample(which(contamdf.freq$contaminant),6)], conc="correctedDNAinput") +
    xlab("correctedDNAinput")

#Using this as concentration instead of DNA input, I get some differences in ARGS and more ARGs are flagged as contaminants. 

# Prevalence-based contaminant identification - since the likelihood of detecting any given contaminant sequence feature will be higher in the blanks than in true samples we can use this method.
# For each sequence feature, a chi-square statistic on the 2 × 2 presence-absence table in true samples and blanks is computed, and a score statistic P is de- fined as the tail probability of the chi-square distribution at that value. The p value from Fisher’s exact test is used as the score statistic instead if there are too few samples for the chi-square approximation. The score statistic ranges from 0 to 1. Small scores indicate the contaminant model of higher prevalence in blanks is a better fit. 
#  In this method, the prevalence (presence/absence across samples) of each sequence feature in true samples COPD/control is compared to the prevalence in blanks to identify contaminants.
# In my phyloseq object, "copdcaco" is the sample variable that holds the COPD/control/blank status. We’ll summarize that data as a logical variable, with TRUE for blank samples, as that is the form required by isContaminant.
#In the prevalence test there is a special value worth knowing, threshold=0.5, that will identify as contaminants all sequences that are are more prevalent in blanks than in copd/control samples
Final.ps@sam_data$copdcaco
sample_data(Final.ps)$is.neg <- sample_data(Final.ps)$copdcaco == "blanc"
contamdf.prev <- isContaminant(Final.ps, method="prevalence", neg="is.neg")
table(contamdf.prev$contaminant)
# This method finds 4 ARGs as contaminants 
# Figure out which ARGs are flagged as the contaminants
which(contamdf.prev$contaminant) # ARGs  45,95, 153 & 194 are classified as contaminants - not very useful not seeing the gene names though...
# View ARG names of contaminants
taxa_names(Final.ps)[sample(which(contamdf.prev$contaminant))]
# "tet(A)_1_AJ313332"    "blaCARB-5_1_AF135373" "tet(A)_2_X00006"  "tet(C)_2_AY046276"  are identified as contaminants 
# Seems strange that no aminoglycoside ARGs are identified as contaminants - mainly tetracycline resistance genes (and 1 beta-lactam)
# prevalence based contaminant identification has identified fewe number of contaminants than frequency-based method 
# Make phyloseq object of presence-absence in negative controls and true samples
ps.pa <- transform_sample_counts(Final.ps, function(abund) 1*(abund>0))
ps.pa.neg <- prune_samples(sample_data(ps.pa)$copdcaco == "blanc", ps.pa)
ps.pa.pos <- prune_samples(sample_data(ps.pa)$copdcaco == c(1,0), ps.pa)
# Make data.frame of prevalence in positive and negative samples
df.pa <- data.frame(pa.pos=taxa_sums(ps.pa.pos), pa.neg=taxa_sums(ps.pa.neg),
                      contaminant=contamdf.prev$contaminant)
ggplot(data=df.pa, aes(x=pa.neg, y=pa.pos, color=contaminant)) + geom_point() +
  xlab("Prevalence (blanc samples)") + ylab("Prevalence (COPD and control Samples)")
# This graph shows that the ARGs seem to split pretty cleanly into a branch that shows up mostly in positive samples, and another that shows up mostly in negative controls, and the contaminant assignment (at default probability threshold) has done a good job of identifying those mostly in negative controls.

# Now that these ARGs have been identified as likely contaminants we need to remove them from the Final.ps object 
Final.ps
Final.ps.decontam.prevbased <- prune_taxa(!contamdf.prev$contaminant, Final.ps)
Final.ps.decontam.prevbased
# Our 'decontaminated' ps object now has 229 taxa instead of 233 as the 4 contaminants have been removed 

# There is also a combined method which which combines the frequency-based and prevalence-based scores into a composite score - this approach has been shown to provide the most robust classifications when both DNA concentration and negative control data are available.
?isContaminant
# "Combined" method
contamdf.comb <- isContaminant(
  Final.ps,
  conc = "correctedDNAinput",
  neg = "is.neg",
  method = "combined",
  threshold = 0.1,
  detailed = TRUE)

# examine numbers of contaminants & non-contaminants 
table(contamdf.comb$contaminant)
# Figure out which ARGs are flagged as the contaminants
which(contamdf.comb$contaminant) # ARGs "25", "38" ...are classified as contaminants 
# View ARG names of contaminants
taxa_names(Final.ps)[sample(which(contamdf.comb$contaminant))]

# So in summary, number of ARGs flagged as contaminants by each method are: frequency = 17, prev =4, combined =8. 
# Strange that the prevalence and combined methods do not flag any aminoglycoside? 
# Alex says not surprised -see emails fron 15/07/2021: " In my view a real indicator that only in very low to negative samples this contamination got a chance and emerged." Alex suggests:"What we usually do is just flag the potential contaminants. See which relative abundance they present...  If these are for instance borderline to be filtered (very low count or prevalence) a good strategy might be to just raise the filter a bit so they get excluded. Problem solved.Another option is just to let them stay but flagged. Do your analysis and if they come out significant somewhere realise the flag status and discuss what to do. This is only applicable if the abundance is not too high since that will influence all relative abundance..."
# Next steps: make a plot of the abundances of these genes over samples/groups just to get an idea of where these taxa are. Do for both FPKM as well as relative abundances…

# Plot of abundances of these 8 genes over COPD, control and blank samples
# "tet(C)_2_AY046276"     "sul1_2_U12338"         "blaOXA-134_1_HQ122933" "tet(A)_1_AJ313332"     "aac(3)-IId_1_EU022314" "blaOXA-60d_1_AY664506" "blaZ_138_CP003979"     "tet(A)_6_AF534183" were identified as contaminants 
Final.ps@otu_table@.Data$aadD_2_M19465
is.recursive(Final.ps@otu_table@.Data)
is.atomic(Final.ps@otu_table@.Data)
par(mfrow = c(4, 4))
# plot for blaOXA-134_1_HQ122933 (gene #25)
boxplot(Final.ps@otu_table@.Data[,25]~Final.ps@sam_data$copdcaco)
# aac(3)-IId_1_EU022314
boxplot(Final.ps@otu_table@.Data[,38]~Final.ps@sam_data$copdcaco)
# blaOXA-60d_1_AY664506
boxplot(Final.ps@otu_table@.Data[,45]~Final.ps@sam_data$copdcaco)
#tet(A)_1_AJ313332
boxplot(Final.ps@otu_table@.Data[,95]~Final.ps@sam_data$copdcaco)
#sul1_2_U12338
boxplot(Final.ps@otu_table@.Data[,123]~Final.ps@sam_data$copdcaco)
# tet(C)_2_AY046276
boxplot(Final.ps@otu_table@.Data[,135]~Final.ps@sam_data$copdcaco)
# tet(A)_6_AF534183
boxplot(Final.ps@otu_table@.Data[,146]~Final.ps@sam_data$copdcaco)
#blaZ_138_CP003979
boxplot(Final.ps@otu_table@.Data[,171]~Final.ps@sam_data$copdcaco)

```

## COPD vs control AMR class comparison
### Abundance of the AMR classes across the COPD, control and blank samples
```{r}
# INITIAL GRAPHS (see next chunk for ggplot graphs (more successful!))
# create a bar chart with % of samples on the y-axis, key (COPD, control, blank), AMR class on x axis (see Carr et al. 2020)
# Create dataframe with %s of COPD/control/blank samples cont
AMRclasstable <- as(otu_table(AMRclass.aggl), "matrix")
# transform into a data frame
AMRclass.dataframe <-as.data.frame(AMRclasstable)
AMRclass.dataframe
colnames(AMRclass.dataframe)
rownames(AMRclass.dataframe)

#Not sure why the column names are (specific) ARG names - not sure what has gone wrong here - I will rename the column names to the AMR classes but not convinced that my code is correct - I hope I have grouped per AMR class...
colnames(AMRclass.dataframe) <- c("Aminoglycoside", "Beta-Lactam", "Macrolide", "Tetracycline", "Phenicol", "Sulphonamide", "Trimethoprim", "Fusidicacid")

# Check the renaming has been performed correctly
AMRclass.dataframe

# create new dataframe of AMR class including copd/control status of patients
AMRclass.dataframe.copdcaco <- AMRclass.dataframe
AMRclass.dataframe.copdcaco$COPDcontrol<- Final.ps@sam_data$copdcaco
AMRclass.dataframe.copdcaco

# create data frames of AMR classes separately for copd,control and blank samples
AMRclass.dataframe.copdonly <- subset(AMRclass.dataframe.copdcaco,COPDcontrol == 1)
AMRclass.dataframe.controlonly <- subset(AMRclass.dataframe.copdcaco,COPDcontrol == 0)
AMRclass.dataframe.blankonly <- subset(AMRclass.dataframe.copdcaco,COPDcontrol == "blanc")

# Create bar charts of Aminoglycoside prevalence per group 
barplot(AMRclass.dataframe.copdonly$Aminoglycoside)
barplot(AMRclass.dataframe.controlonly$Aminoglycoside)
barplot(AMRclass.dataframe.blankonly$Aminoglycoside)

# Using library(lattice) create a combined bar plot 
library(lattice)
barchart(AMRclass.dataframe.copdcaco$Aminoglycoside ~ COPDcontrol, data=AMRclass.dataframe.copdcaco, main ="Aminoglycoside class ARG counts per study group",
  xlab="Group", legend = rownames(AMRclass.dataframe.copdcaco$COPDcontrol), beside=TRUE)

# ALternative way to create bar plot
# Calculate the sums of aminoglycoside per group
Aminoglycoside.sum.COPD <- sum(AMRclass.dataframe.copdonly$Aminoglycoside)
Aminoglycoside.sum.COPD
Aminoglycoside.sum.control <- sum(AMRclass.dataframe.controlonly$Aminoglycoside)
Aminoglycoside.sum.control
Aminoglycoside.sum.blank<- sum(AMRclass.dataframe.blankonly$Aminoglycoside)
Aminoglycoside.sum.blank 

Aminoglycoside.df.all <- as.data.frame(c(Aminoglycoside.sum.COPD,Aminoglycoside.sum.control, Aminoglycoside.sum.blank), "matrix")

barplot(Aminoglycoside.df.all$`c(Aminoglycoside.sum.COPD, Aminoglycoside.sum.control, Aminoglycoside.sum.blank)`, main="Aminoglycoside class ARG counts per study group",
  xlab="AMR class", col=c("red","blue", "green"),
  legend = rownames(AMRclass.dataframe.copdcaco$COPDcontrol), beside=TRUE)

# create box plots of Aminoglycoside prevalence per group 
library(lattice)
bwplot(AMRclass.dataframe.copdcaco$Aminoglycoside~ COPDcontrol, data=AMRclass.dataframe.copdcaco, 
       xlab="Group",
       main="Boxplot of Aminoglycoside prevalence in each group")  
       par.settings = list(box.rectangle = list(col= c("red", "blue", "green")))

# from these graphs, we see that although the bar graph shows a high sum of aminoglysides across the copd and control participants, we can see from the boxplot that this is simply due to a couple of outliers which skew the data. Highlights how a bar chart of cumulative abundance can be misleading...
# to illustrate this, below I have calculated the medians for each of the groups:
median(AMRclass.dataframe.copdonly$Aminoglycoside)
median(AMRclass.dataframe.controlonly$Aminoglycoside)
median(AMRclass.dataframe.blankonly$Aminoglycoside)

# Repeat for the other AMR classes - there must be a way to do this for all at the smae time?
# Beta-Lactam
# Create bar charts of Beta-lactam prevalence per group 

barplot(AMRclass.dataframe.copdonly$`Beta-Lactam`)
barplot(AMRclass.dataframe.controlonly$`Beta-Lactam`)
barplot(AMRclass.dataframe.blankonly$`Beta-Lactam`)

# Using library(lattice) create a combined bar plot 
library(lattice)
barchart(AMRclass.dataframe.copdcaco$`Beta-Lactam` ~ COPDcontrol, data=AMRclass.dataframe.copdcaco, main ="Beta-lactam class ARG counts per study group",
  xlab="Group", legend = rownames(AMRclass.dataframe.copdcaco$COPDcontrol), beside=TRUE)

# create box plots of Aminoglycoside prevalence per group 
library(lattice)
bwplot(AMRclass.dataframe.copdcaco$`Beta-Lactam`~ COPDcontrol, data=AMRclass.dataframe.copdcaco,
       xlab="Group",
       main="Boxplot of Beta-lactam prevalence in each group")  
       par.settings = list(box.rectangle = list(col= c("red", "blue", "green")))

# from these graphs, we see that although the boxplots for each group appear more normally distributed among the 3 groups 

```

```{r}
# Firstly I need to convert the OTU and metadata tables so that sample ID is in the columns
# OTU table first:
otu_tab_to_df <- function(ps) {
otu_tab <- otu_table(ps)
df <- otu_tab %>%
t %>%
data.frame() %>%
rownames_to_column("sample_id")
return(df)
}
# metadata table next:
meta_to_df <-  function(ps) {
meta <- sample_data(ps)
df <- meta %>%
data.frame() %>%
rownames_to_column("sample_id")
return(df)
}
colnames(AMRclass.aggl@sam_data)
meta_df <- AMRclass.aggl %>% meta_to_df() %>% dplyr::select(sample_id,copdcaco)
colnames(meta_df)
# Rename 'sample_id' in the meta_df dataframe to Sample_ID to match the AMRclass.aggl@sam_data
meta_df <- rename(meta_df,SampleID = sample_id)

AMRclass.dataframe.n <- otu_tab_to_df(AMRclass.aggl)
head(AMRclass.dataframe.n)
head(AMRclass.dataframe)
AMRclass.dataframe.n <- dplyr::rename(AMRclass.dataframe.n,AMR_class=sample_id)
AMRclass.dataframe$SampleID <- row.names(AMRclass.dataframe)
# change to long format
long_AMRclass.dataframe <- pivot_longer(data = AMRclass.dataframe,cols = c(1:8),names_to = "AMR_class",values_to = "Abundance")
head(long_AMRclass.dataframe)
head(AMRclass.dataframe)
long_AMRclass.dataframe <- left_join(long_AMRclass.dataframe,meta_df,"SampleID")
head(long_AMRclass.dataframe)

long_AMRclass.dataframe 

# Create some boxplots based on AMR class
AMRclass.boxplot <- ggplot(data=long_AMRclass.dataframe, aes(x =AMR_class, y= Abundance, fill= copdcaco))+ geom_boxplot() + scale_fill_manual(breaks = c("0", "1", "blanc"), values=c("blue", "red", "green")) + ggtitle("Boxplots of AMR abundance per \nAMR class in each group") + theme (title =element_text(size=30), axis.text.x = element_text(size=15, angle = 45), axis.text.y = element_text(size=20), legend.title= element_text(size=20), legend.text = element_text(size=20))
AMRclass.boxplot

# Log transform the abundance data using scale_y_log10() function in ggplot
AMRclass.boxplot.logtrans <- ggplot(data=long_AMRclass.dataframe,  aes(x =AMR_class, y= Abundance, fill= copdcaco))+ geom_boxplot() + scale_y_log10() + scale_fill_manual(breaks = c("0", "1", "blanc"), values=c("blue", "red", "green")) + ggtitle("Boxplots of log transformed AMR abundances \nper AMR class in each group")+ theme (title =element_text(size=30), axis.text.x = element_text(size=15, angle = 45), axis.text.y = element_text(size=20), legend.title= element_text(size=20), legend.text = element_text(size=20))
AMRclass.boxplot.logtrans

# Add 1 to all values (to get rid of the zeros from the data) and then log transform the abundance data using scale_y_log10() function in ggplot
AMRclass.dataframe_Add1 <- AMRclass.dataframe[,1:8] + 1
AMRclass.boxplot.logtrans.add1 <- ggplot(data=long_AMRclass.dataframe,  aes(x =AMR_class, y= Abundance, fill= copdcaco))+ geom_boxplot() + scale_y_log10() + scale_fill_manual(breaks = c("0", "1", "blanc"), values=c("blue", "red", "green")) + ggtitle("Boxplots of plus 1, log transformed AMR abundances per AMR class in each group")
AMRclass.boxplot.logtrans.add1
# this gives us exactly the same plot as the regular log transform on it's own....


# split out the graph per AMR class 
AMRclass.boxplot.split.col <- ggplot(data=long_AMRclass.dataframe, aes(x =AMR_class, y= Abundance, fill= copdcaco)) + 
  scale_fill_manual(breaks = c("0", "1", "blanc"), values=c("blue", "red", "green")) +
  scale_y_log10() +
  geom_boxplot() +
  geom_jitter() +
  facet_wrap(~AMR_class, scales="free")+ # + theme_cowplot() can't get this to work
AMRclass.boxplot.split.col
```
## Core resistome 
### Based on ARG level (pre-clustering)
```{r}
# Create Venn diagram of core resistome
# Used to visually determine the set of AMR genes consistently found within each sample.
# AMR genes in at least 95% of samples from each group will be considered part of the core resistome
# (see tutorial followed here: https://microbiome.github.io/tutorials/core_venn.html)
#install.packages("eulerr") # If not installed
# See how many in each category
table(meta(Final.ps)$copdcaco)

# convert to relative abundances
Final.ps.relative <- microbiome::transform(Final.ps, "compositional")

# My aim is to identify shared core resistome between the COPD/control categories
# Firstly I will rename the codes "1", "0" and "blanc" as COPD, control and blank respectively so that the Venn diagram is more explanatory - directly manipulate the PS matrix
Final.ps.relative@sam_data$copdcaco <- recode(Final.ps.relative@sam_data$copdcaco,"1" = "COPD", "0"= "control", "blanc"= "blank")

# Determine what the lowest detected value of an ARG in the data was (non-zero)- helps us to determine the required detection threshold
min(Final.ps.relative@otu_table@.Data[Final.ps.relative@otu_table@.Data>0])

# Write a for loop to go through each of the disease states (COPD or control) and combine identified core taxa into a list. Detection threshold 
list_core <- c() # an empty object to store information

for (n in Final.ps.relative@sam_data$copdcaco){ # for each variable n in copdcaco
    Final.ps.subset <- subset_samples(Final.ps.relative, Final.ps.relative@sam_data$copdcaco == n) # Choose sample from copdcaco by n
    core_m <- core_members(Final.ps.subset , # Final.ps.subset is the ps object selected with only samples from the copdcaco group of interest
                           detection = 1.478136e-06, # i.e. I am setting this detection threshold as this is the lowest abundance value in the dataset 
                           prevalence = 0.90) # i.e. the ARG should be present in 90% of the samples in each group
    print(paste0("Number of core taxa in ", n, " : ", length(core_m))) # print core taxa identified in each copdcaco
    list_core[[n]] <- core_m # add the core taxa to a list for each group.
}

# Show the list of actual ARGs that comprise the core resistome - i.e. the genes which appear in
print(list_core)

# Specify colors and plot venn diagram of core resistome
# supplying colors in the order they appear in list_core
library(gplots)
mycols2 <- c("COPD"="red", "Control"="blue", blank="green")
plot(venn(list_core),
     fills = mycols2)

```

```{r}
# change the prevalence threshold to 95% 
# Write a for loop to go through each of the disease states (COPD or control) and combine identified core taxa into a list. Detection threshold 
list_core <- c() # an empty object to store information

for (n in Final.ps.relative@sam_data$copdcaco){ # for each variable n in copdcaco
    Final.ps.subset <- subset_samples(Final.ps.relative, Final.ps.relative@sam_data$copdcaco == n) # Choose sample from copdcaco by n
    core_m <- core_members(Final.ps.subset , # Final.ps.subset is the ps object selected with only samples from the copdcaco group of interest
                           detection = 1.478136e-06, # i.e. I am setting this detection threshold as this is the lowest abundance value in the dataset 
                           prevalence = 0.95) # i.e. the ARG should be present in 90% of the samples in each group
    print(paste0("Number of core taxa in ", n, " : ", length(core_m))) # print core taxa identified in each copdcaco
    list_core[[n]] <- core_m # add the core taxa to a list for each group.
}

# Show the list of actual ARGs that comprise the core resistome - i.e. the genes which appear in
print(list_core)


# Specify colors and plot venn diagram of core resistome
# supplying colors in the order they appear in list_core
library(gplots)
mycols2 <- c("COPD"="red", "Control"="blue", blank="green")
plot(venn(list_core),
     fills = mycols2)


```

```{r}
#IGNORE this chunk for now - I have done this in excel instead but would like to find a way to do here in R 
library(VennDiagram)
calculate.overlap(list_core)
list_core$COPD
mycols2 <- c("COPD"="red", "Control"="blue", "blank"="green")
venn.diagram(list_core, 
             filename = NULL
             fill = mycols2
             col = transparent
             main = "Venn diagram of core resistome",
             main.cex = 20)
venn(list_core,
     fills = mycols2,
     )
venn(list_core)

venn(list_core, 
     ellipse = TRUE,
     snames = c("COPD","control", "blank")
     zcolor = c("Red","Blue", "Green"),
     opacity = 0.3, 
     plotsize = 15, 
     ilcs = 0.6, 
     sncs = 0.85,
     borders = TRUE, 
     box = TRUE, 
     par = TRUE, 
     ggplot = FALSE)
taxa_names(Final.ps.relative)[1:5]

draw.triple.venn(list_core,
                 area1 = 10,
                 area2 = 
                 n12 = 1,
                 n13 = 0.5,
                 n123 = 0.25,
                 category = c("COPD", "Control", "Blank"),
                 fill = c("red","blue","green"))

# New try - to add gene names to venn diagram           
data(Final.ps.relative, meta)
# core OTUs
print(list_core)
list_core$blank
# taxa that core OTUs assigned to
core.Crop1 <- core$ITS1$Crop1$taxa
core.Crop2 <- core$ITS1$Crop2$taxa
# venn plot
vectors <- list(list_core$COPD, list_core$control)
group.venn(vectors=vectors, 
           label=TRUE, 
           lab.cex=0.7)
## Not run: 
group.venn(vectors=vectors, label=FALSE, cat.pos=c(330, 150),
           lab.cex=0.7, cex=3)


lapply(list_core)
venn.diagram(list_core, filename =NULL, 
  names = c("COPD", "Control", "Blank"), 
  cex = 1, counts.col = "red")

)

# Present the names of the ARGs making up the 'core resistome' of all samples 
# Compare the lists of ARGs belonging to the core resistomes of COPD vs controls 
# Here I tried to get the names of the core taxa (exceeding prevalence of 50% and detection threshold of 0)
view(phyloseq@sam_data@.Data)
phyloseq@sam_data$var <- as.factor(phyloseq@sam_data$var) 

core.taxa <- core_members(Final.ps, detection = 1/100, prevalence = 50/100, include.lowest = FALSE)
core.taxa

# Issues with this - need to look into...
```

```{r}
#Instead of selecting the lowest value of abundance as the detection threshold I will examine whether there is a more appropriate one. 

hist(Final.ps@otu_table@.Data, breaks = 200)
```
### Based on 90% identity clusters
```{r}
# Create Venn diagram of core resistome
# Used to visually determine the set of AMR genes consistently found within each sample.
# AMR genes in at least 95% of samples from each group will be considered part of the core resistome
# (see tutorial followed here: https://microbiome.github.io/tutorials/core_venn.html)
#install.packages("eulerr") # If not installed
# See how many in each category
table(meta(ARGcluster.aggl)$copdcaco)

# convert to relative abundances
ARGcluster.aggl.relative <- microbiome::transform(ARGcluster.aggl, "compositional") 

# My aim is to identify shared core resistome between the COPD/control categories
# Firstly I will rename the codes "1", "0" and "blanc" as COPD, control and blank respectively so that the Venn diagram is more explanatory - directly manipulate the PS matrix
ARGcluster.aggl.relative@sam_data$copdcaco <- recode(ARGcluster.aggl.relative@sam_data$copdcaco,"1" = "COPD", "0"= "control", "blanc"= "blank")

# Determine what the lowest detected value of an ARG in the data was (non-zero)- helps us to determine the required detection threshold
min(ARGcluster.aggl.relative@otu_table@.Data[ARGcluster.aggl.relative@otu_table@.Data>0])

# Write a for loop to go through each of the disease states (COPD or control) and combine identified core taxa into a list. Detection threshold 
list_core <- c() # an empty object to store information

for (n in ARGcluster.aggl.relative@sam_data$copdcaco){ # for each variable n in copdcaco
    ARGcluster.aggl.subset <- subset_samples(ARGcluster.aggl.relative, ARGcluster.aggl.relative@sam_data$copdcaco == n) # Choose sample from copdcaco by n
    core_m <- core_members(ARGcluster.aggl.subset , # Final.ps.subset is the ps object selected with only samples from the copdcaco group of interest
                           detection = 1.478136e-06, # i.e. I am setting this detection threshold as this is the lowest abundance value in the dataset 
                           prevalence = 0.90) # i.e. the ARG should be present in 90% of the samples in each group
    print(paste0("Number of core taxa in ", n, " : ", length(core_m))) # print core taxa identified in each copdcaco
    list_core[[n]] <- core_m # add the core taxa to a list for each group.
}

# Show the list of actual ARGs that comprise the core resistome - i.e. the genes which appear in
print(list_core)

# Specify colors and plot venn diagram of core resistome
# supplying colors in the order they appear in list_core
library(gplots)
mycols2 <- c("COPD"="red", "Control"="blue", blank="green")
plot(venn(list_core),
     fills = mycols2)

# Determining the core resistome at the 90% cluster level reveals 8 core ARG clusters (all of which are shared between COPD and control groups - there are no unique ARG clusters for COPD of control groups )
# As the ARG clusters are named by the ARG name instead of the actual cluster name I had to look up which ARG cluster name it referred to

```

## Top 10 most abundant ARGs (pre-clustering)
```{r}
# Determine the top 10 most abundant ARGs across all the samples
top10.ARGs <- sort(taxa_sums(Final.ps), TRUE)[1:10]
top10.ARGs
names(top10.ARGs)

# Create a new ps object which holds only the 10 most abundant ARGs
Final.ps.top10 <- prune_taxa(names(top10.ARGs), Final.ps)
```
## Top 10 most abundant ARG clusters
```{r}
# Function from AB to calculate top 10 taxa at different levels of taxonomic identification e.g. at ARG level, ARG cluster level, AMR class level etc. 
toptaxa <- function( ps, rank="ARGCluster", top=10, samples=NA, NArm=FALSE ) {
  if( ! is.na(samples[1]) ) {
    pstemp = prune_samples( samples, ps)
  } else {
    pstemp = ps
  }
  pstemp = tax_glom2( pstemp, taxrank = rank, NArm=NArm )
  pstemp = transform_sample_counts( pstemp, function(x) 100*x/sum(x) )

  if( ntaxa(pstemp) < top ) {
    top = ntaxa(pstemp)
    warning("Number of taxa in object is less then requested top list! Reduced to ", top)
  }

  toplist = names( sort( taxa_sums(pstemp), decreasing = TRUE ) )[1:top]
  return( toplist )
}

top10.ARGclusters.overall<- toptaxa (ARGcluster.aggl, rank="ARGCluster", top=10, samples=NA, NArm=FALSE )
top10.ARGclusters.overall

top10.ARGclusters.overall.noblanks<- toptaxa (ARGcluster.aggl.noblanks, rank="ARGCluster", top=10, samples=NA, NArm=FALSE )
top10.ARGclusters.overall.noblanks

# Top 10 in copd samples 
top10.ARGclusters.COPD<- toptaxa (COPD_ps, rank="ARGCluster", top=10, samples=NA, NArm=FALSE )
top10.ARGclusters.COPD

# Top 10 in control samples 
top10.ARGclusters.control<- toptaxa (control_ps, rank="ARGCluster", top=10, samples=NA, NArm=FALSE )
top10.ARGclusters.control


# Present as actual ARG cluster names 
tt <- data.frame( tax_table( ARGcluster.aggl ) )
# extract the names of the ARGCluster column where the row names of tt match the taxa present in the top10.ARGclusters.noblanks, top10.ARGclusters.COPD and top10.ARGclusters.control

tt$ARGCluster[ row.names(tt) %in% top10.ARGclusters.noblanks] 
tt$ARGCluster[ row.names(tt) %in% top10.ARGclusters.COPD]
tt$ARGCluster[ row.names(tt) %in% top10.ARGclusters.control]


# Create a new ps object which holds only the 10 most abundant ARGs (for all COPD and control samples)
Final.ps.top10ARGClusters <- prune_taxa(names(top10.ARGclusters.noblanks), AMRcluster90.aggl)

#BARPLOT
# Make a barplot like the AMR class level one done above, so that for COPD case and control groups, I present the relative abundance of each of the top 10 ARG classes
bar.graphs2 <- function(physeq, var1 = "ungrouped", level, displayx = 10, filetype = ".jpg") {
  if (level == "Species") {
    print("Error this level is too fine for amplicon sequencing")
  } else if (var1 == "ungrouped") {
    physeqxu <- tax_glom(physeq, level)
    physeqxu <- transform_sample_counts(physeqxu, function(x) 100 * x/sum(x))
    physeqxu <- prune_taxa(names(sort(taxa_sums(physeqxu), TRUE))[1:displayx], physeqxu)
    barchartxu <- plot_bar(physeqxu, fill = level) +
      labs(x = "COPD or control samples",
           y = "% of top 10 ARG clusters present",
           title = paste("Top", displayx, level, "ungrouped")) +
      scale_fill_brewer(palette = "Spectral")
    return(barchartxu)
  } else {
    physeqm <- merge_samples(physeq, var1)
    sample_data(physeqm)[[var1]] <- levels(sample_data(physeqm)[[var1]])
    physeqg <- tax_glom(physeqm, level)
    physeqg <- transform_sample_counts(physeqg, function(x) 100 * x/sum(x))
    physeqg <- prune_taxa(names(sort(taxa_sums(physeqg), TRUE))[1:displayx], physeqg)
    grouped_bar <- plot_bar(physeqg, fill = level) +
      labs(x = "COPD or control samples",
           y = "% of top 10 ARG clusters present",
           title = paste("Top", displayx, level, sep = " ")) +
      scale_fill_brewer(palette = "Spectral")
    return(grouped_bar)
  }
}

# Subset the phyloseq object to include only COPD and control samples
ARGcluster.aggl_subset <- subset_samples(AMRcluster90.aggl, copdcaco %in% c("1", "0"))

# Create a new phyloseq object with only the top 10 ARG clusters
ARGcluster.top10 <- prune_taxa(names(sort(taxa_sums(ARGcluster.aggl_subset), decreasing = TRUE))[1:10], ARGcluster.aggl_subset)

# Generate the stacked barplot at the top 10 ARG cluster level
ARGcluster.barchart <- bar.graphs2(ARGcluster.top10, "copdcaco", "ARGCluster") +
  ggtitle("Relative Abundance of Top 10 ARG Clusters in COPD and Control Groups") +
  labs(fill = "ARG Cluster") +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15),
        axis.text.x = element_text(angle = 0, hjust = 0.5),  # Align labels to the center of tick points
        title = element_text(size = 17),
        legend.title = element_text(size = 15),
        legend.text = element_text(size = 15),
        legend.position = "right") +
  scale_x_discrete(labels = c("COPD" = "COPD", "control" = "Control"))

ARGcluster.barchart

ggsave("ARGcluster.barchart.png",width = 7, height = 5,dpi = 300)


```

# Alpha diversity

**
- Calculate several alpha diversity indices for COPD and control participants, including: Observed, Chao1, Shannon’s, Simpson’s, Evenness
- Present COPD/control as side-by-side boxplots 
- The abundance data contains decimals as it has been corrected per gene length, therefore none are whole numbers. Observing the data I see that there are a maximum of 7 decimals in the dataset but alpha diversity calculations require whole numbers...
- I will multiply by 1,000,000,000 in order to remove all these decimals to do the alpha diversity calculations. 
**

```{r}
Final.ps # this is the PS object which has been corrected but not clustered at the 90% identity level
Final.ps.multiplybillion <- Final.ps
Final.ps.multiplybillion@otu_table@.Data <- (Final.ps.multiplybillion@otu_table@.Data)*1000000000

# Rename sample data codes
Final.ps.multiplybillion@sam_data$copdcaco <- revalue(Final.ps.multiplybillion@sam_data$copdcaco, c('1' ='COPD', '0' = 'control', 'blanc'= 'blank'))

# Plot alpha diversity measures
plot_richness(Final.ps.multiplybillion, measures=c("Shannon", "Simpson","Observed"))

# Create a scatterplot with color
mycols3 <- c("COPD"="red", "control"="blue", "blank"="green")
Alphadiversity.scatter.colour <- plot_richness(Final.ps.multiplybillion, x="copdcaco", measures=c("Shannon", "Simpson","Observed"), color="copdcaco")
Alphadiversity.scatter.colour + scale_color_manual(values = mycols3, aesthetics = c("color", "fill"))

# Calculate alpha diversity tables
Shannon.table <- microbiome::alpha(Final.ps.multiplybillion, (index = "Shannon"), zeroes = TRUE)
SimpsonEvenness.table <- microbiome::alpha(Final.ps.multiplybillion, (index = "Simpson"), zeroes = TRUE)
Observed.table <-  microbiome::alpha(Final.ps.multiplybillion, (index = "Observed"), zeroes = TRUE)

# Add diversity tables to the metadata
sample_data(Final.ps.multiplybillion)$shannon <- Shannon.table$diversity_shannon
sample_data(Final.ps.multiplybillion)$SimpsonEvenness <- SimpsonEvenness.table$evenness_simpson
sample_data(Final.ps.multiplybillion)$Observed <- Observed.table$observed

# Create histograms of diversity scores
par(mfrow = c(2, 2))
hist(sample_data(Final.ps.multiplybillion)$shannon, main="Shannon diversity", xlab="", breaks=15)
hist(sample_data(Final.ps.multiplybillion)$SimpsonEvenness, main="Simpson evenness diversity", xlab="", breaks=15)
hist(sample_data(Final.ps.multiplybillion)$Observed, main="Observed diversity", xlab="", breaks=15)

# Check normality of the distributions
shapiro.test(sample_data(Final.ps.multiplybillion)$shannon)
shapiro.test(sample_data(Final.ps.multiplybillion)$SimpsonEvenness)
shapiro.test(sample_data(Final.ps.multiplybillion)$Observed)

# Shannon and Simpson are not normally distributed, Observed appears normally distributed

# Create boxplots for COPD/control/blanks
metadata.final.ps.billion <- sample_data(Final.ps.multiplybillion)
# Make the 4 boxplots
par(mfrow = c(2, 2))
ggplot(metadata.final.ps.billion, aes(x = copdcaco, y = shannon )) + geom_boxplot(fill= c("blue","red", "green"))
ggplot(metadata.final.ps.billion, aes(x = copdcaco, y = Observed)) + geom_boxplot(fill= c("blue","red", "green"))
ggplot(metadata.final.ps.billion, aes(x = copdcaco, y = SimpsonEvenness))+ geom_boxplot(fill= c("blue","red", "green"))

# BLANKS REMOVED FOR ANALYSIS
# Now I should exclude the blanks and perform the analyses again - blanks will affect any statistical tests 
# Firstly I need to get rid of the blanks from the ps object 
blancs <- c("veldbl16", "veldbl3", "veldbl5")
head(sample.names(Final.ps.multiplybillion))
Final.ps.multiplybillion.noblanks <-  prune_samples(!sample_names(Final.ps.multiplybillion)%in%blancs, Final.ps.multiplybillion)

# Plot alpha diversity measures
plot_richness(Final.ps.multiplybillion.noblanks, x="copdcaco", measures=c("Shannon", "Chao1", "Observed", "Simpson", "Evenness"))

# Add colour to the scatterplot
mycols4 <- c("COPD"="red", "control"="blue")
Alphadiversity.scatter.noblanks.colour <- plot_richness(Final.ps.multiplybillion.noblanks, x="copdcaco", measures=c("Observed", "Chao1", "Shannon", "Simpson"), color="copdcaco")
Alphadiversity.scatter.noblanks.colour + scale_color_manual(values = mycols4, aesthetics = c("color", "fill"))

# Create histograms of diversity scores
par(mfrow = c(2, 2))
hist(sample_data(Final.ps.multiplybillion.noblanks)$shannon, main="Shannon diversity", xlab="", breaks=15)
hist(sample_data(Final.ps.multiplybillion.noblanks)$SimpsonEvenness, main="Simpson evenness diversity", xlab="", breaks=15)
hist(sample_data(Final.ps.multiplybillion.noblanks)$Observed, main="Observed diversity", xlab="", breaks=15)

# check normality of these distributions across all the samples
shapiro.test(sample_data(Final.ps.multiplybillion.noblanks)$shannon)
shapiro.test(sample_data(Final.ps.multiplybillion.noblanks)$SimpsonEvenness)
shapiro.test(sample_data(Final.ps.multiplybillion.noblanks)$Observed)
# Shannon appears normally distributed but Simpson and Observed are not 

# Create a dataframe for the sample data in the PS object 
metadata.final.ps.billion.noblanks <- sam_data(Final.ps.multiplybillion.noblanks)

# Create boxplots for COPD & controls 
Shannon.diversity.plot <- ggplot(metadata.final.ps.billion.noblanks, aes(x = copdcaco, y = shannon)) + 
  geom_boxplot(fill= c("red","blue"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Shannon diversity") + 
    theme(plot.title= element_text(size=26, family = "serif"), axis.title= element_text(size =20, family = "serif"), axis.text = element_text(size = 20, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Shannon diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though

Simpson.diversity.plot <- ggplot(metadata.final.ps.billion.noblanks, aes(x = copdcaco, y = SimpsonEvenness)) + 
  geom_boxplot(fill= c("red","blue"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Simpson Evenness diversity") + 
    theme(plot.title= element_text(size=26, family = "serif"), axis.title= element_text(size =20, family = "serif"), axis.text = element_text(size = 20, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Simpson Evenness diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though

Observed.diversity.plot <- ggplot(metadata.final.ps.billion.noblanks, aes(x = copdcaco, y = Observed)) + 
  geom_boxplot(fill= c("red","blue"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Observed diversity") + 
    theme(plot.title= element_text(size=26, family = "serif"), axis.title= element_text(size =20, family = "serif"), axis.text = element_text(size = 20, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Observed diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though

Shannon.diversity.plot
Simpson.diversity.plot
Observed.diversity.plot
```

### Calculate alpha diversity difference
```{r}
# Compare alpha diversity of COPD and control with a t-test or Mann-Whitney U test 

# t-tests make the following assumptions: 
# 1: The two samples are independent of one another
# 2: homogeneity of variance assumption
# 3: The two populations are normally distributed

# Wilcoxon rank sum test makes the following assumptions:
# 1: The two samples are independent of one another
# 2: homogeneity of variance assumption

# Therefore I will use t-test for shannon (normally distributed) and Wilcoxon rank sum test (Mann-Whitney U) for the others. 

# t-test of shannon diversity between COPD and controls 
shannon.t.test <- t.test(metadata.final.ps.billion.noblanks$shannon~ metadata.final.ps.billion.noblanks$copdcaco)
shannon.t.test

# t-test appears (just) significant - i.e. there is a significant difference in shannon alpha diversity between COPD and controls

# Wilcoxon rank sum (Mann-Whitney U) test of other alpha diversity measures between COPD and controls
# Chao1
wilcox.test(metadata.final.ps.billion.noblanks$Chao1~metadata.final.ps.billion.noblanks$copdcaco)
# Observed
wilcox.test(metadata.final.ps.billion.noblanks$Observed~metadata.final.ps.billion.noblanks$copdcaco)
# Simpson evenness
wilcox.test(metadata.final.ps.billion.noblanks$SimpsonEvenness~metadata.final.ps.billion.noblanks$copdcaco)

# Add t-test results to the boxplot
Shannon.diversity.plot.pval <- Shannon.diversity.plot +  stat_compare_means(method = "t.test", size=6, label.x = 0.6, label.y = 3.2)
Simpson.diversity.plot.pval <- Simpson.diversity.plot +  stat_compare_means(method = "wilcox.test", size=6, label.x = 0.6, label.y = 0.42)
Observed.diversity.plot.pval <- Observed.diversity.plot + stat_compare_means(method = "wilcox.test", size=6,label.x = 0.6, label.y = 100)

ggsave("Output_files//Alpha_diversity//Shannon_diversity_plot.png", Shannon.diversity.plot.pval, dpi = 1000, width = 10, height = 8)
ggsave("Output_files//Alpha_diversity//Simpson_diversity_plot.png", Simpson.diversity.plot.pval, dpi = 1000, width = 10, height = 8)
ggsave("Output_files//Alpha_diversity//Observed_diversity_plot.png", Observed.diversity.plot.pval, dpi = 1000, width = 10, height = 8)

# variance in boxplots appears homogenous 
# Will check formally with levene's test
#  2: homogeneity of variance assumption
leveneTest(y = metadata.final.ps.billion.noblanks$shannon, group = metadata.final.ps.billion.noblanks$copdcaco)
leveneTest(y = metadata.final.ps.billion.noblanks$Chao1, group = metadata.final.ps.billion.noblanks$copdcaco)
leveneTest(y = metadata.final.ps.billion.noblanks$Observed, group = metadata.final.ps.billion.noblanks$copdcaco)
leveneTest(y = metadata.final.ps.billion.noblanks$SimpsonEvenness, group = metadata.final.ps.billion.noblanks$copdcaco)
# Yes all are non-significant- i.e. all homogeneous 
```

## Alpha diversity tests - 90% cluster level
```{r}
ARGcluster.aggl # PS object which is clustered
ARGcluster.aggl.multiplybillion <- ARGcluster.aggl
ARGcluster.aggl.multiplybillion@otu_table@.Data <- (ARGcluster.aggl.multiplybillion@otu_table@.Data)*1000000000

# Rename sample data codes
ARGcluster.aggl.multiplybillion@sam_data$copdcaco <- revalue(ARGcluster.aggl.multiplybillion@sam_data$copdcaco, c('1' ='COPD', '0' = 'control', 'blanc'= 'blank'))

# Plot all alpha diversity measures
plot_richness(ARGcluster.aggl.multiplybillion, measures=c("Shannon", "Simpson", "Observed"))

# Create scatter plot with alpha diversity scores for COPD, controls (and blanks) separately
plot_richness(ARGcluster.aggl.multiplybillion, x="copdcaco", measures=c("Shannon", "Simpson", "Observed"))

#  Create scatter plot with colour
mycols3 <- c("COPD"="red", "control"="blue", "blank"="green")
Alphadiversity_scatter_clustered.colour <- plot_richness(ARGcluster.aggl.multiplybillion, x="copdcaco", measures=c("Shannon", "Simpson", "Observed"), color="copdcaco")
Alphadiversity_scatter_clustered.colour + scale_color_manual(values = mycols3, aesthetics = c("color", "fill"))

# Calculate alpha diversity tables
Shannon.table.clustered <- microbiome::alpha(ARGcluster.aggl.multiplybillion, (index = "Shannon"), zeroes = TRUE)
SimpsonEvenness.table.clustered <- microbiome::alpha(ARGcluster.aggl.multiplybillion, (index = "Simpson"), zeroes = TRUE)
Observed.table.clustered <-  microbiome::alpha(ARGcluster.aggl.multiplybillion, (index = "Observed"), zeroes = TRUE)

# Now add the diversity tables to the metadata
sample_data(ARGcluster.aggl.multiplybillion)$shannon <- Shannon.table.clustered$diversity_shannon
sample_data(ARGcluster.aggl.multiplybillion)$SimpsonEvenness <- SimpsonEvenness.table.clustered$evenness_simpson
sample_data(ARGcluster.aggl.multiplybillion)$Observed <- Observed.table.clustered$observed

# Make a histograms of the diversity scores
par(mfrow = c(2, 2))
hist(sample_data(ARGcluster.aggl.multiplybillion)$shannon, main="Shannon diversity", xlab="", breaks=15)
hist(sample_data(ARGcluster.aggl.multiplybillion)$SimpsonEvenness, main="Simpson evenness diversity", xlab="", breaks=15)
hist(sample_data(ARGcluster.aggl.multiplybillion)$Observed, main="Observed diversity", xlab="", breaks=15)

# Check normality of these distributions across all the samples
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion)$shannon)
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion)$SimpsonEvenness)
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion)$Observed)

# Observed index is normally distributed, Shannon and Simpson Evenness are not 

# Create boxplots for COPD/control/blanks for the different alpha diversity measures
metadata.ARGcluster.aggl.multiplybillion <- sample_data(ARGcluster.aggl.multiplybillion)
# Make the 4 boxplots
par(mfrow = c(2, 2))
ggplot(metadata.ARGcluster.aggl.multiplybillion, aes(x = copdcaco, y = shannon )) + geom_boxplot(fill= c("blue","red", "green"))
ggplot(metadata.ARGcluster.aggl.multiplybillion, aes(x = copdcaco, y = SimpsonEvenness))+ geom_boxplot(fill= c("blue","red", "green"))
ggplot(metadata.ARGcluster.aggl.multiplybillion, aes(x = copdcaco, y = Observed)) + geom_boxplot(fill= c("blue","red", "green"))


# BLANKS REMOVED FOR ANALYSIS
# Now I should exclude the blanks and perform the analyses again - blanks will affect any statistical tests 
blancs <- c("veldbl16", "veldbl3", "veldbl5")
ARGcluster.aggl.multiplybillion.noblanks <-  prune_samples(!sample_names(ARGcluster.aggl.multiplybillion)%in%blancs, ARGcluster.aggl.multiplybillion)

# Plot all alpha diversity measures
plot_richness(ARGcluster.aggl.multiplybillion.noblanks, x="copdcaco", measures=c("Shannon", "Simpson","Observed"), color="copdcaco")

#  Create scatter plot with colour
mycols4 <- c("COPD"="red", "control"="blue")
Alphadiversity_scatter_clustered_noblanks.colour <- plot_richness(ARGcluster.aggl.multiplybillion.noblanks, x="copdcaco", measures=c("Shannon", "Simpson", "Observed"), color="copdcaco")
Alphadiversity_scatter_clustered_noblanks.colour + scale_color_manual(values = mycols3, aesthetics = c("color", "fill"))

# Calculate alpha diversity tables
Shannon.table.clustered.noblanks <- alpha(ARGcluster.aggl.multiplybillion.noblanks, index = "Shannon", zeroes = TRUE)
SimpsonEvenness.table.clustered.noblanks <- alpha(ARGcluster.aggl.multiplybillion.noblanks, index = "Simpson", zeroes = TRUE)
Observed.table.clustered.noblanks <- alpha(ARGcluster.aggl.multiplybillion.noblanks, index = "Observed", zeroes = TRUE)

# Add diversity tables to the metadata
sample_data(ARGcluster.aggl.multiplybillion.noblanks)$shannon <- Shannon.table.clustered.noblanks$diversity_shannon
sample_data(ARGcluster.aggl.multiplybillion.noblanks)$SimpsonEvenness <- SimpsonEvenness.table.clustered.noblanks$evenness_simpson
sample_data(ARGcluster.aggl.multiplybillion.noblanks)$Observed <- Observed.table.clustered.noblanks$observed

# Create histograms of diversity scores
par(mfrow = c(2, 2))
hist(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$shannon, main="Shannon diversity", xlab="", breaks=15)
hist(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$SimpsonEvenness, main="Simpson evenness diversity", xlab="", breaks=15)
hist(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$Observed, main="Observed diversity", xlab="", breaks=15)

# check normality of these distributions across all the samples
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$shannon)
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$SimpsonEvenness)
shapiro.test(sample_data(ARGcluster.aggl.multiplybillion.noblanks)$Observed)

# Shannon and Simpson are not normally distributed, Observed appears normally distributed

# Create a dataframe for the sample data in the PS object 
metadata.ARGcluster.aggl.multiplybillion.noblanks <- sam_data(ARGcluster.aggl.multiplybillion.noblanks)

# Create boxplots for COPD and control for the different alpha diversity measures
Shannon.diversity.plot.clustered <- ggplot(metadata.ARGcluster.aggl.multiplybillion.noblanks, aes(x = copdcaco, y = shannon)) + 
  geom_boxplot(fill= c("cornflowerblue","coral2"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Shannon diversity") + 
    theme(plot.title= element_text(size=50, family = "serif"), axis.title= element_text(size =35, family = "serif"), axis.text = element_text(size = 35, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Shannon diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though

Simpson.diversity.plot.clustered <- ggplot(metadata.ARGcluster.aggl.multiplybillion.noblanks, aes(x = copdcaco, y = SimpsonEvenness)) + 
  geom_boxplot(fill= c("cornflowerblue","coral2"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Simpson Evenness diversity") + 
    theme(plot.title= element_text(size=50, family = "serif"), axis.title= element_text(size =35, family = "serif"), axis.text = element_text(size = 35, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Simpson Evenness diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though

Observed.diversity.plot.clustered <- ggplot(metadata.ARGcluster.aggl.multiplybillion.noblanks, aes(x = copdcaco, y = Observed)) + 
  geom_boxplot(fill= c("cornflowerblue","coral2"),  alpha=0.5, outlier.shape = NA, fatten= 4)+ # alpha argument makes fill more transparent, fatten argument makes median line thicker,
  ggtitle("Observed diversity") + 
    theme(plot.title= element_text(size=50, family = "serif"), axis.title= element_text(size =35, family = "serif"), axis.text = element_text(size = 35, family = "serif"))+
  theme(plot.title = element_text(hjust = 0.5))+ #puts title in centre 
  xlab("Study population")+
  ylab("Observed diversity index")+
  geom_point(position = "jitter", size= 4, alpha=0.3) # adds jittered points to plot, not in matched colours yet though


Shannon.diversity.plot.clustered
Simpson.diversity.plot.clustered
Observed.diversity.plot.clustered

```
### Calculate alpha diversity difference - 90% cluster level
```{r}
# Compare alpha diversity of COPD and control with a t-test or Mann-Whitney U test 

# t-tests make the following assumptions: 
# 1: The two samples are independent of one another
# 2: homogeneity of variance assumption
# 3: The two populations are normally distributed

# Wilcoxon rank sum test makes the following assumptions:
# 1: The two samples are independent of one another
# 2: homogeneity of variance assumption

# Wilcoxon rank sum test (Mann-Whitney U) for Shannon and Simpson Evenness but t-test for Observed diversity

# Wilcoxon rank sum - Shannon
wilcox.test(metadata.ARGcluster.aggl.multiplybillion.noblanks$shannon~metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)
# Wilcoxon rank sum - Simpson evenness
wilcox.test(metadata.ARGcluster.aggl.multiplybillion.noblanks$SimpsonEvenness~metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)
# t-test - Observed diversity
t.test(metadata.ARGcluster.aggl.multiplybillion.noblanks$Observed~ metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)

# Add test results to the boxplots
Shannon.diversity.plot.clustered +  stat_compare_means(method = "wilcox.test", size=11, label.x = 0.6, label.y = 3.5)
Simpson.diversity.plot.clustered +  stat_compare_means(method = "wilcox.test", size=11, label.x = 0.6, label.y = 0.5)
Observed.diversity.plot.clustered+  stat_compare_means(method = "t.test", size=11,label.x = 0.6, label.y = 100)

# Check homogeneity of variance assumption with levene's test
leveneTest(y = metadata.ARGcluster.aggl.multiplybillion.noblanks$shannon, group = metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)
leveneTest(y = metadata.ARGcluster.aggl.multiplybillion.noblanks$SimpsonEvenness, group = metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)
leveneTest(y = metadata.ARGcluster.aggl.multiplybillion.noblanks$Observed, group = metadata.ARGcluster.aggl.multiplybillion.noblanks$copdcaco)
# Yes all are non-significant- i.e. all homogeneous 
```

## Beta diversity - 90% cluster level
** Tells us how different the microbial composition in one environment is compared to another. I.e. I will look at the whether the microbial composition in COPD pts differs to that in controls.**

### PERMANOVA
```{r}
# There are two tests that are widely used in beta diversity analyses: betadisper and adonis. 
# Betadisper tests whether two or more groups are homogeneously dispersed in relation to their species in studied samples (it is a multivariate analogue of Levene's test for homogeneity of variances). This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. So firstly I will perform betadisper: then move onto adonis
# Use betadisper to assess homogeneity of group dispersions - it first calculates the average distance of group members to the group centroid in multivariate space (generated by a distance matrix). Then, an ANOVA is done to test if the dispersions (variances) of groups are different.
#Extract abundance matrix from phyloseq object
ARGclusters.for.beta.div <- as(otu_table(ARGcluster.aggl), "matrix")
# transform into a data frame
ARG.cluster.dataframe <-as.data.frame(ARGclusters.for.beta.div)
ARG.cluster.dataframe
# Calculate BC distances between all sample types
BC.distances <- vegdist(ARG.cluster.dataframe, method =  "bray")
# Add column to dataframe to include Copdcaco status
ARG.cluster.dataframe$copdcaco <- ARGcluster.aggl@sam_data$copdcaco
head(ARG.cluster.dataframe)

# Betadisper 
Betadisper.ARGcluster.copdcontrolblank <- betadisper(BC.distances, ARG.cluster.dataframe$copdcaco)
Betadisper.ARGcluster.copdcontrolblank
# Perform an anova using the group dispersions.
Anova.betadisper.copdcontrolblank <- anova(Betadisper.ARGcluster.copdcontrolblank)
# We  see that the ANOVA's p-value is not significant meaning that group dispersions are homogenous ("Null hypothesis of no difference in dispersion between groups
Anova.betadisper.copdcontrolblank$`Pr(>F)`# to add to PCoA in above chunk
# make a plot to see our first two PCoA axes to see how our data fits there:
plot(Anova.betadisper.copdcontrolblank)
# Here we see that group dispersions (distances from centroids) are very similar, but also that compositions seem to be very similar too. Let's go to adonis to test whether these compositions are different or not. 

# PERMANOVA
# Use adonis2 to assess compositional dissimilarity - The adonis2() function in the vegan package is an analysis of variance using distance matrices 
set.seed(123)
Permanova_results_COPDcontrolblank <- adonis2(BC.distances~ ARG.cluster.dataframe$copdcaco, permutations = 999)
# we see that the groups do  have significantly different compositions (p<0.05). 
# We can conclude that our groups (COPD and control and blank) present homogeneity among group dispersions (compositions vary similarly) but that they have different  compositions.

# Now repeat with no blanks (this is likely the cause of the observed difference in composition)
ARGclusters.for.beta.div <- as(otu_table(ARGcluster.aggl), "matrix")
ARG.cluster.dataframe.noblanks <-as.data.frame(ARGclusters.for.beta.div)[-c(70, 71, 72),] # Remove blanks from the df

# Calculate the BC distances between COPD case and control only
BC.distances.noblanks <- vegdist(ARG.cluster.dataframe.noblanks, method =  "bray")
# Add column to dataframe to include Copdcaco status
ARG.cluster.dataframe.noblanks$copdcaco <- ARGcluster.aggl.noblanks@sam_data$copdcaco
head(ARG.cluster.dataframe.noblanks)

# Betadisper 
Betadisper.ARGcluster.COPDcaco <- betadisper(BC.distances.noblanks, ARG.cluster.dataframe.noblanks$copdcaco)
Betadisper.ARGcluster.COPDcaco
# Perform an anova using the group dispersions.
Anova_betadisper <- anova(Betadisper.ARGcluster.COPDcaco)
# We  see that the ANOVA's p-value is not significant meaning that group dispersions are homogenous ("Null hypothesis of no difference in dispersion between groups
Anova_betadisper$`Pr(>F)`# to add to PCoA in above chunk
# make a plot to see our first two PCoA axes to see how our data fits there:
plot(Betadisper.ARGcluster.COPDcaco)
# Here we see that group dispersions (distances from centroids) are very similar, but also that compositions seem to be very similar too. Let's go to adonis to test whether these compositions are different or not. 

# PERMANOVA
# Use adonis2 to assess compositional dissimilarity - The adonis2() function in the vegan package is an analysis of variance using distance matrices 
set.seed(123)
Permanova_results_COPDcontrol<- adonis2(BC.distances.noblanks~ ARG.cluster.dataframe.noblanks$copdcaco, permutations = 999)
# we see that the groups do not have significantly different compositions (p>0.05). 
# We can conclude that our groups (COPD and control) present homogeneity among group dispersions (compositions vary similarly) and also they have very similar compositions.
```
### PCoA for manuscript
```{r}
# PCoA for manuscript - COPD case vs control
set.seed(123)
Permanova_results_COPDcontrol <- adonis2(BC.distances.noblanks~ ARG.cluster.dataframe.noblanks$copdcaco, permutations = 999)
Permanova_pval_COPDcontrol <- Permanova_results_COPDcontrol$`Pr(>F)`[1]
# p-value = 0.19 - no significant differences in group centroids-  We can conclude that our groups

# Create PCoA for manuscript
# Firstly I will rename the codes "1", "0" as COPD & control 
ARGcluster.aggl.noblanks@sam_data$copdcaco <- revalue(ARGcluster.aggl.noblanks@sam_data$copdcaco, c('1' ='COPD', '0' = 'control'))

ARGcluster.aggl.PCoA.noblanks.ordination <- ordinate(ARGcluster.aggl.noblanks, method="PCoA", distance="bray")

ARGcluster.aggl.PCoA.plot.noblanks <- plot_ordination(ARGcluster.aggl.noblanks, ARGcluster.aggl.PCoA.noblanks.ordination, color = "copdcaco", shape = "copdcaco") +
  ggtitle("Bray-Curtis PCoA - COPD vs control") +
  labs(legend = "COPD/control status") +
  geom_point(aes(color = copdcaco), size = 2) + # coloring points based on copdcaco variable
  scale_color_manual(values = c("cornflowerblue", "coral2")) + # assigning blue to COPD and red to control
  geom_text(aes(label = row.names(ARGcluster.aggl.noblanks@sam_data), hjust = -0.3, vjust = -0.5), size = 2) +
  theme(plot.title = element_text(size = 12)) +
  theme(axis.text = element_text(size = 15),
        axis.title = element_text(size = 15)) + 
  theme(legend.text = element_text(size = 10),
        legend.title = element_text(size = 10)) +
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = copdcaco, color = copdcaco)) + # coloring ellipses based on copdcaco variable
  scale_fill_manual(values = c("cornflowerblue", "coral2")) + # assigning blue to COPD and red to control
  scale_color_manual(values = c("cornflowerblue", "coral2")) # assigning blue to COPD and red to control


# Add the p-value annotation to the PCoA plot using geom_text
ARGcluster.aggl.PCoA.plot.noblanks <- ARGcluster.aggl.PCoA.plot.noblanks +
  geom_text(x = -0.4, y = 0.48, label = paste("PERMANOVA, p-value =", Permanova_pval_COPDcontrol), size = 3.7, color = "black")
ARGcluster.aggl.PCoA.plot.noblanks

ggsave("Output_files//PCoAs//ARGcluster.aggl.PCoA.plot.noblanks.png", ARGcluster.aggl.PCoA.plot.noblanks, width = 9, height = 6, dpi = 1000)
```


## Differential abundance analysis - ARGs clustered at 90% identity level 
### DESeq2 (unfiltered data)
```{r}
# Since the DESeq2 function requires count data which has not been corrected for gene length, I need to create a new ps object with only corrections for volume input and qPCR bacterial count
Countdata.DESeq2 # This is the ps object to be used for DESeq2 analysis (not gene length corrected but corrected for input volumes and qPCR bacterial counts). BUT this is not at the 90% cluster level so I should agglomerate this ps object (using previously created tax_glom2 function from AB - see below too)

# # Tax_glom2 function as created by AB - save this in the global environment for later use
# tax_glom2 <- function(physeq, taxrank=rank_names(physeq)[1],
#                      NArm=TRUE, bad_empty=c(NA, "", " ", "\t")){
#   #### This part is identical to phyloseq's tax_glom
#   # Error if tax_table slot is empty
#   if( is.null(access(physeq, "tax_table")) ){
#     stop("The tax_glom2() function requires that physeq contain a taxonomyTable")
#   }
#   # Error if bad taxrank
#   if( !taxrank[1] %in% rank_names(physeq) ){
#     stop("Bad taxrank argument. Must be among the values of rank_names(physeq)")
#   }
#   # Make a vector from the taxonomic data.
#   CN  <- which( rank_names(physeq) %in% taxrank[1] )
#   tax <- as(access(physeq, "tax_table"), "matrix")[, CN]
#   # if NArm is TRUE, remove the empty, white-space, NA values from
#   if( NArm ){
#     keep_species <- names(tax)[ !(tax %in% bad_empty) ]
#     physeq <- prune_taxa(keep_species, physeq)
#   }
#   # Concatenate data up to the taxrank column, use this for agglomeration
#   tax <- as(access(physeq, "tax_table"), "matrix")[, 1:CN, drop=FALSE]
#   tax <- apply(tax, 1, function(i){paste(i, sep=";_;", collapse=";_;")})
#   #### **Speedyseq changes start here**
#   ## Make the new OTU table
#   # Work with taxa as rows
#   if (!taxa_are_rows(physeq)) {
#     physeq <- phyloseq::t(physeq)
#     # Note that we need to flip back to samples as rows at the end
#     needs_flip <- TRUE
#   } else {
#     needs_flip <- FALSE
#   }
#   # Starting point is a tibble with rows as taxa, to be able to combine taxa
#   # with the dplyr::summarize_*() functions
#   otu <- otu_table(physeq)
#   tb <- otu %>%
#     as("matrix") %>%
#     tibble::as_tibble(rownames = "OTU")
#   # We want to name each new taxon (group of merged OTUs) by its "archetype",
#   # the most abundant OTU in the group
#   tb <- tb %>%
#     tibble::add_column(Tax = tax, Sum = taxa_sums(physeq)) %>%
#     dplyr::group_by(Tax)
#   # Name new taxa by the most abundant OTU; pick the first OTU in case of
#   # ties (to be consistent with phyloseq)
#   new_taxa_names <- tb %>%
#     dplyr::top_n(1, Sum) %>%
#     dplyr::slice(1) %>%
#     dplyr::select(Tax, OTU)
#   # Sum abundances and rename taxa
#   tb0 <- tb %>%
#     dplyr::summarize_at(dplyr::vars(sample_names(physeq)), sum) %>%
#     dplyr::left_join(new_taxa_names, by = "Tax") %>%
#     dplyr::select(OTU, dplyr::everything(), -Tax)
#   # Put back into phyloseq form
#   mat <- tb0 %>%
#     dplyr::select(-OTU) %>%
#     as("matrix")
#   rownames(mat) <- tb0$OTU
#   otu0 <- otu_table(mat, taxa_are_rows = TRUE)
#   ## Make the new phyloseq object
#   # Replacing the original otu_table with the new, smaller table will
#   # automatically prune the taxonomy, tree, and refseq to the smaller set of
#   # archetypal otus
#   otu_table(physeq) <- otu0
#   # "Empty" the taxonomy values to the right of the rank, using
#   # NA_character_.
#   if (CN < length(rank_names(physeq))) {
#     bad_ranks <- seq(CN + 1, length(rank_names(physeq)))
#     tax_table(physeq)[, bad_ranks] <- NA_character_
#   }
#   ## Return.
#   if (needs_flip) {
#     physeq <- phyloseq::t(physeq)
#   }
#   return(physeq)
# }


# Use function tax_glom2 - 6 is the ARGCluster level in the tax_table of Final.ps object. This creates a new ps object with ARGcluster as default 
ARGcluster.aggl.DESeq2 <- tax_glom2(Countdata.DESeq2, taxrank=rank_names(Countdata.DESeq2)[6], NArm=TRUE)
taxa_names(ARGcluster.aggl.DESeq2)
# Confused why the 'taxa_names' are not the arg cluster names? AB responded saying: "What you see is perfectly normal. If you checkout the tax_table you will see that the lowest level is cluster. The lower ranks should be NA. That is why you use explaining Ps object names to realise some aggregation happened. The data is underneath still linked on indiv accno level but incomplete. Only one per arg cluster should be present.  If you now directly ask the taxon name it is indeed "wrong" the accession name. Just use the arg cluster name corresponding to it and you should be fine. Most functions suited for Ps object allow to mention the tax rank or the fill rank to use. They handle it for you underneath in the same way.No worries, just check above and if that matches. Just continue and be sure to use the name of the tax rank. 
# Check tax-table of ps object
tax_table(ARGcluster.aggl.DESeq2)

# How many taxa before/after agglomeration?
ntaxa(Countdata.DESeq2); ntaxa(ARGcluster.aggl.DESeq2)
# 85 ARGClusters vs 233 ARGs 

# Using WvK method - with phyloseq_to_deseq2 function
dds <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2,~copdcaco)

# RUN  DESeq function 
dds <- DESeq(dds)
res <- results(dds)
res
#summary of differential gene expression
summary(res)
head(res)

# Information about which variables and tests were used for the results is given by this function
mcols(res)$description

# Order the results table by the smallest p value:
resOrdered <- res[order(res$pvalue),]
resOrdered #tet(K)_1_U38656 has the greatest difference between COPD and controls

# How many adjusted p-values are < 0.1? 
sum(res$padj < 0.1, na.rm=TRUE) # 0 ARG clusters are <0.1

#we can use plotCounts fxn to compare the normalized counts between COPD and control groups for our top 4 ARGs
par(mfrow=c(1,2))
plotCounts(dds_method2, gene="msr(D)_3_AF227520", intgroup="copdcaco")
plotCounts(dds_method2, gene="tet(M)_4_X75073", intgroup="copdcaco")

# Plot to show the log2 fold changes attributable to a given variable over the mean of normalized counts for all the samples in the DESeqDataSet. Points will be colored red if the adjusted p value is less than 0.1. Points which fall out of the window are plotted as open triangles pointing either up or down.
plotMA(res_method2, ylim=c(-2,2), size= 20)

# Shrink the log2fold changes as these provide better LFC estimates
resultsNames(dds_method2)
BiocManager::install("apeglm")
resLFCshrink <- lfcShrink(dds_method2, coef="copdcaco_1_vs_0", type="apeglm")
resLFCshrink

#It is more useful visualize the MA-plot for the shrunken log2 fold changes, which remove the noise associated with log2 fold changes from low count genes without requiring arbitrary filtering thresholds.
plotMA(resLFCshrink, ylim=c(-2,2)) # plot looks odd...?

# Using ggplot2 to create an MA plot
# Load libraries
# install.packages(c("ggplot2", "scales", "viridis"))
library(ggplot2)
library(scales) # needed for oob parameter
library(viridis)
# Coerce to a data frame
deseq2ResDF <- as.data.frame(res_method2)
# Examine this data frame
head(deseq2ResDF)
# Set a boolean column for significance
deseq2ResDF$significant <- ifelse(deseq2ResDF$padj < .1, "Significant", NA)
# Plot the results similar to DEseq2
ggplot(deseq2ResDF, aes(baseMean, log2FoldChange, colour=significant)) + geom_point(size=1) + scale_y_continuous(limits=c(-3, 3), oob=squish) + scale_x_log10() + geom_hline(yintercept = 0, colour="tomato1", size=2) + labs(x="mean of normalized counts", y="log fold change") + scale_colour_manual(name="q-value", values=("Significant"="red"), na.value="grey50") + theme_bw()
# Let's add some more detail
ggplot(deseq2ResDF, aes(baseMean, log2FoldChange, colour=padj)) + geom_point(size=1) + scale_y_continuous(limits=c(-3, 3), oob=squish) + scale_x_log10() + geom_hline(yintercept = 0, colour="darkorchid4", size=1, linetype="longdash") + labs(x="mean of normalized counts", y="log fold change") + scale_colour_viridis(direction=-1, trans='sqrt') + theme_bw() + geom_density_2d(colour="black", size=2)

# volcano plot
#reset par
par(mfrow=c(1,1))
# Make a basic volcano plot
with(res_method2, plot(log2FoldChange, -log10(pvalue), pch=20, main="Volcano plot", xlim=c(-3,3)))

# Add colored points: blue if padj<0.01, red if log2FC>1 and padj<0.05)
with(res_method2, plot(log2FoldChange, -log10(pvalue), pch=20, main="Volcano plot", xlim=c(-3,3)))
with(subset(res_method2, padj<.1 ), points(log2FoldChange, -log10(pvalue), pch=20, col="red"))

```
### DESeq2 (filtered - 0.1% relative abundance, 15% prevalent) 
```{r}
# Copy over ps object to be used
ARGcluster.aggl.DESeq2.filtered <- ARGcluster.aggl.DESeq2
# Rename ARG cluster names to correct cluster names
colnames(ARGcluster.aggl.DESeq2.filtered@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")
# Create prevalence-abundance filter
# 0.1% abundant, 25% prevalent:  
pres_abund_filter_deseq <- function(ARGcluster.aggl.DESeq2.filtered, pres=10.35, abund=0.001) { # Subramanian filter, keep taxa that are more than 0.1% (proportion = 0.001) abundant in more than 15% of samples (15% of 69= 10.35)
filter_taxa(ARGcluster.aggl.DESeq2.filtered, function(x) sum(x >= abund)>=pres, TRUE) }
# apply filter
ARGcluster.aggl.DESeq2.filtered.relative <- transform_sample_counts(ARGcluster.aggl.DESeq2.filtered, function(x) x/sum(x) *100 ) %>% pres_abund_filter_deseq()
# Now there are 30 taxa (instead of 85)

# Now run DESeq analysis again with this newly filtered data on COPD status 
ARGcluster.aggl.DESeq2.filtered.relative@sam_data$copdcaco
deseq2.copd.filt <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.filtered.relative,~ copdcaco)
# RUN  DESeq function 
deseq2.copd.filt <- DESeq(deseq2.copd.filt, parallel = FALSE)
deseq2.copd.results.filt <- results(deseq2.copd.filt)
deseq2.copd.results.filt
#summary of differential gene expression
summary(deseq2.copd.results.filt)
head(deseq2.copd.results.filt)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.copd.results.filt)$description
# Order the results table by the smallest p value:
deseq2.copd.results.filt.ordered <- deseq2.copd.results.filt[order(deseq2.copd.results.filt$pvalue),]
deseq2.copd.results.filt.ordered 
sum(deseq2.copd.results.filt.ordered$padj < 0.05, na.rm=TRUE) # I now find 1 ARG cluster which has a p-value of <0.1


# # create plot - from mol epi course - see DADA2 practical
# theme_set(theme_bw())
# sigtabgen = subset(sigtab, !is.na(Genus))
# # Family order
# x = tapply(sigtabgen$log2FoldChange, sigtabgen$Family, function(x) max(x))
# x = sort(x, TRUE)
# sigtabgen$Family = factor(as.character(sigtabgen$Family), levels=names(x))
# # Genus order
# x = tapply(sigtabgen$log2FoldChange, sigtabgen$Genus, function(x) max(x))
# x = sort(x, TRUE)
# sigtabgen$Genus = factor(as.character(sigtabgen$Genus), levels=names(x))
# ggplot(sigtabgen, aes(y=Genus, x=log2FoldChange, color=Family)) + 
#   geom_vline(xintercept = 0.0, color = "gray", size = 0.5) +
#   geom_point(size=6) + 
#   theme(axis.text.x = element_text(angle = -90, hjust = 0, vjust=0.5))

# Plot to show the log2 fold changes attributable to a given variable over the mean of normalized counts for all the samples in the DESeqDataSet. Points will be colored red if the adjusted p value is less than 0.1. Points which fall out of the window are plotted as open triangles pointing either up or down.
plotMA(deseq2.copd.results.filt.ordered, ylim=c(-2,2))

# Shrink the log2fold changes as these provide better LFC estimates
resultsNames(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
BiocManager::install("apeglm")
library(apeglm)
resultsNames(deseq2.copd.filt)
deseq2.copd.filt.lfcshrink <- lfcShrink(deseq2.copd.filt, coef="copdcaco_1_vs_0", type="apeglm")
deseq2.copd.filt.lfcshrink
#It is more useful visualize the MA-plot for the shrunken log2 fold changes, which remove the noise associated with log2 fold changes from low count genes without requiring arbitrary filtering thresholds.
plotMA(deseq2.copd.results.filt, ylim=c(-2,2)) # plot looks odd...?

# Using ggplot2 to create an MA plot
# Load libraries
# install.packages(c("ggplot2", "scales", "viridis"))
library(ggplot2)
library(scales) # needed for oob parameter
library(viridis)
# Coerce to a data frame
deseq2.copd.results.filt.copdcaco.MAplot <- as.data.frame(deseq2.copd.results.filt)
# Examine this data frame
head(deseq2.copd.results.filt.copdcaco.MAplot)
# Set a boolean column for significance
deseq2.copd.results.filt.copdcaco.MAplot$significant <- ifelse(deseq2.copd.results.filt.copdcaco.MAplot$padj < .05, "Significant", NA)
# Plot the results in an MA plot 
ggplot(deseq2.copd.results.filt.copdcaco.MAplot, aes(baseMean, log2FoldChange, colour=significant)) + geom_point(size=3) + scale_y_continuous(limits=c(-2, 2)) + scale_x_log10() + geom_hline(yintercept = 0, colour="red", size=0.5) + labs(x="Mean of ARG counts", y="Log2 fold change - COPD case to control") +  scale_colour_manual(name="q-value", values=("Significant"="red"), na.value="black") + theme_bw()

# Create a plot of counts 
deseq2.copd.results.filt.ordered[1,]
rowname(deseq2.copd.results.filt.ordered[1,]) <- ""

plotCounts(deseq2.copd.filt, gene=which.min(deseq2.copd.results.filt$padj), intgroup="copdcaco", xlab = "Control (0) vs COPD case (1)")
```
### ALDEx2 (unfiltered data)
```{r}
# 
BiocManager::install("ALDEx2")
library(ALDEx2)

Countdata.DESeq2 # This is the ps object to be used for ALDex2 analysis (not gene length corrected but corrected for input volumes and qPCR bacterial counts). BUT this is not at the 90% cluster level so I should agglomerate this ps object (using previously created tax_glom2 function from AB - see below too)
# Use function tax_glom2 - 6 is the ARGCluster level in the tax_table of Final.ps object. This creates a new ps object with ARGcluster as default 
ARGcluster.aggl.ALDEx2 <- tax_glom2(Countdata.DESeq2, taxrank=rank_names(Countdata.DESeq2)[6], NArm=TRUE)
taxa_names(ARGcluster.aggl.ALDEx2)
# Confused why the 'taxa_names' are not the arg cluster names? AB responded saying: "What you see is perfectly normal. If you checkout the tax_table you will see that the lowest level is cluster. The lower ranks should be NA. That is why you use explaining Ps object names to realise some aggregation happened. The data is underneath still linked on indiv accno level but incomplete. Only one per arg cluster should be present.  If you now directly ask the taxon name it is indeed "wrong" the accession name. Just use the arg cluster name corresponding to it and you should be fine. Most functions suited for Ps object allow to mention the tax rank or the fill rank to use. They handle it for you underneath in the same way.No worries, just check above and if that matches. Just continue and be sure to use the name of the tax rank. 
# Check tax-table of ps object
tax_table(ARGcluster.aggl.ALDEx2)

# How many taxa before/after agglomeration?
ntaxa(Countdata.DESeq2); ntaxa(ARGcluster.aggl.ALDEx2)
# 85 ARGClusters vs 233 ARGs 

#Rename ARG clusters by cluster name
colnames(ARGcluster.aggl.ALDEx2@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

# create otu table for aldex2 analysis - this needs to be "A non-negative, integer-only data.frame or matrix with unique names for all rows and columns. Rows should contain genes and columns should contain sequencing read counts (i.e., sample vectors). Rows with 0 reads in each sample are deleted prior to analysis
ALDEx2.otutable.trans <- t(otu_table(ARGcluster.aggl.ALDEx2))
ALDEx2.otutable.trans.bill.round <- round(ALDEx2.otutable.trans * 1000000000,0) # I multiply by billion as aldex2 requires that reads are all integers 
otutab.for.ALDEx2 <- data.frame(ALDEx2.otutable.trans.bill.round)
# Then we set the comparison groups. This must be a vector of conditions in the same order as the samples in the input counts table.
sampledata.for.ALDEx2 <- sample_data(ARGcluster.aggl.ALDEx2)$copdcaco

# Run the aldex command - I ask aldex to do a 2-sample t-test to calculate effect sizes,
# The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code. Specifically, this function: (a) generates Monte Carlo samples of the Dirichlet distribution for each sample, (b) converts each instance using a log-ratio transform, then (c) returns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests. This function also estimates effect size for two sample analyses.
ALDEx2_COPDcontrol <- ALDEx2::aldex(otutab.for.ALDEx2, sampledata.for.ALDEx2, test="t", effect = TRUE, denom="iqlr")  # Aldex parameters set: test = "t" runs Welch's t-test and Wilcoxon tests. Effect = TRUE - tells aldex to calculate effect sizes. Denom="iqlr" is commonly used and indicates which features to retain as the denominator for the geometric mean calculation - "iqlr" accounts for data with systematic variation.

# Create MA and Effect plots of ALDEx2 output
# create Bland-Altman plot 
par(mfrow = c(1, 2))
aldex.plot(ALDEx2_COPDcontrol, type="MA", test="welch", xlab="Log-ratio abundance",
    ylab="Difference")
# create variance-difference plot 
aldex.plot(ALDEx2_COPDcontrol, type="MW", test="welch", xlab="Dispersion",
    ylab="Difference", all.cex = 3)

# Compute an aldex.clr object
ALDEx2_COPDcontrol.clr <- aldex.clr(otutab.for.ALDEx2, sampledata.for.ALDEx2, mc.samples=16, denom="all", verbose=F)
# aldex.ttest calculates the expected values of the Wilcoxon Rank Sum test and Welch’s t-test on the data returned by aldex.clr.
ALDEx2_COPDcontrol.tt <- aldex.ttest(ALDEx2_COPDcontrol.clr, paired.test=FALSE, verbose=FALSE)
ALDEx2_COPDcontrol.tt 
# where: we.ep - Expected P value of Welch’s t test, we.eBH - Expected Benjamini-Hochberg corrected P value of Welch’s t test, wi.ep - Expected P value of Wilcoxon rank test, wi.eBH - Expected Benjamini-Hochberg corrected P value of Wilcoxon test
# I will look at Wilcoxon test results with BH correction (we.eBH)
# Reorder the p-values based on size of wi.eBH value
reOrdered_ALDEx2_COPDcontrol.tt <- ALDEx2_COPDcontrol.tt[order(ALDEx2_COPDcontrol.tt$wi.eBH),]
reOrdered_ALDEx2_COPDcontrol.tt
# check to identify how many values are significant from the t-test (q<0.1)
which(ALDEx2_COPDcontrol.tt$wi.eBH < 0.1)
# No clusters have BH corrected p-value < 0.1
# Therefore we conclude that using the ALDEx2 method, we do not identify any ARGs which are differentially abundant. 
# Estimate effect size and the within and between condition values in the case of two conditions
ALDEx2_COPDcontrol.effect <- aldex.effect(ALDEx2_COPDcontrol.clr, CI=T, verbose=FALSE)
# Merging the t-test and effect data are merged into one object
ALDEx2_COPDcontrol.all <- data.frame(ALDEx2_COPDcontrol.tt,ALDEx2_COPDcontrol.effect)
```

### ALDEx2 (filtered - 0.1% relative abundance, 15% prevalent)
```{r}
BiocManager::install("ALDEx2")
library(ALDEx2)

# copy over the previously agglomerated and filtered ps object from my above DESeq2 analysis
ARGcluster.aggl.ALDEx2.filtered <- ARGcluster.aggl.DESeq2.filtered.relative
ARGcluster.aggl.ALDEx2.filtered
tax_table(ARGcluster.aggl.ALDEx2.filtered)
# 30 ARG Clusters 

# create otu table for aldex2 analysis - this needs to be "A non-negative, integer-only data.frame or matrix with unique names for all rows and columns. Rows should contain genes and columns should contain sequencing read counts (i.e., sample vectors). Rows with 0 reads in each sample are deleted prior to analysis
ALDEx2.filtered.otutable.trans <- t(otu_table(ARGcluster.aggl.ALDEx2.filtered))
ALDEx2.filtered.otutable.trans.bill.round <- round(ALDEx2.filtered.otutable.trans * 1000000000,0) # I multiply by billion as aldex2 requires that reads are all integers 
otutab.for.ALDEx2.filtered <- data.frame(ALDEx2.filtered.otutable.trans.bill.round)
# Then we set the comparison groups. This must be a vector of conditions in the same order as the samples in the input counts table.
sampledata.for.ALDEx2 <- sample_data(ARGcluster.aggl.ALDEx2.filtered)$copdcaco

# Run the aldex command - I ask aldex to do a 2-sample t-test to calculate effect sizes,
# The aldex function is a wrapper that performs log-ratio transformation and statistical testing in a single line of code. Specifically, this function: (a) generates Monte Carlo samples of the Dirichlet distribution for each sample, (b) converts each instance using a log-ratio transform, then (c) returns test results for two sample (Welch’s t, Wilcoxon) or multi-sample (glm, Kruskal-Wallace) tests. This function also estimates effect size for two sample analyses.
ALDEx2.filtered.COPDcontrol <- ALDEx2::aldex(otutab.for.ALDEx2.filtered, sampledata.for.ALDEx2, test="t", effect = TRUE, denom="iqlr", include.sample.summary = TRUE) # Aldex parameters set: test = "t" runs Welch's t-test and Wilcoxon tests. Effect = TRUE - tells aldex to calculate effect sizes. Denom="iqlr" is commonly used and indicates which features to retain as the denominator for the geometric mean calculation - "iqlr" accounts for data with systematic variation. 
ALDEx2.filtered.COPDcontrol

# Create MA and Effect plots of ALDEx2 output
# create Bland-Altman plot 
par(mfrow = c(1, 2))
aldex.plot(ALDEx2_COPDcontrol, type="MA", test="welch", xlab="Log-ratio abundance",
    ylab="Difference")
# create variance-difference plot 
aldex.plot(ALDEx2_COPDcontrol, type="MW", test="welch", xlab="Dispersion",
    ylab="Difference", all.cex = 3)

# Compute an aldex.clr object
ALDEx2.filtered.COPDcontrol.clr <- aldex.clr(otutab.for.ALDEx2.filtered, sampledata.for.ALDEx2, mc.samples=128, denom="all", verbose=F)
# aldex.ttest calculates the expected values of the Wilcoxon Rank Sum test and Welch’s t-test on the data returned by aldex.clr.
ALDEx2.filtered.COPDcontrol.ttest <- aldex.ttest(ALDEx2.filtered.COPDcontrol.clr, paired.test=FALSE, verbose=FALSE)
ALDEx2.filtered.COPDcontrol.ttest 
# where: we.ep - Expected P value of Welch’s t test, we.eBH - Expected Benjamini-Hochberg corrected P value of Welch’s t test, wi.ep - Expected P value of Wilcoxon rank test, wi.eBH - Expected Benjamini-Hochberg corrected P value of Wilcoxon test
# I will look at Wilcoxon test results with BH correction (we.eBH)
# Reorder the p-values based on size of wi.eBH value
reOrdered_ALDEx2.filtered.COPDcontrol.ttest <- ALDEx2.filtered.COPDcontrol.ttest[order(ALDEx2.filtered.COPDcontrol.ttest$wi.eBH),]
reOrdered_ALDEx2.filtered.COPDcontrol.ttest
# check to identify how many values are significant from the t-test (q<0.1)
which(ALDEx2.filtered.COPDcontrol.ttest$wi.eBH < 0.1)
# 0 ARG clusters are differentially abundant 
# Therefore we conclude that using the ALDEx2 method, we do not identify any ARGs which are differentially abundant

# Estimate effect size and the within and between condition values in the case of two conditions
ALDEx2.filtered.COPDcontrol.effect <- aldex.effect(ALDEx2.filtered.COPDcontrol.clr, CI=T, verbose=FALSE)
```

## Predicting outcome using Machine learning algorithms
### Support Vector Machine
```{r}
#A support vector machine is a supervised machine learning model that can be used for classification i.e. classifying into COPD or control groups
# The aim is to find a hyperplane that distinctly classifies the data points.
# I give the SVM a set of labelled training data and then see if it’s able to categorise the test dataset. 

# load required package 
dataframe1 <- data.frame(Countdata.DESeq2@otu_table)
dataframe2 <- data.frame(Countdata.DESeq2@sam_data)
dataframe1$copdcaco <- dataframe2$copdcaco
dataframe1 # my df for the analysis 

# setting the features (i.e the 233 genes in the dataset)
dataset <- dataframe1[,1:233]
#encode outcome as factor (COPD/control)
dataframe1$copdcaco <- factor(dataframe1$copdcaco, levels = c(0,1))

# Splitting the dataset into the Training set and Test set
install.packages('caTools')
library(caTools)

set.seed(123)
split <- sample.split(dataframe1$copdcaco, SplitRatio = 0.75)
  
training_set <- subset(dataframe1, split == TRUE)
test_set <- subset(dataframe1, split == FALSE)

# Inspect the splitting of the data 
split 
training_set #52 samples in training set
test_set # 17 in test set 


# Fitting SVM to the Training set
dataset
classifier <- svm(formula = copdcaco ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'linear')
classifier

# Predicting the Test set results
y_pred <- predict(classifier, newdata = test_set)
y_pred

# Making the Confusion Matrix
confusion.matrix <- table(test_set[, 234], y_pred)
confusion.matrix
# This is the confusion matrix that came from testing the algorithm on the test dataset: columns show predictions from the model and rows are actual values in the test dataset. This shows the model had 5 false positives and 4 false negatives, out of a total of 17 samples – doesn’t seem great. 

# calculating accuracy of the model prediction from the confusion matrix 
(3+5)/(17)

# Calculate AUC of model 
library(ModelMetrics)
auc.linear  <- auc(test_set$copdcaco, y_pred)
auc.linear

# Try SVM with radial kernel 
dataset
classifier.radial <- svm(formula = copdcaco ~ .,
                 data = training_set,
                 type = 'C-classification',
                 kernel = 'radial')
classifier.radial

# Predicting the Test set results
y_pred_radial <- predict(classifier.radial, newdata = test_set)
y_pred_radial

# Making the Confusion Matrix
confusion.matrix.radial <- table(test_set[, 234], y_pred_radial)
confusion.matrix.radial

library(ModelMetrics)
aucs.radial  <- auc(test_set$hd, y_pred_radial)
aucs.radial
```


```{r}
# # Other attempt
# dat <-  data.frame(x, y = as.factor(y))
# svmfit <- svm(y ~ ., data = dat, kernel = "radial", cost = 5)
# print(svmfit)
# 
# #plot function for SVM that shows the decision boundary
# plot(svmfit, dat)
# 
# # create grid of values 
# make.grid = function(x, n = 69) {
#   grange = apply(x, 2, range)
#   x1 = seq(from = grange[1,1], to = grange[2,1], length = n)
#   x2 = seq(from = grange[1,2], to = grange[2,2], length = n)
#   expand.grid(X1 = x1, X2 = x2)
```

### Random Forest 
```{r}
library(randomForest)
require(caTools)

head(dataframe1)
summary(dataframe1)
sapply(dataframe1,class)

# Set portion of data aside for testing/training
sample_RF <- sample.split(dataframe1$copdcaco, SplitRatio = .75)
train_RF <- subset(dataframe1, sample_RF == TRUE)
test_RF <- subset(dataframe1, sample_RF == FALSE)
train_RF
test_RF


RF <- randomForest(
  copdcaco ~ .,
  data=train_RF
)

# By default, the number of decision trees in the forest is 500 and the number of features used as potential candidates for each split is 3. The model will automatically attempt to classify each of the samples in the Out-Of-Bag dataset and display a confusion matrix with the results.


# Now, we use our model to predict whether the RF can categorise people into COPD cases and controls 
pred_RF <- predict(RF, newdata=test_RF[-234])

#Since this is a classification problem, we use a confusion matrix to evaluate the performance of our model. Recall that values on the diagonal correspond to true positives and true negatives (correct predictions) whereas the others correspond to false positives and false negatives.
confusionmatrix_RF <- table(test_RF[,234], pred_RF)
confusionmatrix_RF

# accuracy of model
(6+3)/(17)
```
# Association between resistome and bacteriome 
## Procrustes
```{r}
# I will use Procrustes plot to look at the association between the resistome (90% ID clustering) and the bacteriome (species level?)
# Procrustes compares different ordinations of the data
microbiome.ps <- readRDS("Input_files//ps_COPD_d2_f_01%in2samples_08APR2021")
# Inspect WvK's ps object 
microbiome.ps

# There are 356 samples in total, I have only 72 so I need to create a vector of my sample IDs and then prune the microbiome ps to just include these samples 
# create vector of my sample IDs 
vector.sampleIDs <- c('10279','10839','11143','11502','11588','11814','12927','12965','12980','13571','13743','13764','13908','14587','15035','15391','15446','15573','16236','16716','16994','17053','17918','18439','18859','19586','20148','20264','20278','20407','21130','21167','21397','21548','21696','27187','27446','27641','27984','28038','29117','29401','29593','29745','30113','30148','30256','30491','30972','31465','31516','31563','31637','32372','32530','33076','33579','35287','35537','36022','36166','36424','36503','36845','37083','37250','38249','38407','38776','veldbl16','veldbl3','veldbl5')

vector.OP.time <- "T1"

# As there are multiple time points where samples were taken from each participant, firstly I should prune to just have T1 (as all participants have sample taken at T1 (actually T0))
# Use prune_samples to select only these T1 samples from the microbiome dataset 
microbiome.ps.T1 <- prune_samples(microbiome.ps@sam_data$OP_time %in% vector.OP.time, microbiome.ps)
# Now I prune the ps object to just my sample IDs from the resistome data
microbiome.ps.final.noblanks <- prune_samples(microbiome.ps.T1@sam_data$ID %in% vector.sampleIDs, microbiome.ps.T1)
microbiome.ps.final.noblanks  # this ps object has only 68 samples
# check which sample is missing from the microbiome data that I have in the resistome data
microbiome.ps.final.noblanks@sam_data$ID # Sample ID 19586 was filtered out of the microbiome ps object as it had 0 reads (WvK checked this)

# Check to see what ARGs were present in this sample 
get_taxa(ARGcluster.aggl, "19586") # There are some ARGs detected in this sample but for the procrustes analysis we will also remove this from the resistome data set
# I need to now remove sample 19586 from my resistome ps object - use 90% clustered ps with no blanks
Resistome.ps <- prune_samples(sample_names(ARGcluster.aggl) != "19586", ARGcluster.aggl) 
blancs <- c("veldbl16", "veldbl3", "veldbl5")
Resistome.ps.noblanks <- prune_samples(!sample_names(Resistome.ps)%in%blancs, Resistome.ps)
# Now there are 68 samples in my resistome dataset as I have now removed sample 19586 from it

Resistome.ps.noblanks;microbiome.ps.final.noblanks
# Both ps objects now have 68 samples which match

# Now agglomerate the microbiome data to genus level 
# Using the functions created by AB, I will now agglomerate the microbiome data to genus level - i.e. merge ASVs which are in the same genus  
# I will therefore create a new ps object with genus level as default
# print the available taxonomic ranks
colnames(tax_table(microbiome.ps.final.noblanks))
row.names(microbiome.ps.final.noblanks@sam_data)
# how many ARGs are there before agglomeration in original ps object? 
microbiome.ps.final.noblanks # 1092 taxa (ASVs)
# Tax_glom2 function as created by AB - this is previously saved in the global environment from above
# Use function tax_glom2 - 6 is the Genus level in the tax_table of microbiome.ps.final.noblanks object. This creates a new ps object with genus level as default 
microbiome.ps.genus <- tax_glom2(microbiome.ps.final.noblanks, taxrank=rank_names(microbiome.ps.final.noblanks)[6], NArm=TRUE)
taxa_names(microbiome.ps.genus)
microbiome.ps.genus
# There are now just 91 taxa (genus level), instead of 1092 individual ASVs
# The 'taxa_names' are not the genus level names - instead these are the ASV names
# Check tax-table of ps object
tax_table(microbiome.ps.genus) # Great - all 'species level' data is now NA so the lowest level in the tax_table is Genus level 
# How many taxa before/after agglomeration?
ntaxa(microbiome.ps.final.noblanks); ntaxa(microbiome.ps.genus)
# 91 Genus level bacteria vs 1092 species level bacteria 
microbiome.ps.genus@tax_table[,6]
head(microbiome.ps.genus@otu_table)
taxa_names(microbiome.ps.genus)

# HELLINGER TRANSFORMATION (of microbiome data)
# Hellinger transform the resistome and microbiome otu tables - using the definition of the Hellinger transform given by Equation (13) in Legendre and Gallagher (2001) DOI: 10.1007/s004420100716---convert the data to proportions and then take the square root
# vegan 'decostand()' function Hellinger transforms the data
# Microbiome data:
microbiome.ps.genus.hell<- microbiome.ps.genus
otu_table(microbiome.ps.genus.hell) <-otu_table(decostand(otu_table(microbiome.ps.genus.hell), method = "hellinger"), taxa_are_rows=FALSE)


# PCoA  
# Firstly ordinate the data 
microbiome.PCoA <- ordinate( microbiome.ps.genus.hell, method="PCoA", distance="bray")
microbiome.PCoA.plot <- plot_ordination( microbiome.ps.genus.hell, microbiome.PCoA, color="copdcaco", shape="copdcaco" ) +
    ggtitle("MICROBIOME: PCoA on Bray-Curtis distances, hellinger transformed data") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(microbiome.ps.genus.hell@sam_data),hjust=-0.3, vjust=-0.5 ) )+
  stat_ellipse()
microbiome.PCoA.plot
# Extract the data from the PCoA plot for use in the procrustes analysis
microbiome.procrustes.PCoA.data <- microbiome.PCoA.plot$data[, 1:2]
# Get rid of T1 from ID name so that it matches the resistome data
rownames(microbiome.procrustes.PCoA.data)
rownames(microbiome.procrustes.PCoA.data) <- stringr::str_replace(rownames(microbiome.procrustes.PCoA.data), "T\\d", replacement = "")
rownames(microbiome.procrustes.PCoA.data) #'T1' has now been removed from the row names

# PCoA of resistome data 
resistome.PCoA<- ordinate( Resistome.ps.noblanks, method="PCoA", distance="bray")
resistome.PCoA.plot <- plot_ordination( Resistome.ps.noblanks, resistome.PCoA, color="copdcaco", shape="copdcaco" ) +
    ggtitle("RESISTOME: PCoA on Bray-Curtis distances") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(Resistome.ps.noblanks@sam_data),hjust=-0.3, vjust=-0.5 ) )+
   stat_ellipse()
resistome.PCoA.plot
# Extract the data from the PCoA plot for use in the procrustes analysis
resistome.procrustes.PCoA.data <- resistome.PCoA.plot$data[, 1:2]



# Run procrustes
procrustes.PCoA.symm <- procrustes(microbiome.procrustes.PCoA.data, resistome.procrustes.PCoA.data, 
                                          symmetric = TRUE, 
                                          permutations = 9999)

# Results of procrustes
summary(procrustes.PCoA.symm)
print(procrustes.PCoA.symm)

# Plot procrustes
# Kind 1 gives a visual indication of the degree of match between the two ordinations. Symbols or labels show the position of the samples in the first ordination, and arrows point to their positions in the target ordination. The plot also shows the rotation between the two ordinations necessary to make them match as closely as possible.
plot(procrustes.PCoA.symm,
     kind = 1,
     main = "Procrustes plot - PCoA microbiome/resistome",
     xlab = "PCoA 1",
     ylab = "PCoA 2")

# Kind 2 plots show the residuals for each sample. This allows identification of samples with the worst fit. The horizontal lines, from bottom to top, are the 25% (dashed), 50% (solid), and 75% (dashed) quantiles of the residuals.
plot(procrustes.PCoA.symm, kind =2, main= "Procrustes errors - PCoA microbiome/resistome")

# Significance testing using 'protest' - this is a permutational test of the significance of the procrustes result based on the correlation from a symmetric procrustes analysis 
protest(microbiome.procrustes.PCoA.data, resistome.procrustes.PCoA.data, scores = "sites", permutations = 9999) # correlation coefficient = 0.3477, p= 0.0006

# Randomising the procrustes so that the microbiome and resistome samples are now no longer linked - correlation coefficients should be low but this is a good check - good to compare the correlation with that seen in the linked procrustes analysis 
# Only need to 'randomise' one of the procrustes' input datasets (here I do microbiome)
microbiome.PCoA.random <- microbiome.procrustes.PCoA.data
row.names(microbiome.PCoA.random)
row.names(microbiome.PCoA.random) <- sample(row.names(microbiome.PCoA.random))
microbiome.PCoA.random <- microbiome.PCoA.random[ match( row.names(resistome.procrustes.PCoA.data), row.names(microbiome.PCoA.random) ), ]

procrustes.PCoA.randomised <- procrustes(microbiome.PCoA.random, resistome.procrustes.PCoA.data, symmetric = TRUE, permutations= 9999)

# Results of random procrustes
summary(procrustes.PCoA.randomised)
print(procrustes.PCoA.randomised)

# Plot procrustes
# Kind 1 gives a visual indication of the degree of match between the two ordinations. Symbols or labels show the position of the samples in the first ordination, and arrows point to their positions in the target ordination. The plot also shows the rotation between the two ordinations necessary to make them match as closely as possible.
plot(procrustes.PCoA.randomised,
     kind = 1,
     main = "Procrustes plot randomised - PCoA microbiome/resistome",
     xlab = "PCoA 1",
     ylab = "PCoA 2")

# Kind 2 plots show the residuals for each sample. This allows identification of samples with the worst fit. The horizontal lines, from bottom to top, are the 25% (dashed), 50% (solid), and 75% (dashed) quantiles of the residuals.
plot(procrustes.PCoA.randomised, kind =2)
# Significance testing using 'protest' - this is a permutational test of the significance of the procrustes result based on the correlation from a symmetric procrustes analysis 
protest(microbiome.PCoA.random, resistome.procrustes.PCoA.data, scores = "sites", permutations = 9999) # correlation coefficient = 0.1655 , p=0.2805  (non-significant)- as expected, when we compare the resistome and microbiome of random samples within this data, we do not see a correlation between resistome and microbiome (p>0.05 )


#GGPLOT FOR PROCRUSTES PLOTS - see tutorial https://stackoverflow.com/questions/30325739/ggplot2-for-procrustes-rotation-in-vegan 
# Create df with copd/control status
# PCoA procrustes
PCoA.pro.df <- data.frame(rda1=procrustes.PCoA.symm$Yrot[,1], rda2=procrustes.PCoA.symm$Yrot[,2], xrda1 = procrustes.PCoA.symm$X[,1], xrda2 = procrustes.PCoA.symm$X[,2])
PCoA.pro.df$copdcontrol <- microbiome.ps.genus.hell@sam_data$copdcaco
# Convert copdcaco to a factor with custom labels
PCoA.pro.df$copdcontrol <- factor(PCoA.pro.df$copdcontrol, levels = c(0, 1), labels = c("Control", "COPD"))


# Create the ggplot
PCoA.pro.plot <- ggplot(PCoA.pro.df) +
  geom_point(aes(x=rda1, y=rda2, colour=copdcontrol, shape="Microbiome", fill=copdcontrol), size = 4) +
  geom_point(aes(x=xrda1, y=xrda2, colour=copdcontrol, shape="Resistome", fill=copdcontrol), size = 4) +
  geom_segment(aes(x=rda1, y=rda2, xend=xrda1, yend=xrda2, colour=copdcontrol), arrow=arrow(length=unit(0.2, "cm")))

# Reverse the colors and set shape and fill mapping
PCoA.pro.plot + 
  scale_colour_manual(values = c("cornflowerblue", "coral1")) +  # Define the color mapping
  scale_shape_manual(values = c("Microbiome" = 21, "Resistome" = 24)) +  # Define the shape mapping
  scale_fill_manual(values = c("Control" = "cornflowerblue", "COPD" = "coral1")) +  # Define the fill mapping
  labs(title = "PCoA procrustes", y = "PCo2", x = "PCo1", subtitle = "Correlation: 0.35, Significance: 6e-04") +
  guides(fill = guide_legend(override.aes = list(shape = NA)))  # Remove the contents of the legend







# Random PCoA procrustes
PCoA.pro.r.df <- data.frame(rda1=pro.PCoA.r$Yrot[,1], rda2=pro.PCoA.r$Yrot[,2], xrda1 = pro.PCoA.r$X[,1], xrda2 = pro.PCoA.r$X[,2])
PCoA.pro.r.df$copdcontrol <- copdcaco.df.noblanks.no19586$COPD.control.blank
# Create the ggplot 
PCoA.pro.plot.r <- ggplot(PCoA.pro.r.df) +
geom_point(aes(x=rda1, y=rda2, colour=copdcontrol)) +
geom_point(aes(x=xrda1, y=xrda2, colour=copdcontrol)) +
geom_segment(aes(x=rda1,y=rda2,xend=xrda1,yend=xrda2,colour=copdcontrol),arrow=arrow(length=unit(0.2,"cm")))
PCoA.pro.plot.r + labs(title = "PCoA procrustes - randomised microbiome", y="PCo2", x = "PCo1", subtitle = "Correlation: 0.17, Significance: 0.28")





```

## Co-occurrence network (1st attempt)
```{r}
# Used to explore connections in microbial communities 
# Ordinations can uncover temporally dynamic correlations between the overall composition of AMR genes and microbiome, network analysis can be used to identify associations between specific genomic features
# Following the tutorial from: https://medium.com/analytics-vidhya/how-to-create-co-occurrence-networks-with-the-r-packages-cooccur-and-visnetwork-f6e1ceb1c523 
# Install the required packages 
install.packages("cooccur") # to calculate pairwise co-occurrences 
install.packages("visNetwork") # to visualise network 
library("cooccur")
library("visNetwork")

# tutorial says that co-occurrence begins with presence-absence data - therefore maybe I need to convert my microbiome and resistome data to PA data? 
# Firstly I will create a new data matrix with both the microbiome and resistome data in one - sample ID as the columns and ARGs and ASVs as the rows
# Remove the label 'T1' from the microbiome ps object so that it matches up with the resistome ps object
microbiome.ps.genus.cooccur<- microbiome.ps.genus
microbiome.ps.genus.cooccur@otu_table
rownames(microbiome.ps.genus.cooccur@otu_table)
rownames(microbiome.ps.genus.cooccur@otu_table) <- stringr::str_replace(rownames(microbiome.ps.genus.cooccur@otu_table), "T\\d", replacement = "")
rownames(microbiome.ps.genus.cooccur@otu_table) #'T1' has now been removed from the row names 
# I join the otu table of the microbiome data with the otu table of the resistome data 
microbiome.ps.genus.cooccur@otu_table
Resistome.ps.noblanks@otu_table
merged.micro.resis.table <- cbind(microbiome.ps.genus.cooccur@otu_table, Resistome.ps.noblanks@otu_table)
head(merged.micro.resis.table)
# Now convert these abundances to presence absence data 
merged.micro.resis.table.PA <- merged.micro.resis.table
merged.micro.resis.table.PA[merged.micro.resis.table.PA>0] <- 1
merged.micro.resis.table.PA

# Resistome cluster names are not the actual cluster names (as before) - manually change these in the matrix
# Microbiome ASV names should be changed to the genus names and the ARG names should be changed to the cluster names 
colnames(merged.micro.resis.table.PA)
colnames(merged.micro.resis.table.PA) <- c('Actinomyces','F0332','Alloscardovia','Bifidobacterium','Parascardovia','Scardovia','Corynebacterium','Rothia','Propionibacterium','Atopobium','Olsenella','Cryptobacterium','Phocaeicola','F0058','Porphyromonas','Alloprevotella','Prevotella','Rikenellaceae RC9 gut group','Tannerella','Capnocytophaga','Bergeyella','Chryseobacterium','Lentimicrobium','Campylobacter','Bacillus','Solobacterium','Abiotrophia','Alloiococcus','Granulicatella','Enterococcus','Lactobacillus','Listeria','Lactococcus','Streptococcus','Mycoplasma','Gemella','Staphylococcus','Defluviitaleaceae UCG-011','Butyrivibrio','Catonella','Johnsonella','Lachnoanaerobaculum','Oribacterium','Shuttleworthia','Stomatobaculum','Peptococcus','[Eubacterium] brachy group','[Eubacterium] nodatum group','[Eubacterium] saphenum group','Amnipila','Family XIII UCG-001','Mogibacterium','[Eubacterium] yurii group','Filifactor','Peptoanaerobacter','Peptostreptococcus','Centipeda','Selenomonas','Anaeroglobus','Dialister','Megasphaera','Veillonella','Fusobacterium','Leptotrichia','Oceanivirga','Streptobacillus','Candidatus Saccharimonas','TM7a','TM7x','Afipia','Lautropia','Ralstonia','Alysiella','Conchiformibius','Eikenella','Kingella','Neisseria','Simonsiella','Cardiobacterium','Escherichia-Shigella','Salmonella','Erwinia','Actinobacillus','Aggregatibacter','Haemophilus','Faucicola','Moraxella','Pseudomonas','Stenotrophomonas','Treponema','Fretibacterium',"aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

# Check the renaming has been performed correctly
list(colnames(merged.micro.resis.table.PA)) # there are 176 columns which is correct (85 ARG clusters and 91 ASVs)

# cocooccur() function works with rows as species and columns as samples - therefore I need to transpose my current matrix
transposed.matrix.micro.resis.table <- t(merged.micro.resis.table.PA)

# Using the cooccur() function - this will calculate the p-values associated with pairwise co-occurrences and return a list of class ‘cooccur’. Printing this class returns only the significant interactions. We’ll store the significant interactions to create our edge list for visNetwork.
co.occurrence.table <- print(cooccur(transposed.matrix.micro.resis.table))
# This gives us a table with the co-occurrences between all the ARGs and ASVs, but is difficult to interpret as no gene names are in the table - I should therefore add these in 
rownames(transposed.matrix.micro.resis.table)[co.occurrence.table$sp1]
rownames(transposed.matrix.micro.resis.table)[co.occurrence.table$sp2]

co.occurrence.table <- print(cooccur(transposed.matrix.micro.resis.table, spp_names = T))
# This table now includes 2 extra columns which also show us the AS/ARG names 
# This table only contains significant interactions (there are 280 rows)

# Network visualisation - we plot the co-occurrence network using 'visNetwork' package 
# "the visNetwork() function takes two arguments: 1) a data frame describing the nodes in the network, and 2) a data frame describing the edges in the network."
# Starting with nodes df - this df needs an ID column (ARGs and ASV names)
# Create a data frame of the nodes in the network. 
# We’ll set our ids to match the numeric labels returned with cooccur (1–13). In addition, we’ll label our nodes by species name, specify a color, and add shadow for some depth.

require('RColorBrewer')
                     
colour.nodes <- list(transposed.matrix.micro.resis.table = c('ASV12'= "#000000",'ASV1563'= "#000000",'ASV269'= "#000000",'ASV61'= "#000000",'ASV1152'= "#000000",'ASV736'= "#000000",'ASV758'= "#000000",'ASV13'= "#000000",'ASV988'= "#000000",'ASV36'= "#000000",'ASV1517'= "#000000",'ASV1015'= "#000000",'ASV1782'= "#000000",'ASV751'= "#000000",'ASV91'= "#000000",'ASV93'= "#000000",'ASV8'= "#000000",'ASV1656'= "#000000",'ASV353'= "#000000",'ASV318'= "#000000",'ASV315'= "#000000",'ASV668'= "#000000",'ASV2265'= "#000000",'ASV37'= "#000000",'ASV106'= "#000000",'ASV301'= "#000000",'ASV241'= "#000000",'ASV1929'= "#000000",'ASV22'= "#000000",'ASV176'= "#000000",'ASV700'= "#000000",'ASV135'= "#000000",'ASV96'= "#000000",'ASV1'= "#000000",'ASV1236'= "#000000",'ASV16'= "#000000",'ASV122'= "#000000",'ASV1802'= "#000000",'ASV199'= "#000000",'ASV123'= "#000000",'ASV50'= "#000000",'ASV40'= "#000000",'ASV42'= "#000000",'ASV1631'= "#000000",'ASV64'= "#000000",'ASV499'= "#000000",'ASV254'= "#000000",'ASV133'= "#000000",'ASV1029'= "#000000",'ASV1371'= "#000000",'ASV1319'= "#000000",'ASV201'= "#000000",'ASV971'= "#000000",'ASV325'= "#000000",'ASV1068'= "#000000",'ASV92'= "#000000",'ASV502'= "#000000",'ASV14'= "#000000",'ASV356'= "#000000",'ASV1382'= "#000000",'ASV51'= "#000000",'ASV3'= "#000000",'ASV23'= "#000000",'ASV9'= "#000000",'ASV498'= "#000000",'ASV1638'= "#000000",'ASV213'= "#000000",'ASV2300'= "#000000",'ASV31'= "#000000",'ASV1052'= "#000000",'ASV173'= "#000000",'ASV1714'= "#000000",'ASV1325'= "#000000",'ASV1706'= "#000000",'ASV1159'= "#000000",'ASV568'= "#000000",'ASV26'= "#000000",'ASV1346'= "#000000",'ASV1300'= "#000000",'ASV137'= "#000000",'ASV130'= "#000000",'ASV1597'= "#000000",'ASV98'= "#000000",'ASV420'= "#000000",'ASV17'= "#000000",'ASV1211'= "#000000",'ASV524'= "#000000",'ASV807'= "#000000",'ASV1732'= "#000000",'ASV425'= "#000000",'ASV302'= "#000000","aac(3)-II_clust"= "#00ff00","aac(3)-Iva"= "#00ff00","aac(6')-aph(2'')_clust"= "#00ff00","aadA_ant(3'')-Ia_clust"= "#00ff00","aadA_clust1"= "#00ff00","aadD"= "#00ff00","ant(6)-Ia_clust2"= "#00ff00","aph(3'')-Ib"= "#00ff00","aph(3')-Ia_aph(3')-Ic"= "#00ff00","aph(3')-Ib"= "#00ff00","aph(3')-III"= "#00ff00","aph(4)-Ia"= "#00ff00","aph(6)-Id"= "#00ff00","str"= "#00ff00","blaACT_clust"= "#00ff00","blaACT_CMG_MIR_clust"= "#00ff00","blaBRO"= "#00ff00","blaCARB_clust2"= "#00ff00","blaCTX-M_clust1"= "#00ff00","blaOXA-22"= "#00ff00","blaOXA-395_clust"= "#00ff00","blaOXA-60_clust"= "#00ff00","blaOXA-85"= "#00ff00","blaOXA_clust19"= "#00ff00","blaOXA_clust3"= "#00ff00","blaOXA_clust8"= "#00ff00","blaOXA_clust9"= "#00ff00","blaOXY_clust1"= "#00ff00","blaSPU-1"= "#00ff00","blaTEM_clust"= "#00ff00","blaZ_clust"= "#00ff00","cfxA_clust"= "#00ff00","mecA_clust"= "#00ff00","mecA1"= "#00ff00","penA"= "#00ff00","fusB"= "#00ff00","lsa(A)"= "#00ff00","lsa(C)"= "#00ff00","erm(A)_2_AF002716"= "#00ff00","erm(B)_clust"= "#00ff00","erm(C)_clust"= "#00ff00","erm(F)_clust"= "#00ff00","erm(T)_4_AJ488494"= "#00ff00","erm(X)_clust"= "#00ff00","lnu(C)"= "#00ff00","mdf(A)"= "#00ff00","mef(A)-3"= "#00ff00","mef(A)_clust"= "#00ff00","mph(A)"= "#00ff00","mph(C)"= "#00ff00","msr(A)"= "#00ff00","msr(D)"= "#00ff00","msr(E)"= "#00ff00","vga(A)_clust"= "#00ff00","cat(pC194)"= "#00ff00","cat_2"= "#00ff00","catA1"= "#00ff00","catQ"= "#00ff00","catS"= "#00ff00","cml_clust"= "#00ff00","cmx"= "#00ff00","cfr(C)"= "#00ff00","sul1"= "#00ff00","sul2"= "#00ff00","tet(32)"= "#00ff00","tet(33)"= "#00ff00","tet(37)"= "#00ff00","tet(39)"= "#00ff00","tet(40)"= "#00ff00","tet(44)"= "#00ff00","tet(A)"= "#00ff00","tet(B)"= "#00ff00","tet(C)"= "#00ff00","tet(G)"= "#00ff00","tet(K)"= "#00ff00","tet(L)_clust1"= "#00ff00","tet(M)"= "#00ff00","tet(O)"= "#00ff00","tet(O/32/O)"= "#00ff00","tet(O/W/32/O/W/O)"= "#00ff00","tet(O/W/O)-1"= "#00ff00","tet(Q)"= "#00ff00","tet(T)"= "#00ff00","tet(W)"= "#00ff00","dfrA15_clust"= "#00ff00"))

nodes2 <- data.frame(id = 1:nrow(transposed.matrix.micro.resis.table),
                    label = rownames(transposed.matrix.micro.resis.table),
                    color = c("#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#000000","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00","#00ff00"),
                    shadow = TRUE)

#Inspect nodes to ensure correct labelling 
nodes

# Edges
#"The edges data frame needs at least two columns, ‘from’ and ‘to’, which should correspond with the ids from the nodes data frame. Since we used cooccur’s numeric labels as our ids, we can use sp1 as our ‘from’ column and ‘sp2’ as our to column. (Since our network isn’t directed, we could reverse these and it wouldn’t matter.) We’ll then add some color so our edges match our nodes such that we’ll have a lighter color for co-occurrences that occur at a lower frequency than expected and a darker color for co-occurrences that occur at a higher frequency. To make the distinction between ‘higher’ and ‘lower’ even more obvious, we’ll also specify that co-occurrences that are lower than expected have a dashed line.
# Create an edges dataframe from the significant pairwise co-occurrences.
edges <- data.frame(from = co.occurrence.table$sp1, to = co.occurrence.table$sp2,
      color = ifelse(co.occurrence.table$p_lt <= 0.05, "#000000", "#FF0000"),
      dashes = ifelse(co.occurrence.table$p_lt <= 0.05, TRUE, FALSE))
edges
# Dashed lines mean that the correlation has a p-value > 0.05, red solid lines mean that the correlation has a p-value <0.05 

# Now putting the nodes and edges together - using a layout from the ‘igraph’ package. "You don’t need to load ‘igraph’ to have access to these layouts...Specifically, we’ll use the Kamada-Kawai layout algorithm but I encourage you to play around with the layouts to see which one you like best." (https://medium.com/analytics-vidhya/how-to-create-co-occurrence-networks-with-the-r-packages-cooccur-and-visnetwork-f6e1ceb1c523)
# Plot.
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_with_kk")

# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_with_lgl")

# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_as_star")

# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_nicely")


# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_on_grid")

# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_with_mds")

# try different visgraph layout
visNetwork(nodes = nodes2, edges = edges) %>%
    visIgraphLayout(layout = "layout_with_sugiyama")


# Figure out which is the most significant co-occurrence
# When p_gt is low this means that there is a significant co-occurrence therefore I shoudl order them from lowest to highest p_gt value 
library(dplyr)
top10co.occurrences <- top_n(co.occurrence.table, 10, -p_gt)
#This is the top 10 highest co-occurrences between ARGs/ASVs
# Most significant positive co-occurrence  is between ASV36 and tet(O)
# I will see what happens when I focus in on tet(O)


nodes3 <- data.frame(id = c("ASV64", "ASV36","ASV93","ASV96", "ASV241", "tet(O)", "tet(37)"),
                    label = c("ASV64", "ASV36","ASV93","ASV96", "ASV241", "tet(O)", "tet(37)"),
                    shadow = TRUE)

edges3 <- data.frame(from = co.occurrence.table$sp1_name, to = co.occurrence.table$sp2_name)
edges3
edges3$from[,"tet(O)"]


visNetwork(nodes = nodes3, edges = edges3) %>%
    visIgraphLayout(layout = "layout_with_kk")

# Now I want to change the ASV names to more meaningful ones from the taxonomy table 
taxtable.microbiome <- as.data.frame(tax_table(microbiome.ps.genus.cooccur))
taxtable.microbiome$Genus

install.packages('writexl')
install.packages('openxlsx')
library(writexl)
library(openxlsx)

taxtable.microbiome$ASV <- rownames(taxtable.microbiome)
write_xlsx(taxtable.microbiome, 'microbiome_taxtable2.xlsx')


doby.xtabs(co.occurrence.table)
```

## Co-occurrence network (AB method)
```{r}
# Investigating bacteria-bacteria and ARG-ARG co-occurrences and bacteria-ARG co-occurrences. 
# Creating these networks gives me an idea of whether there is co-selection/co-occurrence or competition between bacteria and ARGs. 
# I will assess whether there are positive or negative co-occurences between certain ARGs or bacteria. 
# I will use 90% clustered ARGs (Relative rarefied, gene length corrected and 16S qPCR count corrected), and bacterial taxa on the genus level (not yet filtered but may need to do)

Resistome.ps.noblanks # This ARG PS object is 90% identity clustered, 68 samples because of the missing sample 19586 from the microbiome data (therefore also removed from this resistome ps data) - data in this ps object is Relative rarefied, gene length corrected and 16S qPCR count corrected. 
microbiome.ps.genus # This microbiome PS object is agglomerated at the genus level, 68 samples matching that in the resistome PS (not yet filtered but may need to do).
# Microbiome data has been prevalence/abundance filtered - taxa are kept if their relative abundance is > 0.1% (proportion = 0.001) (abundance) in more than 2 samples (prevalence)

# I need to modify both of the above PS objects before analysis 
# Resistome ps
# I need to rename the ARGs by their cluster name rather than the individual ARG name 
Resistome.ps.noblanks.names <- Resistome.ps.noblanks
colnames(Resistome.ps.noblanks.names@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

# Get rid of T1 from ID name so that it matches the resistome data
microbiome.ps.genus.matched <- microbiome.ps.genus
rownames(microbiome.ps.genus.matched@otu_table)
rownames(microbiome.ps.genus.matched@otu_table) <- stringr::str_replace(rownames(microbiome.ps.genus@otu_table), "T\\d", replacement = "")
rownames(microbiome.ps.genus.matched@otu_table) #'T1' has now been removed from the ps row names (sample IDs)

# Rename the genus level labels - currently ASV names are used but I need to manually change this to the genus level names in the ps object 
microbiome.ps.genus.matched.names <- microbiome.ps.genus.matched
colnames(microbiome.ps.genus.matched.names@otu_table) <- c('Actinomyces','F0332','Alloscardovia','Bifidobacterium','Parascardovia','Scardovia','Corynebacterium','Rothia','Propionibacterium','Atopobium','Olsenella','Cryptobacterium','Phocaeicola','F0058','Porphyromonas','Alloprevotella','Prevotella','Rikenellaceae RC9 gut group','Tannerella','Capnocytophaga','Bergeyella','Chryseobacterium','Lentimicrobium','Campylobacter','Bacillus','Solobacterium','Abiotrophia','Alloiococcus','Granulicatella','Enterococcus','Lactobacillus','Listeria','Lactococcus','Streptococcus','Mycoplasma','Gemella','Staphylococcus','Defluviitaleaceae UCG-011','Butyrivibrio','Catonella','Johnsonella','Lachnoanaerobaculum','Oribacterium','Shuttleworthia','Stomatobaculum','Peptococcus','[Eubacterium] brachy group','[Eubacterium] nodatum group','[Eubacterium] saphenum group','Amnipila','Family XIII UCG-001','Mogibacterium','[Eubacterium] yurii group','Filifactor','Peptoanaerobacter','Peptostreptococcus','Centipeda','Selenomonas','Anaeroglobus','Dialister','Megasphaera','Veillonella','Fusobacterium','Leptotrichia','Oceanivirga','Streptobacillus','Candidatus Saccharimonas','TM7a','TM7x','Afipia','Lautropia','Ralstonia','Alysiella','Conchiformibius','Eikenella','Kingella','Neisseria','Simonsiella','Cardiobacterium','Escherichia-Shigella','Salmonella','Erwinia','Actinobacillus','Aggregatibacter','Haemophilus','Faucicola','Moraxella','Pseudomonas','Stenotrophomonas','Treponema','Fretibacterium')

#' Calculate the co-occurrence of taxa (pair-wise) between one or two tax tables
#' 
#' Alex adapted to fit two different matrices to test co-occurrence to
#'
#' Inspired on Jin Choi's within set algorithm plain R one set
#' Inspired by phylosmith package doing co-occurrence on ps objects but within one object
#' 
#' Method details:
#'    Using Spearman's rank (default), pearson or kendall tau as methods.
#' 
#'    Requires one or two phyloseq objects (subset where appropriate first)
#'      - Providing one ps will calculate WITHIN the ps
#'      - Providing two PS objects will calculate BETWEEN
#' 
#'    Provide filtering cutoffs for rho and p
#' 
#'    By default it will filter correlations where one or both columns are only zero (filt_zero)
#' 
#'    Verbose just prints a % text line to monitor progress.
#' 
#' @param ps_x phyloseq object X (obligatory)
#' @param ps_y phyloseq object Y (optional)
#' @param method method to use (spearman default). Basically all methods corr.test supports.
#' @param alternative (optional) - alternative indicates the alternative hypothesis and must be one of "two.sided", "greater" or "less". "greater" corresponds to positive association, "less" to negative association.
#' @param exact (optional) default FALSE - a logical indicating whether an exact p-value should be computed. Used for Kendall's \(\tau\) and Spearman's \(\rho\).
#' @param adjust default Bejamini-Hochberg (BH). Any method from p.adjust function should work. Provide "none" for raw.
#' @param rho.filt default 0.6
#' @param p.filt filter adjusted p value. default 0.01. if raw p use adjust method "none"
#' @param filt_zeo default TRUE 
#' @param verbose default TRUE
#'
#' @returns Co-correlation data frame of all taxa (including self-self)
#'
#' @author Alex Bossers \email{a.bossers@uu.nl} 
#' 

ps_co_occurrence <- function( ps_x='', ps_y='', 
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.6, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 
{
  # check input params
  if ( is(ps_x,"phyloseq") ) {
    a = ps_x
  } else {
    stop("Phyloseq object X required!")
  }
  
  # Check if co-occurrence on itself or on two different tables
  if ( is( ps_y, "phyloseq" ) ) {
    b = ps_y
    n.exclude = 0
    cat("Phyloseq Y provided => all vs all taxa (incl self-self complete matrix)\n")
  } else {
    b = ps_x
    n.exclude = 1 # counter to avoid self-self
    cat("Phyloseq Y NOT provided => self all vs all taxa excluding self-self (half matrix)\n")
  }
  
  a.n <- ntaxa(a)
  b.n <- ntaxa(b)
  
  a.otu <- data.frame( otu_table(a) )
  b.otu <- data.frame( otu_table(b) )
  
  # Check that samples are the same
  if ( ! all( row.names(a.otu) == row.names(b.otu) ) ) {
    stop("Sample rows are not equal")
  }
  
  results <- data.frame()
  ps_y_from <- 1
  
  for( x in 1:(a.n - n.exclude) ) {
    if(n.exclude) {
      ps_y_from <- x + n.exclude
    }
    for( y in ps_y_from:b.n ) {
      
      # track abundance and test if zero
      species1.ab <- sum( a.otu[,x] )
      species2.ab <- sum( b.otu[,y] )
      
      test <- cor.test( a.otu[,x], b.otu[,y], method="spearman", alternative=alternative, exact=exact, na.action=na.rm )
      rho<-test$estimate
      p.value<-test$p.value
      
      if ( filt_zero & ( species1.ab <= 0 | species2.ab <= 0 ) ) {
        rho <- 0
        p.value <- 1
      }
      
      # if results meet filters add to df
      if( abs(rho) >= rho.filt & p.value <= p.filt ) {
        new.row <- data.frame( "taxa.X"=taxa_names(a)[x], "taxa.Y"=taxa_names(b)[y], rho, p.value, species1.ab, species2.ab )
        results <- rbind( results, new.row )
      }
    }
    if(verbose) { cat( paste( round( x / a.n * 100), "% Done of dataset.\n")) }
  }
  
  # apply multiple testing correction
  results$p.adjust <- p.adjust( results$p.value, method=adjust )
  cat("\nMultiple testing correction method '",adjust,"' applied\n")
  #filter again
  results <- results[ results$p.adjust <= p.filt, ]
  
  return(results)
}

# Bacteria-Bacteria co-occurrence - Spearman correlations 
bact_bact_co_occurrence <- ps_co_occurrence(ps_x = microbiome.ps.genus.matched.names,
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.6, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# ARG-ARG Spearman correlations 
ARG_ARG_co_occurrence <- ps_co_occurrence(ps_x = Resistome.ps.noblanks.names,
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.6, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# Bacteria-ARG Spearman correlations
bact_ARG_co_occurrence <- ps_co_occurrence( Resistome.ps.noblanks.names, microbiome.ps.genus.matched.names, 
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.6, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# Combined bacteria-bacteria & bacteria-ARG
# Merge ARG-ARG and bacteria-ARG co-occurrence data frames
combined_co_occurrence <- rbind(ARG_ARG_co_occurrence, bact_ARG_co_occurrence)


# Now I will use iGraph to create the network based on these Spearman rank correlations
# Start with the bacteria-bacteria correlations 

# Create the network - bacteria-bacteria
#"The edges data frame needs at least two columns, ‘from’ and ‘to’, which should correspond with the ids from the nodes data frame. Since we used cooccur’s numeric labels as our ids, we can use sp1 as our ‘from’ column and ‘sp2’ as our to column. (Since our network isn’t directed, we could reverse these and it wouldn’t matter.) We’ll then add some color so our edges match our nodes such that we’ll have a lighter color for co-occurrences that occur at a lower frequency than expected and a darker color for co-occurrences that occur at a higher frequency. To make the distinction between ‘higher’ and ‘lower’ even more obvious, we’ll also specify that co-occurrences that are lower than expected have a dashed line.
# Create an edges dataframe from the significant pairwise co-occurrences.
edges_bac_bac <- data.frame(from = bact_bact_co_occurrence$taxa.X, to = bact_bact_co_occurrence$taxa.Y)
network_bact_bact <- graph_from_data_frame(d= edges_bac_bac , directed = F, vertices = NULL)
print(network_bact_bact, e=TRUE, v=TRUE)
plot(network_bact_bact) # default igraph plot of co-occurrence network

# Count the number of degree for each node (used for scaling of nodes in plot function)
deg.bac <- degree(network_bact_bact, mode="all")

network_graph_bact_bact <- 
  plot(
  network_bact_bact,
  layout = layout.fruchterman.reingold, 
  main = "Bacteria-Bacteria co-occurrence network",
  vertex.size = deg.bac*6,
  vertex.color = "lightgreen",
  vertex.frame.color = "darkgray",
  vertex.shape = "sphere",
  vertex.label = V(network_bact_bact)$name, # names of the nodes
  vertex.label.family = "sans",
  vertex.label.font = 4,            # Font of node names
  vertex.label.cex = 1, 
  vertex.label.dist = 0,
  vertex.label.color = "black",     # Color of node names
  color = ifelse(bact_bact_co_occurrence$rho <= 0,"#FF0000","#00FF00"), # if rho is negative then colour connecting line is red, if rho is positive then connecting line is green
  width = ifelse(bact_bact_co_occurrence$p.adjust <= 0.00001,5,1), # if p.adj is <0.00001 then width of edge is 5, if p.adj is > 0.00001 then width of edge is 1
  lty = ifelse(bact_bact_co_occurrence$p.adjust <= 0.00001, "solid", "dashed"),
  label = NA, # no edge labels wanted
  curved = TRUE) # instead of straight edges, use curved edges

# count number of nodes and edges in the network
vcount(network_bact_bact)
ecount(network_bact_bact)

# export edge list as csv 
as.data.frame(edges_bac_bac)
write.csv(as.data.frame(edges_bac_bac), "edges_bac_bac.csv")

# Export graph as graphml 
write.graph(network_bact_bact, "Output_files//Co-occurrence_networks//network_bact_bact_graphml_file.graphml", format= "graphml") # Now import this file into gephi for graph visualisation and creation of networks here

# Create the network - ARG-ARG
# Create an edges dataframe from the significant pairwise co-occurrences.
edges_ARG_ARG <- data.frame(from = ARG_ARG_co_occurrence$taxa.X, to = ARG_ARG_co_occurrence$taxa.Y) 

network_ARG_ARG <- graph_from_data_frame(d= edges_ARG_ARG , directed = F, vertices = NULL)
print(network_ARG_ARG, e=TRUE, v=TRUE)
plot(network_ARG_ARG)
# Count the number of degree for each node (used for scaling of nodes in plot function)
deg.ARG <- degree(network_ARG_ARG, mode="all")

network_graph_ARG_ARG<- 
  plot(
  network_ARG_ARG,
  layout = layout.fruchterman.reingold,
  main = "ARG-ARG co-occurrence network",
  vertex.size = deg.ARG*4,
  vertex.color = "pink",
  vertex.frame.color = "darkgray",
  vertex.shape = "sphere",
  vertex.label = V(network_ARG_ARG)$name, # names of the nodes
  vertex.label.family = "sans",
  vertex.label.font = 2,            # Font of node names
  vertex.label.cex = 1, 
  vertex.label.dist = 0,
  vertex.label.color = "black",     # Color of node names
  color = ifelse(ARG_ARG_co_occurrence$rho <= 0,"#FF0000","#00FF00"), # if rho is negative then colour connecting line is red, if rho is positive then connecting line is green
  width = 20, # if p.adj is <0.00001 then width of edge is 5, if p.adj is > 0.00001 then width of edge is 1
  lty = ifelse(ARG_ARG_co_occurrence$p.adjust <= 0.00001, "solid", "dashed"),
  label = NA, # no edge labels wanted
  curved = TRUE) # instead of straight edges, use curved edges

# count number of nodes and edges in the network
vcount(network_ARG_ARG)
ecount(network_ARG_ARG)

# Export graph as graphml 
write.graph(network_ARG_ARG, "Output_files//Co-occurrence_networks//network_ARG_ARG_graphml_file.graphml", format= "graphml") # Now import this file into gephi for graph visualisation and creation of networks here


# Create the network - bacteria-ARG
# Create an edges dataframe from the significant pairwise co-occurrences.
edges_bacteria_ARG <- data.frame(from = bact_ARG_co_occurrence$taxa.X, to = bact_ARG_co_occurrence$taxa.Y) 

network_bacteria_ARG <- graph_from_data_frame(d= edges_bacteria_ARG , directed = F, vertices = NULL)
print(network_bacteria_ARG, e=TRUE, v=TRUE)
plot(network_bacteria_ARG)
# Count the number of degree for each node (used for scaling of nodes in plot function)
deg.bac.ARG <- degree(network_bacteria_ARG, mode="all")

network_graph_bact_ARG <- 
  plot(
  network_bacteria_ARG,
  layout = layout.fruchterman.reingold,
  main = "bacteria-ARG co-occurrence network",
  vertex.size = deg.ARG*6,
  vertex.color = "grey",
  vertex.frame.color = "darkgray",
  vertex.shape = "sphere",
  vertex.label = V(network_bacteria_ARG)$name, # names of the nodes
  vertex.label.family = "sans",
  vertex.label.font = 2,            # Font of node names
  vertex.label.cex = 1, 
  vertex.label.dist = 0,
  vertex.label.color = "black",     # Color of node names
  color = ifelse(bact_ARG_co_occurrence$rho <= 0,"#FF0000","#00FF00"), # if rho is negative then colour connecting line is red, if rho is positive then connecting line is green
  width = 20, # if p.adj is <0.00001 then width of edge is 5, if p.adj is > 0.00001 then width of edge is 1
  lty = ifelse(bact_ARG_co_occurrence$p.adjust <= 0.00001, "solid", "dashed"),
  label = NA, # no edge labels wanted
  curved = TRUE) # instead of straight edges, use curved edges

# count number of nodes and edges in the network
vcount(network_bacteria_ARG)
ecount(network_bacteria_ARG)

# Export graph as graphml 
write.graph(network_bacteria_ARG, "Output_files//Co-occurrence_networks//network_bacteria_ARG_graphml_file.graphml", format= "graphml") # Now import this file into gephi for graph visualisation and creation of networks here


# Now for the combined co-occurrence network 
# Create the network - Combined Co-occurrence
# Create an edges dataframe from the significant pairwise co-occurrences.
edges_combined <- data.frame(from = combined_co_occurrence$taxa.X, to = combined_co_occurrence$taxa.Y)
network_combined <- graph_from_data_frame(d = edges_combined, directed = FALSE, vertices = NULL)

# Count the number of degree for each node (used for scaling of nodes in the plot function)
deg_combined <- degree(network_combined, mode = "all")

network_graph_combined <- 
  plot(
    network_combined,
    layout = layout.fruchterman.reingold,
    main = "Combined Co-occurrence Network",
    vertex.size = deg_combined * 4,
    vertex.color = "pink",
    vertex.frame.color = "darkgray",
    vertex.shape = "sphere",
    vertex.label = V(network_combined)$name, # names of the nodes
    vertex.label.family = "sans",
    vertex.label.font = 2,            # Font of node names
    vertex.label.cex = 1, 
    vertex.label.dist = 0,
    vertex.label.color = "black",     # Color of node names
    color = ifelse(combined_co_occurrence$rho <= 0, "#FF0000", "#00FF00"), # if rho is negative then the connecting line is red, if rho is positive then the connecting line is green
    width = ifelse(combined_co_occurrence$p.adjust <= 0.00001, 5, 1), # if p.adj is <0.00001 then the width of the edge is 5, if p.adj is > 0.00001 then the width of the edge is 1
    lty = ifelse(combined_co_occurrence$p.adjust <= 0.00001, "solid", "dashed"),
    label = NA, # no edge labels wanted
    curved = TRUE) # instead of straight edges, use curved edges

# Count the number of nodes and edges in the network
vcount(network_combined)
ecount(network_combined)

# Export the graph as a graphml file
write.graph(network_combined, "Output_files//Co-occurrence_networks//network_combined_graphml_file.graphml", format = "graphml")

# Import the graphs into Gephi for network visualization and further analysis.







# NOW try with rho > 0.8 instead of 0.6 
# Bacteria-Bacteria co-occurrence - Spearman correlations 
bact_bact_co_occurrence_rho_0.8 <- ps_co_occurrence(ps_x = microbiome.ps.genus.matched.names,
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.8, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# ARG-ARG Spearman correlations 
ARG_ARG_co_occurrence_rho_0.8 <- ps_co_occurrence(ps_x = Resistome.ps.noblanks.names,
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.8, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# Bacteria-ARG Spearman correlations with higher rho cut-off 
bact_ARG_co_occurrence_rho_0.8 <- ps_co_occurrence( Resistome.ps.noblanks.names, microbiome.ps.genus.matched.names, 
                              method="spearman", alternative='two.sided', exact=FALSE, adjust="BH",
                              rho.filt=0.8, p.filt=0.01, filt_zero=TRUE, verbose=TRUE ) 

# Inspect the correlations
bact_bact_co_occurrence_rho_0.8 # Only 2 significant bacteria-bacteria correlations are identified when rho > 0.8
ARG_ARG_co_occurrence_rho_0.8 # 19 significant ARG-ARG correlations are identified when rho > 0.8 
bact_ARG_co_occurrence_rho_0.8 # Only 4 significant bacteria-ARG correlations are identified when rho > 0.8, therefore probably best to stay with cut-off of 0.6 

# Try creating the 4 edge bacteria-ARG network with rho>0.8 out of interest
# Create an edges dataframe from the significant pairwise co-occurrences.
edges_bacteria_ARG_rho_0.8 <- data.frame(from = bact_ARG_co_occurrence_rho_0.8$taxa.X, to = bact_ARG_co_occurrence_rho_0.8$taxa.Y) 

network_bacteria_ARG_rho_0.8 <- graph_from_data_frame(d= edges_bacteria_ARG_rho_0.8 , directed = F, vertices = NULL)
print(network_bacteria_ARG_rho_0.8, e=TRUE, v=TRUE)
plot(network_bacteria_ARG_rho_0.8)
# Count the number of degree for each node (used for scaling of nodes in plot function)
deg.bac.ARG.rho0.8 <- degree(network_bacteria_ARG_rho_0.8, mode="all")

network_graph_bact_ARG_rho_0.8 <- 
  plot(
  network_bacteria_ARG_rho_0.8,
  layout = layout.fruchterman.reingold,
  main = "bacteria-ARG co-occurrence network, rho 0.8",
  vertex.size = deg.bac.ARG.rho0.8*6,
  vertex.color = "grey",
  vertex.frame.color = "darkgray",
  vertex.shape = "sphere",
  vertex.label = V(network_bacteria_ARG_rho_0.8)$name, # names of the nodes
  vertex.label.family = "sans",
  vertex.label.font = 2,            # Font of node names
  vertex.label.cex = 1, 
  vertex.label.dist = 0,
  vertex.label.color = "black",     # Color of node names
  color = ifelse(bact_ARG_co_occurrence_rho_0.8$rho <= 0,"#FF0000","#00FF00"), # if rho is negative then colour connecting line is red, if rho is positive then connecting line is green
  width = 20, # if p.adj is <0.00001 then width of edge is 5, if p.adj is > 0.00001 then width of edge is 1
  lty = ifelse(bact_ARG_co_occurrence_rho_0.8$p.adjust <= 0.00001, "solid", "dashed"),
  label = NA, # no edge labels wanted
  curved = TRUE) # instead of straight edges, use curved edges

# count number of nodes and edges in the network
vcount(network_bacteria_ARG_rho_0.8)
ecount(network_bacteria_ARG_rho_0.8)

# Export graph as graphml 
write.graph(network_bacteria_ARG_rho_0.8, "network_bacteria_ARG_rho_0.8_graphml_file.graphml", format= "graphml") # Now import this file into gephi for graph visualisation and creation of networks here
```
## Correlation plot 
```{r}
# Create a pearson correlation plot like that in MdR thesis
# Looking at the correlations between microbes and resistance genes

# take the old dataframe that I created for the co-occurrence network (merged.micro.resis.table) which contains the 68 samples (total 72 - 2xblanks - 1xsample with no microbiome data)
merged.micro.resis.table # inspecting the dataframe I remember that the ASVs are not labelled in a useful way, I should add the genus level names to the dataframe. ALso the ARGs are not names according to their cluster name, instead just at the ARG level so shoudl rename these too. 
colnames(merged.micro.resis.table) 
colnames(merged.micro.resis.table) <- c('Actinomyces','F0332','Alloscardovia','Bifidobacterium','Parascardovia','Scardovia','Corynebacterium','Rothia','Propionibacterium','Atopobium','Olsenella','Cryptobacterium','Phocaeicola','F0058','Porphyromonas','Alloprevotella','Prevotella','Rikenellaceae RC9 gut group','Tannerella','Capnocytophaga','Bergeyella','Chryseobacterium','Lentimicrobium','Campylobacter','Bacillus','Solobacterium','Abiotrophia','Alloiococcus','Granulicatella','Enterococcus','Lactobacillus','Listeria','Lactococcus','Streptococcus','Mycoplasma','Gemella','Staphylococcus','Defluviitaleaceae UCG-011','Butyrivibrio','Catonella','Johnsonella','Lachnoanaerobaculum','Oribacterium','Shuttleworthia','Stomatobaculum','Peptococcus','[Eubacterium] brachy group','[Eubacterium] nodatum group','[Eubacterium] saphenum group','Amnipila','Family XIII UCG-001','Mogibacterium','[Eubacterium] yurii group','Filifactor','Peptoanaerobacter','Peptostreptococcus','Centipeda','Selenomonas','Anaeroglobus','Dialister','Megasphaera','Veillonella','Fusobacterium','Leptotrichia','Oceanivirga','Streptobacillus','Candidatus Saccharimonas','TM7a','TM7x','Afipia','Lautropia','Ralstonia','Alysiella','Conchiformibius','Eikenella','Kingella','Neisseria','Simonsiella','Cardiobacterium','Escherichia-Shigella','Salmonella','Erwinia','Actinobacillus','Aggregatibacter','Haemophilus','Faucicola','Moraxella','Pseudomonas','Stenotrophomonas','Treponema','Fretibacterium',"aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

m.r.cor.matrix <- cor(merged.micro.resis.table) # correlation matrix created between all ARGs and bacteria, pearson correlation coefficient is used (defualt here). Warning mssg given: Warning in cor(merged.micro.resis.table) : the standard deviation is zero
# There are lots of NAs for correlation coefficients- not due to missing data therefore must be due to the fact that the sd for many of the genes is 0
# THis is because there are still microbes in the dataframe for which there are no counts - I need to delete these columns as no samples contain these microbes so all are 0, hence sd is also = 0 
m.r.table.final <- merged.micro.resis.table[, colSums(merged.micro.resis.table != 0) > 0]
m.r.table.final <- merged.micro.resis.table[, colSums(merged.micro.resis.table != 0) > 0]

# 22 columns (bacteria) have been removed as the column sum was equal 0 i.e. none of the samples contained it! 
# Create the correlation matrix again with this reduced dataframe 
m.r.cor.matrix.final <- cor(m.r.table.final)
m.r.cor.matrix.final.round <- round (m.r.cor.matrix.final,2) # round pearson correlation coefficients to 2 decimal places. 
# the function cor() returns only the correlation coefficients between variables not p-values - I will do separately using the Hmisc package
m.r.cor.matrix.pvals <- rcorr(m.r.table.final, type = c("pearson"))
# Extract the correlation coefficients
m.r.cor.matrix.pvals$r
# Extract p-values
m.r.cor.matrix.pvals$P


# Create the correlation plot from the (rounded) matrix 
corrplot(m.r.cor.matrix.final.round) # default settings, v crowded, difficutl to decipher
corrplot(m.r.cor.matrix.final.round, type = "upper", order = "hclust", 
         tl.col = "black", tl.srt = 45, tl.cex=0.5)

corrplot(m.r.cor.matrix.final.round, p.mat = m.r.cor.matrix.pvals$P, method = 'circle', type = 'lower', insig='blank', addCoef.col ='black', number.cex = 0.2, order = 'AOE', diag=FALSE, tl.cex = 0.5, tl.col = "black")

# since there are too many elements in the correlation plot, I will instead select the top correlations from the dataset and visualise these.
devtools::install_github("laresbernardo/lares")
library("lares")
# corr_cross function creates a correlation full study and returns a rank of the highest correlation variables obtained in a cross-table.
top10.corr.bar <- corr_cross(m.r.table.final,
           max_pvalue = 0.05,
           top = 10)

corr_cross(m.r.table.final,
           max_pvalue = 0.05,
           pvalue = T,
           top = 10)

ggplot(top10.corr.bar, aes() )


# Simplify the correlation plot using function found online: https://towardsdatascience.com/how-to-create-a-correlation-matrix-with-too-many-variables-309cc0c0a57 
corr_simple <- function(data=m.r.table.final, sig=0.05){
  #convert data to numeric in order to run correlations
  #run a correlation and drop the insignificant ones
  corr <- cor(m.r.cor.matrix.final)
  #prepare to drop duplicates and correlations of 1     
  corr[lower.tri(corr,diag=TRUE)] <- NA 
  #drop perfect correlations
  corr[corr == 1] <- NA 
  #turn into a 3-column table
  corr <- as.data.frame(as.table(corr))
  #remove the NA values from above 
  corr <- na.omit(corr) 
  #select significant values  
  corr <- subset(corr, abs(Freq) > sig) 
  #sort by highest correlation
  corr <- corr[order(-abs(corr$Freq)),] 
  #print table
  print(corr)
  #turn corr back into matrix in order to plot with corrplot
  mtx_corr <- reshape2::acast(corr, Var1~Var2, value.var="Freq")
  
  #plot correlations visually
  corrplot(mtx_corr, is.corr=FALSE, type = "upper",na.label=" ", tl.col = "black", tl.srt = 45, tl.cex=0.3 )
}

m.r.table.final.df <- as.data.frame(m.r.table.final)
corr_simple(m.r.table.final.df)

# Now I want to just create a correlation plot of microbe vs ARG rather than microbe-microbe and ARG-ARG correlations too
# Firstly I need to remove the bacteria names for which column rows are = 0
# Inspect the bacteria names that have been removed when the above argument is passed and the new matrix ( m.r.cor.matrix.final) is created (22 bacteria were removed as a consequence)
rownames(m.r.cor.matrix.final)
colnames(merged.micro.resis.table)

x <- merged.micro.resis.table[, c('Actinomyces','Bifidobacterium','Corynebacterium','Atopobium','Cryptobacterium','Porphyromonas','Prevotella','Tannerella','Bergeyella','Solobacterium','Granulicatella','Lactococcus','Mycoplasma','Butyrivibrio','Johnsonella','Oribacterium','Stomatobaculum','Alloscardovia','Parascardovia','Rothia','Olsenella','F0058','Alloprevotella','Rikenellaceae RC9 gut group','Capnocytophaga','Campylobacter','Abiotrophia','Lactobacillus','Streptococcus','Gemella','Catonella','Lachnoanaerobaculum','Shuttleworthia','Peptococcus','[Eubacterium] brachy group','Amnipila','[Eubacterium] yurii group','Peptoanaerobacter','Centipeda','Anaeroglobus','Megasphaera','Fusobacterium','Oceanivirga','Candidatus Saccharimonas','Lautropia','Conchiformibius','Kingella','Simonsiella','Actinobacillus','Haemophilus','Pseudomonas','Fretibacterium','[Eubacterium] nodatum group','Mogibacterium','Filifactor','Peptostreptococcus','Selenomonas','Dialister','Veillonella','Leptotrichia','Streptobacillus','TM7x','Alysiella','Eikenella','Neisseria','Cardiobacterium','Aggregatibacter','Faucicola','Treponema')]
y <- merged.micro.resis.table[, c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")]


m.r.sep.cor.matrix <- cor(y, x)

corrplot(m.r.sep.cor.matrix, method = 'color',tl.col = "black", tl.srt = 45, tl.cex=0.5) # colorful number

# Insert p-values into the correlation plot
m.r.cor.sep.matrix.pvals <- rcorr(y,x, type = c("pearson"))
# Extract the correlation coefficients
m.r.cor.sep.matrix.pvals$P

corrplot(m.r.cor.matrix.final.round, p.mat = m.r.cor.matrix.pvals$P, method = 'circle', type = 'lower', insig='blank', addCoef.col ='black', number.cex = 0.2, order = 'AOE', diag=FALSE, tl.cex = 0.5, tl.col = "black")
```

## Livestock related exposures and the resistome
### Alpha diversity 
```{r}
blancs <- c("veldbl16", "veldbl3", "veldbl5")
Final.ps.noblanks <- Final.ps
Final.ps.noblanks <-  prune_samples(!sample_names(Final.ps.noblanks)%in%blancs, Final.ps.noblanks)
Final.ps.noblanks;Final.ps # checking to ensure blanks have been removed - yes now there are 69 instead of 72 samples 

# PS object which I need to add metadata to
Final.ps.noblanks
# Current meta data in this ps object
Final.ps.noblanks@sam_data

# Read in the exposure data from MdR
livestock.exp.df <- read.csv("VGOcopdcaco_resistome_Livestpred_forBea.csv", header=TRUE, row.names='respnr')
# Remove the randomly added column 'X' from the dataframe 
livestock.exp.df <- subset (livestock.exp.df, select = -X)
# Remove blanks from the dataframe 
livestock.exp.df.noblanks <- livestock.exp.df[-(70:72),]
livestock.exp.df.noblanks.df <- as.data.frame(livestock.exp.df.noblanks)

# copy over original ps object 
Final.ps.noblanks.exp <- Final.ps.noblanks
sample_data(Final.ps.noblanks.exp)

sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(Final.ps.noblanks.exp) <- sam.data.new
Final.ps.noblanks.exp@sam_data

# Alpha diversity - MULTIPLY BY BILLION METHOD
# The abundance data contains decimals as it has been corrected per gene length, therefore none are whole numbers. Observing the data I see that there are a maximum of 7 decimals in the dataset.
# Alpha diversity calculations require whole numbers...
# I will multiply by 1,000,000,000 in order to remove all these decimals to do the alpha diversity calculations. 
  
Final.ps.noblanks.exp.multiplybillion <- Final.ps.noblanks.exp
Final.ps.noblanks.exp.multiplybillion@otu_table@.Data <- (Final.ps.noblanks.exp.multiplybillion@otu_table@.Data)*1000000000
head(Final.ps.noblanks.exp.multiplybillion@otu_table@.Data)

# Add in the copdcaco data to this new ps object 
Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco<-Final.ps.noblanks@sam_data$copdcaco
Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco
# Firstly I will rename the codes "1" & "0" as COPD & control respectively so that the graphs are easier to interpret - directly manipulate the new PS matrix
str(Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco)
as.factor(Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco)
Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco<- recode(Final.ps.noblanks.exp.multiplybillion@sam_data$copdcaco,"1"="COPD", "0"="control")

# Plot all alpha diversity measures
# Alpha diversity is a measure of microbiome diversity applicable to a single sample (number of species in each sample) i.e. the higher the alpha diversity score of a sample the more species are present in it
# There are many alpha diversity indices which each reflect different aspects of community heterogeneity - I have chosen: observed, Chao1, shannon, simpson, and evenness
plot_richness(Final.ps.noblanks.exp.multiplybillion, measures=c("Richness", "Shannon", "Chao1", "Observed", "Simpson"))
Alphadiversity_scatter<- plot_richness(Final.ps.noblanks.exp.multiplybillion, x="copdcaco", measures=c("Shannon", "Chao1", "Observed", "Simpson", "Evenness"))
Alphadiversity_scatter

# Add colour to the scatterplot
mycols1 <- c("COPD"="red", "control"="blue")
Alphadiversity_scatter.colour.noblanks <- plot_richness(Final.ps.noblanks.exp.multiplybillion, x="copdcaco", measures=c("Observed", "Chao1", "Shannon", "Simpson"), color="copdcaco")
Alphadiversity_scatter.colour + scale_color_manual(values = mycols1, aesthetics = c("color", "fill"))

# Create tables with alpha diversity measures
library(microbiome)
Shannon.table.exp <- microbiome::alpha(Final.ps.noblanks.exp.multiplybillion, (index = "Shannon"), zeroes = TRUE)
Observed.table.exp <-  microbiome::alpha(Final.ps.noblanks.exp.multiplybillion, (index = "Observed"), zeroes = TRUE)
Chao1.table.exp <- microbiome::alpha(Final.ps.noblanks.exp.multiplybillion, (index = "Chao1"), zeroes = TRUE)
SimpsonEvenness.table.exp <- microbiome::alpha(Final.ps.noblanks.exp.multiplybillion, (index = "simpson"), zeroes = TRUE)

Shannon.table.exp
Observed.table.exp
Chao1.table.exp
SimpsonEvenness.table.exp

#Now add the diversity tables to the metadata
sample_data(Final.ps.noblanks.exp.multiplybillion)
sample_data(Final.ps.noblanks.exp.multiplybillion)$shannon <- Shannon.table.exp$diversity_shannon
sample_data(Final.ps.noblanks.exp.multiplybillion)$Chao1 <- Chao1.table.exp$chao1
sample_data(Final.ps.noblanks.exp.multiplybillion)$Observed <- Observed.table.exp$observed
sample_data(Final.ps.noblanks.exp.multiplybillion)$SimpsonEvenness <- SimpsonEvenness.table.exp$evenness_simpson
#check whether this has been added to the metadata table
head(sample_data(Final.ps.noblanks.exp.multiplybillion))
# Yes these columns have been added to the metadata of the phyloseq object now

# Now I want to relate alpha diversity with the exposure variables e.g. MinDistAnyFarm.NEG, all farms within radius, number of animals in radius, endotoxin (dispersion modelled), PM10 (dispersion modelled)
# As all predictor variables are continuous and outcome is continuous too, I can use a linear regression model

# POST meeting on 13th aug 2021
# Will run all linear regressions with all exposure proxies, set a p-value threshold and decide from there which are the important parameters to look at further
exp.proxy.alpha.df <-  as.data.frame(Final.ps.noblanks.exp.multiplybillion@sam_data)
colnames(exp.proxy.alpha.df)
# rename copdcaco as 0 and 1 
exp.proxy.alpha.df$copdcaco <- factor(exp.proxy.alpha.df$copdcaco)
exp.proxy.alpha.df$copdcaco <- recode_factor(exp.proxy.alpha.df$copdcaco, "COPD" = "1", "control" = "0")

# Run the linear models
# Start with Shannon diversity
# I should include COPD/control status as an additional variable in the linear regression models. Potential loss of power by doing this, but doesn’t take many degrees of freedom so should include in these models.  
# UNIVARIABLE alpha diversity models:
# Univariable linear models were used for each alpha diversity index for: 
# The general livestock predictor (I selected the variable “N distance weighted farms within 3000m” as I felt this was most accurate representation of the general livestock-related characteristics (compared to say “min distance to farm” etc.)
# Dispersion modelled endotoxin and PM10
# My LUR-modelled microbial concentrations
# Univariable models used here as no likely confounders with these predictors. 

# General livestock related predictor - I choose nAnyFarmWghtDist.3000m.sum as the predictor for general livestock exposure as this seems like the most suitable exposure proxy for general livestock exposure
lm_shannon_nAnyFarmWghtDist.3000m.sum <- lm(exp.proxy.alpha.df$shannon~exp.proxy.alpha.df$nAnyFarmWghtDist.3000m.sum + exp.proxy.alpha.df$copdcaco)
lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum <- lm(exp.proxy.alpha.df$SimpsonEvenness~exp.proxy.alpha.df$nAnyFarmWghtDist.3000m.sum + exp.proxy.alpha.df$copdcaco)
lm_Observed_nAnyFarmWghtDist.3000m.sum <- lm(exp.proxy.alpha.df$Observed~exp.proxy.alpha.df$nAnyFarmWghtDist.3000m.sum + exp.proxy.alpha.df$copdcaco)

summary(lm_shannon_nAnyFarmWghtDist.3000m.sum)
summary(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum)
summary(lm_Observed_nAnyFarmWghtDist.3000m.sum)

# Dispersion-modelled endotoxin and PM10 concentrations
lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$shannon~exp.proxy.alpha.df$DISP_EUinPM10_AnnualAv_WP99.5 + exp.proxy.alpha.df$copdcaco)
lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$shannon~exp.proxy.alpha.df$DISP_PM10CONC_AnnualAv_WP99.5+ exp.proxy.alpha.df$copdcaco)
lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$SimpsonEvenness~exp.proxy.alpha.df$DISP_EUinPM10_AnnualAv_WP99.5 + exp.proxy.alpha.df$copdcaco)
lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$SimpsonEvenness~exp.proxy.alpha.df$DISP_PM10CONC_AnnualAv_WP99.5+ exp.proxy.alpha.df$copdcaco)
lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$Observed~exp.proxy.alpha.df$DISP_EUinPM10_AnnualAv_WP99.5 + exp.proxy.alpha.df$copdcaco)
lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5 <- lm(exp.proxy.alpha.df$Observed~exp.proxy.alpha.df$DISP_PM10CONC_AnnualAv_WP99.5+ exp.proxy.alpha.df$copdcaco)

summary(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)
summary(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5)
summary(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5)
summary(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5)
summary(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5)
summary(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5)

# SLR LUR-modelled E.coli, staph, tetW and mecA concentrations
# I previously ran these linear models - see other R notebook "LUR_modelling.Rmd"
lm_shannon_SLR_E.coli <- lm(SLR.LUR.estim.alpha.df$shannon~SLR.LUR.estim.alpha.df$E.coli_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_shannon_SLR_Staph <- lm(SLR.LUR.estim.alpha.df$shannon~SLR.LUR.estim.alpha.df$Staph_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_shannon_SLR_tetW <- lm(SLR.LUR.estim.alpha.df$shannon~SLR.LUR.estim.alpha.df$tetW_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_shannon_SLR_mecA <- lm(SLR.LUR.estim.alpha.df$shannon~SLR.LUR.estim.alpha.df$mecA_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)

lm_simpeveness_SLR_E.coli <- lm(SLR.LUR.estim.alpha.df$SimpsonEvenness~SLR.LUR.estim.alpha.df$E.coli_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_simpeveness_SLR_Staph <- lm(SLR.LUR.estim.alpha.df$SimpsonEvenness~SLR.LUR.estim.alpha.df$Staph_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_simpeveness_SLR_tetW <- lm(SLR.LUR.estim.alpha.df$SimpsonEvenness~SLR.LUR.estim.alpha.df$tetW_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_simpeveness_SLR_mecA <- lm(SLR.LUR.estim.alpha.df$SimpsonEvenness~SLR.LUR.estim.alpha.df$mecA_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)

lm_observed_SLR_E.coli <- lm(SLR.LUR.estim.alpha.df$Observed~SLR.LUR.estim.alpha.df$E.coli_SLR.LUR3.estim+ SLR.LUR.estim.alpha.df$copdcaco)
lm_observed_SLR_Staph <- lm(SLR.LUR.estim.alpha.df$Observed~SLR.LUR.estim.alpha.df$Staph_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_observed_SLR_tetW <- lm(SLR.LUR.estim.alpha.df$Observed~SLR.LUR.estim.alpha.df$tetW_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)
lm_observed_SLR_mecA <- lm(SLR.LUR.estim.alpha.df$Observed~SLR.LUR.estim.alpha.df$mecA_SLR.LUR3.estim + SLR.LUR.estim.alpha.df$copdcaco)

summary(lm_shannon_SLR_E.coli)
summary(lm_shannon_SLR_Staph)
summary(lm_shannon_SLR_tetW)
summary(lm_shannon_SLR_mecA)

summary(lm_simpeveness_SLR_E.coli)
summary(lm_simpeveness_SLR_Staph)
summary(lm_simpeveness_SLR_tetW)
summary(lm_simpeveness_SLR_mecA)

summary(lm_observed_SLR_E.coli)
summary(lm_observed_SLR_Staph)
summary(lm_observed_SLR_tetW)
summary(lm_observed_SLR_mecA)


# MULTIVARIABLE alpha diversity models
# I am only using a multivariable model for the distance weighted animals (to correct for different livestock species)
# 1000m buffer animals 
lm_shannon_multivariable_animals_1000m <- lm(exp.proxy.alpha.df$shannon ~
                                exp.proxy.alpha.df$copdcaco + 
                                exp.proxy.alpha.df$npigsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.1000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.1000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.1000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.1000m.sum)
                             
lm_simpson_multivariable_animals_1000m <- lm(exp.proxy.alpha.df$SimpsonEvenness ~ 
                                exp.proxy.alpha.df$copdcaco + 
                                exp.proxy.alpha.df$npigsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.1000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.1000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.1000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.1000m.sum)

lm_observed_multivariable_animals_1000m <- lm(exp.proxy.alpha.df$Observed ~ 
                                exp.proxy.alpha.df$copdcaco +
                                  exp.proxy.alpha.df$npigsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.1000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.1000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.1000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.1000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.1000m.sum)

# 3000m buffer animals 
lm_shannon_multivariable_animals_3000m <- lm(exp.proxy.alpha.df$shannon ~
                                exp.proxy.alpha.df$copdcaco + 
                                exp.proxy.alpha.df$npigsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.3000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.3000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.3000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.3000m.sum)

lm_simpson_multivariable_animals_3000m <- lm(exp.proxy.alpha.df$SimpsonEvenness ~ 
                                exp.proxy.alpha.df$copdcaco + 
                                exp.proxy.alpha.df$npigsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.3000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.3000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.3000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.3000m.sum)

lm_observed_multivariable_animals_3000m <- lm(exp.proxy.alpha.df$Observed ~ 
                               exp.proxy.alpha.df$copdcaco+ 
                                 exp.proxy.alpha.df$npigsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$npoultryWghtDist.3000m.sum + 
                                exp.proxy.alpha.df$ncowsWghtDist.3000m.sum+ 
                                exp.proxy.alpha.df$nhorsesWghtDist.3000m.sum+
                                exp.proxy.alpha.df$ngoatsWghtDist.3000m.sum+
                                exp.proxy.alpha.df$nsheepWghtDist.3000m.sum)
                             

# Summary of these multiple linear regression models 
# 1000m buffer
summary(lm_shannon_multivariable_animals_1000m)
summary(lm_simpson_multivariable_animals_1000m)
summary(lm_observed_multivariable_animals_1000m)
# 3000m buffer
summary(lm_shannon_multivariable_animals_3000m)
summary(lm_simpson_multivariable_animals_3000m)
summary(lm_observed_multivariable_animals_3000m)

# Create a data table using table1 package to summarise the linear regression results 
# Factor the basic variables that we're interested in
library(sjPlot)
# Data table for simple linear regression models
# General livestock exposure
tab_model(lm_shannon_nAnyFarmWghtDist.3000m.sum, lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum, lm_Observed_nAnyFarmWghtDist.3000m.sum, auto.label = FALSE, pred.labels = c("(Intercept)", "N of distance weighted farms within 3000m (any type)", "COPD case vs. control"))

# Data table for simple linear regression models
# Dispersion modelled endotoxin and PM10
summary(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)
tab_model(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5, lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5, lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5, auto.label = FALSE, pred.labels = c("(Intercept)", "Dispersion modelled endotoxin concentration", "COPD case vs. control"))
tab_model(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5,lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5, lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5, auto.label = FALSE, pred.labels = c("(Intercept)", "Dispersion modelled PM10 concentration", "COPD case vs. control"))

# Data table for multiple linear regression models
tab_model(lm_shannon_multivariable_animals_1000m)
table_lm_shannon_multivariable_animals_1000m <- tab_model(lm_shannon_multivariable_animals_1000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 1000m","N of distance weighted poultry within 1000m", "N of distance weighted cows within 1000m", "N of distance weighted horses within 1000m", "N of distance weighted goats within 1000m", "N of distance weighted sheep within 1000m"))
table_lm_simpson_multivariable_animals_1000m <- tab_model(lm_simpson_multivariable_animals_1000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 1000m","N of distance weighted poultry within 1000m", "N of distance weighted cows within 1000m", "N of distance weighted horses within 1000m", "N of distance weighted goats within 1000m", "N of distance weighted sheep within 1000m"))
table_lm_observed_multivariable_animals_1000m <- tab_model(lm_observed_multivariable_animals_1000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 1000m","N of distance weighted poultry within 1000m", "N of distance weighted cows within 1000m", "N of distance weighted horses within 1000m", "N of distance weighted goats within 1000m", "N of distance weighted sheep within 1000m"))
table_lm_shannon_multivariable_animals_3000m <- tab_model(lm_shannon_multivariable_animals_3000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 3000m","N of distance weighted poultry within 3000m", "N of distance weighted cows within 3000m", "N of distance weighted horses within 3000m", "N of distance weighted goats within 3000m", "N of distance weighted sheep within 3000m"))
table_lm_simpson_multivariable_animals_3000m <- tab_model(lm_simpson_multivariable_animals_3000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 3000m","N of distance weighted poultry within 3000m", "N of distance weighted cows within 3000m", "N of distance weighted horses within 3000m", "N of distance weighted goats within 3000m", "N of distance weighted sheep within 3000m"))
table_lm_observed_multivariable_animals_3000m <- tab_model(lm_observed_multivariable_animals_3000m, auto.label = FALSE, pred.labels = c("Intercept", "COPD case vs.control", "N of distance weighted pigs within 3000m","N of distance weighted poultry within 3000m", "N of distance weighted cows within 3000m", "N of distance weighted horses within 3000m", "N of distance weighted goats within 3000m", "N of distance weighted sheep within 3000m"))


# Checking assumptions of multiple linear regression models
# The four key assumptions are: Linearity of the relationship between y and its explanatory variables, Independence of variables where explanatory variables are not highly correlated with each other, Normal distribution of residuals, Homoscedasticity or equal variance of residuals
library(ggplot2)
library(corrplot)
library(car)
library(magrittr)

# R has a set of built in regression diagnostic plots (4 in total) set of diagnostics plots
# Checking assumptions of simple linear regression models
{par(mfrow= c(2,2))
plot(lm_shannon_nAnyFarmWghtDist.3000m.sum)
plot(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum)
plot(lm_Observed_nAnyFarmWghtDist.3000m.sum)}

{par(mfrow= c(2,2))
plot(lm_shannon_SLR_E.coli)
plot(lm_shannon_SLR_Staph)
plot(lm_shannon_SLR_tetW)
plot(lm_shannon_SLR_mecA)
plot(lm_simpeveness_SLR_E.coli)
plot(lm_simpeveness_SLR_Staph)
plot(lm_simpeveness_SLR_tetW)
plot(lm_simpeveness_SLR_mecA)
plot(lm_observed_SLR_E.coli)
plot(lm_observed_SLR_Staph)
plot(lm_observed_SLR_tetW)
plot(lm_observed_SLR_mecA)}

{par(mfrow= c(2,2))
plot(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5)
plot(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5)
plot(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5)}

# Checking assumptions of multiple linear regression models
{par(mfrow= c(2,2))
plot(lm_shannon_multivariable_animals_1000m) 
plot(lm_simpson_multivariable_animals_1000m)
plot(lm_observed_multivariable_animals_1000m) 
plot(lm_shannon_multivariable_animals_3000m)
plot(lm_simpson_multivariable_animals_3000m)
plot(lm_observed_multivariable_animals_3000m)}

# First plot shows whether there are non-linear relationships between X and Y - if the red lines lies on the straight dotted line then assumptions are met of linearity. A horizontal line with no patterns indicates a linear relationship.Residuals should be evenly spread around the line - in my case we can see 2 distinct lines of data in most plots due to the dichotomous (COPD case/control) variable that is in the model.
# Q-Q plot shows whether our residuals are normally distributed - they should lie on the diagonal line - doesn't have to be perfect - if it curves off a lot in the centre this is a warning sign - normally they tail away a bit 
# Scale-location plot tells us if the residuals are evenly spread across the predictors. This is the assessment of homoscedasticity - you want to see a pretty much horizontal line. 
# Residuals vs leverage plot allows us to identify any potential influential points - we are interested in points in either the top right or bottom right corners - problematic cases will fall within dotted lines that are automatically on the plot making it easy to spot

# Histograms to look graphically at the distribution of the residuals
{par(mfrow= c(2,2))
plot(lm_shannon_SLR_E.coli)
plot(lm_shannon_SLR_Staph)
plot(lm_shannon_SLR_tetW)
plot(lm_shannon_SLR_mecA)
plot(lm_simpeveness_SLR_E.coli)
plot(lm_simpeveness_SLR_Staph)
plot(lm_simpeveness_SLR_tetW)
plot(lm_simpeveness_SLR_mecA)
plot(lm_observed_SLR_E.coli)
plot(lm_observed_SLR_Staph)
plot(lm_observed_SLR_tetW)
plot(lm_observed_SLR_mecA)}
plot(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5)
plot(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5)
plot(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5)
plot(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5)}
plot(lm_shannon_multivariable_animals_1000m) 
plot(lm_simpson_multivariable_animals_1000m)
plot(lm_observed_multivariable_animals_1000m) 
plot(lm_shannon_multivariable_animals_3000m)
plot(lm_simpson_multivariable_animals_3000m)
plot(lm_observed_multivariable_animals_3000m)}

hist(lm_shannon_nAnyFarmWghtDist.3000m.sum$residuals)
hist(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum$residuals) 
hist(lm_Observed_nAnyFarmWghtDist.3000m.sum$residuals)
hist(lm_shannon_SLR_E.coli$residuals)
hist(lm_shannon_SLR_Staph$residuals) 
hist(lm_shannon_SLR_tetW$residuals)
hist(lm_shannon_SLR_mecA$residuals)
hist(lm_simpeveness_SLR_E.coli$residuals)
hist(lm_simpeveness_SLR_Staph$residuals)
hist(lm_simpeveness_SLR_tetW$residuals)
hist(lm_simpeveness_SLR_mecA$residuals)
hist(lm_observed_SLR_E.coli$residuals)
hist(lm_observed_SLR_Staph$residuals)
hist(lm_observed_SLR_tetW$residuals)
hist(lm_observed_SLR_mecA$residuals)
hist(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5$residuals)
hist(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5$residuals)
hist(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5$residuals)
hist(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5$residuals)
hist(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5$residuals)
hist(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5$residuals)
hist(lm_shannon_multivariable_animals_1000m$residuals) 
hist(lm_simpson_multivariable_animals_1000m$residuals)
hist(lm_observed_multivariable_animals_1000m$residuals) 
hist(lm_shannon_multivariable_animals_3000m$residuals)
hist(lm_simpson_multivariable_animals_3000m$residuals)
hist(lm_observed_multivariable_animals_3000m$residuals)

# Durbin watson test is used to detect the presence of autocorrelation in the residuals
library(car)
durbinWatsonTest(lm_shannon_nAnyFarmWghtDist.3000m.sum)
durbinWatsonTest(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum)
durbinWatsonTest(lm_Observed_nAnyFarmWghtDist.3000m.sum)
durbinWatsonTest(lm_shannon_SLR_E.coli)
durbinWatsonTest(lm_shannon_SLR_Staph)
durbinWatsonTest(lm_shannon_SLR_tetW)
durbinWatsonTest(lm_shannon_SLR_mecA)
durbinWatsonTest(lm_simpeveness_SLR_E.coli)
durbinWatsonTest(lm_simpeveness_SLR_Staph)
durbinWatsonTest(lm_simpeveness_SLR_tetW)
durbinWatsonTest(lm_simpeveness_SLR_mecA)
durbinWatsonTest(lm_observed_SLR_E.coli)
durbinWatsonTest(lm_observed_SLR_Staph)
durbinWatsonTest(lm_observed_SLR_tetW)
durbinWatsonTest(lm_observed_SLR_mecA)
durbinWatsonTest(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)
durbinWatsonTest(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5)
durbinWatsonTest(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5)
durbinWatsonTest(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5)
durbinWatsonTest(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5)
durbinWatsonTest(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5)
durbinWatsonTest(lm_shannon_multivariable_animals_1000m)
durbinWatsonTest(lm_simpson_multivariable_animals_1000m)
durbinWatsonTest(lm_observed_multivariable_animals_1000m)
durbinWatsonTest(lm_shannon_multivariable_animals_3000m)
durbinWatsonTest(lm_simpson_multivariable_animals_3000m)
durbinWatsonTest(lm_observed_multivariable_animals_3000m)

# None come out as significant - no significant autocorrelation detected
# Cook's distance 
library(olsrr)
{par(mfrow=c(5,5))
ols_plot_cooksd_bar(lm_shannon_nAnyFarmWghtDist.3000m.sum, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum, print_plot = TRUE)
ols_plot_cooksd_bar(lm_Observed_nAnyFarmWghtDist.3000m.sum, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_SLR_E.coli, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_SLR_Staph, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_SLR_tetW, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_SLR_mecA, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpeveness_SLR_E.coli, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpeveness_SLR_Staph, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpeveness_SLR_tetW, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpeveness_SLR_mecA, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_SLR_E.coli, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_SLR_Staph, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_SLR_tetW, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_SLR_mecA, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_multivariable_animals_1000m, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpson_multivariable_animals_1000m, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_multivariable_animals_1000m, print_plot = TRUE)
ols_plot_cooksd_bar(lm_shannon_multivariable_animals_3000m, print_plot = TRUE)
ols_plot_cooksd_bar(lm_simpson_multivariable_animals_3000m, print_plot = TRUE)
ols_plot_cooksd_bar(lm_observed_multivariable_animals_3000m, print_plot = TRUE)}

cooks.distance(lm_shannon_nAnyFarmWghtDist.3000m.sum)>1
cooks.distance(lm_simpsonevenness_nAnyFarmWghtDist.3000m.sum)>1
cooks.distance(lm_Observed_nAnyFarmWghtDist.3000m.sum)>1
cooks.distance(lm_shannon_SLR_E.coli)>1
cooks.distance(lm_shannon_SLR_Staph)>1
cooks.distance(lm_shannon_SLR_tetW)>1
cooks.distance(lm_shannon_SLR_mecA)>1
cooks.distance(lm_simpeveness_SLR_E.coli)>1
cooks.distance(lm_simpeveness_SLR_Staph)>1
cooks.distance(lm_simpeveness_SLR_tetW)>1
cooks.distance(lm_simpeveness_SLR_mecA)>1
cooks.distance(lm_observed_SLR_E.coli)>1
cooks.distance(lm_observed_SLR_Staph)>1
cooks.distance(lm_observed_SLR_tetW)>1
cooks.distance(lm_observed_SLR_mecA)>1
cooks.distance(lm_shannon_DISP_EUinPM10_AnnualAv_WP99.5)>1
cooks.distance(lm_shannon_DISP_PM10CONC_AnnualAv_WP99.5)>1
cooks.distance(lm_simpsonevenness_DISP_EUinPM10_AnnualAv_WP99.5)>1
cooks.distance(lm_simpsonevenness_DISP_PM10CONC_AnnualAv_WP99.5)>1
cooks.distance(lm_Observed_DISP_EUinPM10_AnnualAv_WP99.5)>1
cooks.distance(lm_Observed_DISP_PM10CONC_AnnualAv_WP99.5)>1
cooks.distance(lm_shannon_multivariable_animals_1000m)>1
cooks.distance(lm_simpson_multivariable_animals_1000m)>1
cooks.distance(lm_observed_multivariable_animals_1000m)>1
cooks.distance(lm_shannon_multivariable_animals_3000m)>1
cooks.distance(lm_simpson_multivariable_animals_3000m)>1
cooks.distance(lm_observed_multivariable_animals_3000m)>1

# In each regression model there are several points with a cook's D > 4/n threshold (in our case n=69, therefore threshold = 0.058). 
# Not many points have Cook's Distance > 1 though

```
### Ordinations
** Tells us how different the resistome composition in one environment is compared to another. I.e. I will look at the whether the resistome composition in COPD pts differs to that in controls.**
```{r}
# I will construct ordinations based on exposure proxies
# I start with the dispersion modelled PM10 and endotoxin
# Then will use the 4 exposure proxies that came out as significant for the alpha diversity metrics: nhorsesWghtDist.1000m.sum, nsheepWghtDist.1000m.sum, nhorsesWghtDist.3000m.sum, ngoatsWghtDist.3000m.sum

# PCoA 
# I will construct principal coordinates analysis based on Bray-Curtis dissimilarities. These measures are based on abundance or read count data. It generates differences in ARG abundances between two samples (e.g., at species level) which are from 0 to 1 (where 0= both samples share the same species at exactly the same abundances and 1= both samples have complete different species abundances).
ARGcluster.aggl.noblanks.exp <- ARGcluster.aggl.noblanks
sample_data(ARGcluster.aggl.noblanks.exp)
sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(ARGcluster.aggl.noblanks.exp) <- sam.data.new
names(ARGcluster.aggl.noblanks.exp@sam_data)

# Firstly I need to categorise my exposure proxy variables which are currently continuous numeric to quantiles (factor variables) (as WvK did for his analysis too)- I want an equal % of variables in each quantile
# I start with the livestock proxies which came out significant in the alpha diversity linear models 
ARGcluster.aggl.noblanks.exp.quant <- ARGcluster.aggl.noblanks.exp
# I will use the quantcut() function in gtools -quantcut(x, q = 4, na.rm = TRUE, ...)

ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.NEG<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.NEG, q=4, labels =c("1st quantile (minDistAnyFarm.NEG)", "2nd quantile (minDistAnyFarm.NEG)", "3rd quantile (minDistAnyFarm.NEG)", "4th quantile (minDistAnyFarm.NEG)"))
# MinDistAnyFarm.INV
ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.INV <- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.INV, q=4, labels =c("1st quantile (MinDistAnyFarm.INV)", "2nd quantile (MinDistAnyFarm.INV)", "3rd quantile (MinDistAnyFarm.INV)", "4th quantile (MinDistAnyFarm.INV)"))
# AllFarm.3000m
ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.3000m<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.3000m, q=4, labels =c("1st quantile (AllFarm.3000m)", "2nd quantile (AllFarm.3000m)", "3rd quantile (AllFarm.3000m)", "4th quantile (AllFarm.3000m)"))
#AllFarm.1000m
ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.1000m<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.1000m, q=4, labels =c("1st quantile (AllFarm.1000m)", "2nd quantile (AllFarm.1000m)", "3rd quantile (AllFarm.1000m)", "4th quantile (AllFarm.1000m)"))
#AllFarm.500m - unable to split into quantiles due to lots of zeros - I have split in half instead
ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.500m<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.500m, q=2,labels =c("Bottom half (AllFarm.500m)", "Top half (AllFarm.500m)"))
# #AllFarm.250m  -  difficult to split as the majority are equal to zero…I could split into 0 vs not 0 maybe? Is this that explanatory though?
# ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.250m<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.250m, q=2)
#npigsWghtDist.1000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.1000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.1000m.sum, q=2, labels=c("Bottom half (npigsWghtDist.1000m.sum)", "Top half (npigsWghtDist.1000m.sum)"))
#npoultryWghtDist.1000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.1000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.1000m.sum, q=2, labels=c("Bottom half (npoultryWghtDist.1000m.sum)", "Top half (npoultryWghtDist.1000m.sum)"))
#ncowsWghtDist.1000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.1000m.sum<- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.1000m.sum, q=4, labels =c("1st quantile (ncowsWghtDist.1000m.sum)", "2nd quantile (ncowsWghtDist.1000m.sum)", "3rd quantile (ncowsWghtDist.1000m.sum)", "4th quantile (ncowsWghtDist.1000m.sum)"))
#ngoatsWghtDist.1000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum <- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum, q=2, labels=c("Bottom half (ngoatsWghtDist.1000m.sum)", "Top half (ngoatsWghtDist.1000m.sum)")) # unable to split ngoatsWghtDist.1000m.sum into half
#nhorsesWghtDist.1000m.sum 
ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.1000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.1000m.sum, q=4, labels =c("1st quantile (nhorsesWghtDist.1000m.sum)", "2nd quantile (nhorsesWghtDist.1000m.sum)", "3rd quantile (nhorsesWghtDist.1000m.sum)", "4th quantile (nhorsesWghtDist.1000m.sum)"))
# ngoatsWghtDist.1000m.sum
# Difficult to split as the majority are equal to zero…I could split into 0 vs not 0 maybe?
ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum <- ifelse(ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum == 0, "No goat farms in 1000m","1 or more goat farms within 1000m")
# Split nsheepWghtDist.1000m.sum into quantiles
# It is not possible to split this variable into quantiles (because more than one quantile obtains the same value - as there are so many zero values), therefore instead I can split into 2 - bottom and top halves
ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.1000m.sum <- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.1000m.sum, q=2, labels =c("bottom half", "top half"))
# npigsWghtDist.3000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.3000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.3000m.sum, q=2, labels =c("Bottom half (npigsWghtDist.3000m.sum)", "Top half (npigsWghtDist.3000m.sum)"))
#npoultryWghtDist.3000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.3000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.3000m.sum, q=4, labels =c("1st quantile (npoultryWghtDist.3000m.sum)", "2nd quantile (npoultryWghtDist.3000m.sum)", "3rd quantile (npoultryWghtDist.3000m.sum)", "4th quantile (npoultryWghtDist.3000m.sum)"))
#ncowsWghtDist.3000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.3000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.3000m.sum, q=4, labels =c("1st quantile (ncowsWghtDist.3000m.sum)", "2nd quantile (ncowsWghtDist.3000m.sum)", "3rd quantile (ncowsWghtDist.3000m.sum)", "4th quantile (ncowsWghtDist.3000m.sum)"))
# Split nhorsesWghtDist.3000m.sum into quantiles
ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.3000m.sum <- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.3000m.sum, q=4, labels =c("1st quantile (nhorsesWghtDist.3000m.sum)", "2nd quantile (nhorsesWghtDist.3000m.sum)", "3rd quantile (nhorsesWghtDist.3000m.sum)", "4th quantile(nhorsesWghtDist.3000m.sum)"))
# Split ngoatsWghtDist.3000m.sum into quantiles
ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.3000m.sum <- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.3000m.sum, q=4, labels =c("1st quantile (ngoatsWghtDist.3000m.sum)", "2nd quantile (ngoatsWghtDist.3000m.sum)", "3rd quantile (ngoatsWghtDist.3000m.sum)", "4th quantile(ngoatsWghtDist.3000m.sum)"))
#nsheepWghtDist.3000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.3000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.3000m.sum, q=4, labels =c("1st quantile (nsheepWghtDist.3000m.sum)", "2nd quantile (nsheepWghtDist.3000m.sum)", "3rd quantile (nsheepWghtDist.3000m.sum)", "4th quantile (nsheepWghtDist.3000m.sum)"))
#nAnyFarmWghtDist.1000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.1000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.1000m.sum, q=4, labels =c("1st quantile (nAnyFarmWghtDist.1000m.sum)", "2nd quantile (nAnyFarmWghtDist.1000m.sum)", "3rd quantile (nAnyFarmWghtDist.1000m.sum)", "4th quantile (nAnyFarmWghtDist.1000m.sum)"))
#nAnyFarmWghtDist.3000m.sum
ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.3000m.sum<-quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.3000m.sum, q=4, labels =c("1st quantile (nAnyFarmWghtDist.3000m.sum)", "2nd quantile (nAnyFarmWghtDist.3000m.sum)", "3rd quantile (nAnyFarmWghtDist.3000m.sum)", "4th  quantile (nAnyFarmWghtDist.3000m.sum) "))
# DISP_EUinPM10_AnnualAv_WP99.5  
ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5<- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, q=4, labels =c("1st quantile (DISP_EUinPM10_AnnualAv_WP99.5)", "2nd quantile (DISP_EUinPM10_AnnualAv_WP99.5)", "3rd quantile (DISP_EUinPM10_AnnualAv_WP99.5)", "4th quantile (DISP_EUinPM10_AnnualAv_WP99.5)"))
# DISP_PM10CONC_AnnualAv_WP99.5 
ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_PM10CONC_AnnualAv_WP99.5<- quantcut(ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_PM10CONC_AnnualAv_WP99.5, q=4, labels =c("1st quantile (DISP_PM10CONC_AnnualAv_WP99.5)", "2nd quantile (DISP_PM10CONC_AnnualAv_WP99.5)", "3rd quantile (DISP_PM10CONC_AnnualAv_WP99.5)", "4th quantile (DISP_PM10CONC_AnnualAv_WP99.5)"))


# Check a few variabes to ensure quantile/half splitting has been successful

# check the quantile splitting of these other exposure proxies has been successful
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.NEG)
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.INV)
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.3000m)
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.1000m)
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.500m)
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum) # should be in half
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.1000m.sum)# should be in half
barchart(ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5)
# Yes it appears that splitting of the exposure proxy variables has all been performed correctly 

# PCoA on the BC dissimilarity matrix of the clustered resistome data (as before)
ARGcluster.aggl.PCoA.exp.quant <- ordinate(ARGcluster.aggl.noblanks.exp.quant, method="PCoA", distance="bray")

# Extract the vectors from the PCoA 
ARGcluster.aggl.PCoA.exp.quant$vectors # i.e. this tells us how much of the total variance is explained by each of the axes with respect to the total variance

# Show dominant eigen planes
plot_scree(ARGcluster.aggl.PCoA.exp.quant)

# Exploring all exposure variables in PCoAs 
# MinDistAnyFarm.NEG
ARGcluster.aggl.PCoA.exp.plot.MinDistAnyFarm.NEG.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="MinDistAnyFarm.NEG", shape="MinDistAnyFarm.NEG" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by MinDistAnyFarm.NEG quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.MinDistAnyFarm.NEG.quant

# MinDistAnyFarm.INV
ARGcluster.aggl.PCoA.exp.plot.MinDistAnyFarm.INV.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="MinDistAnyFarm.INV", shape="MinDistAnyFarm.INV" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by MinDistAnyFarm.INV quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.MinDistAnyFarm.INV.quant

# AllFarm.3000m
ARGcluster.aggl.PCoA.exp.plot.Allfarm.3000m <- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="AllFarm.3000m", shape="AllFarm.3000m" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by AllFarm.3000m quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.Allfarm.3000m

# AllFarm.1000m
ARGcluster.aggl.PCoA.exp.plot.Allfarm.1000m.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="AllFarm.1000m", shape="AllFarm.1000m" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by AllFarm.1000m quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.Allfarm.1000m.quant

#AllFarm.500m
ARGcluster.aggl.PCoA.exp.plot.Allfarm.500m <- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="AllFarm.500m", shape="AllFarm.500m" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by AllFarm.500m halves") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.Allfarm.500m

#npigsWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.npigsWghtDist.1000m.sum.half<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="npigsWghtDist.1000m.sum", shape="npigsWghtDist.1000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by npigsWghtDist.1000m.sum halves") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.npigsWghtDist.1000m.sum.half

# npoultryWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.npoultryWghtDist.1000m.sum.half<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="npoultryWghtDist.1000m.sum", shape="npoultryWghtDist.1000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by npoultryWghtDist.1000m.sum halves") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.npoultryWghtDist.1000m.sum.half

#ncowsWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.ncowsWghtDist.1000m.sum<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="ncowsWghtDist.1000m.sum", shape="ncowsWghtDist.1000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by ncowsWghtDist.1000m.sum quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.ncowsWghtDist.1000m.sum

#nhorsesWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.1000m.sum.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "nhorsesWghtDist.1000m.sum", shape="nhorsesWghtDist.1000m.sum") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by nhorsesWghtDist.1000m.sum quantiles") +
  labs(legend="nhorsesWghtDist.1000m.sum exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = nhorsesWghtDist.1000m.sum))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.1000m.sum.quant
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.1000m.sum.quant + stat_ellipse()

# ngoatsWghtDist.1000m.sum.quant
ARGcluster.aggl.PCoA.exp.plot.ngoatsWghtDist.1000m.sum.quant <- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="ngoatsWghtDist.1000m.sum", shape="ngoatsWghtDist.1000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - \ncoloured by quantiles of ngoatsWghtDist.1000m.sum") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() +
  theme(plot.title = element_text(size = 20, face = "bold"), legend.title=element_text(size=20), legend.text=element_text(size=15))
ARGcluster.aggl.PCoA.exp.plot.ngoatsWghtDist.1000m.sum.quant

# nsheepWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.nsheepWghtDist.1000m.sum.half <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "nsheepWghtDist.1000m.sum", shape="nsheepWghtDist.1000m.sum") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by nsheepWghtDist.1000m.sum quantiles") +
  labs(legend="nsheepWghtDist.1000m.sum exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = nsheepWghtDist.1000m.sum))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.nsheepWghtDist.1000m.sum.half
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.nsheepWghtDist.1000m.sum.half + stat_ellipse()



#npigsWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.npigsWghtDist.3000m.sum.half<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="npigsWghtDist.3000m.sum", shape="npigsWghtDist.3000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by npigsWghtDist.3000m.sum halves") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.npigsWghtDist.3000m.sum.half

#npoultryWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.npoultryWghtDist.3000m.sum<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="npoultryWghtDist.3000m.sum", shape="npoultryWghtDist.3000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by npoultryWghtDist.3000m.sum quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.npoultryWghtDist.3000m.sum

#ncowsWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.ncowsWghtDist.3000m.sum.quant<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="ncowsWghtDist.3000m.sum", shape="ncowsWghtDist.3000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by ncowsWghtDist.3000m.sum quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.ncowsWghtDist.3000m.sum.quant

#nhorsesWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.3000m.sum.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "nhorsesWghtDist.3000m.sum", shape="nhorsesWghtDist.3000m.sum") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by nhorsesWghtDist.3000m.sum quantiles") +
  labs(legend="nhorsesWghtDist.3000m.sum exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = nhorsesWghtDist.3000m.sum))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.3000m.sum.quant
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.nhorsesWghtDist.3000m.sum.quant + stat_ellipse()

#ngoatsWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.ngoatsWghtDist.3000m.sum.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "ngoatsWghtDist.3000m.sum", shape="ngoatsWghtDist.3000m.sum") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by ngoatsWghtDist.3000m.sum quantiles") +
  labs(legend="ngoatsWghtDist.3000m.sum exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = ngoatsWghtDist.3000m.sum))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.ngoatsWghtDist.3000m.sum.quant
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.ngoatsWghtDist.3000m.sum.quant + stat_ellipse()

#nsheepWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.nsheepWghtDist.3000m.sum.quant<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="nsheepWghtDist.3000m.sum", shape="nsheepWghtDist.3000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by nsheepWghtDist.3000m.sum quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.nsheepWghtDist.3000m.sum.quant

#nAnyFarmWghtDist.1000m.sum
ARGcluster.aggl.PCoA.exp.plot.nAnyFarmWghtDist.1000m.sum<- plot_ordination(ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color="nAnyFarmWghtDist.1000m.sum", shape="nAnyFarmWghtDist.1000m.sum" ) +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, 90% identity clustered) - coloured by nAnyFarmWghtDist.1000m.sum quantiles") +
    geom_point(size = 3) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp@sam_data),hjust=-0.3, vjust=-0.5 ) )+ stat_ellipse() 
ARGcluster.aggl.PCoA.exp.plot.nAnyFarmWghtDist.1000m.sum

# nAnyFarmWghtDist.3000m.sum
ARGcluster.aggl.PCoA.exp.plot.nAnyFarmWghtDist.3000m.sum <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "nAnyFarmWghtDist.3000m.sum", shape="nAnyFarmWghtDist.3000m.sum") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by nAnyFarmWghtDist.3000m.sum quantiles") +
  labs(legend="nAnyFarmWghtDist.3000m.sum exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = nAnyFarmWghtDist.3000m.sum))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.nAnyFarmWghtDist.3000m.sum
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.nAnyFarmWghtDist.3000m.sum + stat_ellipse()

# Ordination on dispersion modelled endotoxin concentration DISP_EUinPM10_AnnualAv_WP99.5 quantiles 
ARGcluster.aggl.PCoA.exp.plot.DISP.endotoxin.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "DISP_EUinPM10_AnnualAv_WP99.5", shape="DISP_EUinPM10_AnnualAv_WP99.5") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by endotoxin exposure quantiles") +
  labs(legend="Endotoxin exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = DISP_EUinPM10_AnnualAv_WP99.5))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.DISP.endotoxin.quant
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.DISP.endotoxin.quant + stat_ellipse()

# Ordination on dispersion modelled PM10 concentration quantiles (DISP_PM10CONC_AnnualAv_WP99.5)
ARGcluster.aggl.PCoA.exp.plot.DISPPM10.quant <- plot_ordination( ARGcluster.aggl.noblanks.exp.quant, ARGcluster.aggl.PCoA.exp.quant, color = "DISP_PM10CONC_AnnualAv_WP99.5", shape="DISP_PM10CONC_AnnualAv_WP99.5") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by PM10 exposure quantiles") +
  labs(legend="PM10 exposure quantiles")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.quant@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = DISP_PM10CONC_AnnualAv_WP99.5))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.DISPPM10.quant
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.DISPPM10.quant + stat_ellipse()

# For all of the ordinations it appears that there are no associations - i.e. lots of overlap of all ordinations

# Now I will try an alternative splitting of the variables
# Try splitting endotoxin exposure into high and low concentrations instead 
# Split DISP_EUinPM10_AnnualAv_WP99.5 into high and low 
ARGcluster.aggl.noblanks.exp.half <- ARGcluster.aggl.noblanks
sample_data(ARGcluster.aggl.noblanks.exp.half)
sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(ARGcluster.aggl.noblanks.exp.half) <- sam.data.new
ARGcluster.aggl.noblanks.exp.half@sam_data$DISP_EUinPM10_AnnualAv_WP99.5<- quantcut(ARGcluster.aggl.noblanks.exp.half@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, q=2, labels =c("Low exposure (DISP_EUinPM10_AnnualAv_WP99.5)", "High exposure (DISP_EUinPM10_AnnualAv_WP99.5)"))

# Run the ordination on this DISP_EUinPM10_AnnualAv_WP99.5 
ARGcluster.aggl.PCoA.exp.plot.DISP.ENDO.half <- plot_ordination( ARGcluster.aggl.noblanks.exp.half, ARGcluster.aggl.PCoA.exp.half, color = "DISP_EUinPM10_AnnualAv_WP99.5", shape="DISP_EUinPM10_AnnualAv_WP99.5") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by endotoxin exposure quantiles") +
  labs(legend="Endotoxin exposure (high vs.low)")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.half@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = DISP_EUinPM10_AnnualAv_WP99.5))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.DISP.ENDO.half
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.DISP.ENDO.half + stat_ellipse()

# Try splitting PM10 exposure into high and low concentrations instead
ARGcluster.aggl.noblanks.exp.half@sam_data$DISP_PM10CONC_AnnualAv_WP99.5<- quantcut(ARGcluster.aggl.noblanks.exp.half@sam_data$DISP_PM10CONC_AnnualAv_WP99.5, breaks=2, labels =c("Low exposure (DISP_PM10CONC_AnnualAv_WP99.5)", "High exposure (DISP_PM10CONC_AnnualAv_WP99.5)"))

ARGcluster.aggl.PCoA.exp.plot.DISPPM10.half <- plot_ordination( ARGcluster.aggl.noblanks.exp.half, ARGcluster.aggl.PCoA.exp.half, color = "DISP_PM10CONC_AnnualAv_WP99.5", shape="DISP_PM10CONC_AnnualAv_WP99.5") +
    ggtitle("BC PCoA (fully corrected and rarefied ARG counts, \n90% identity clustered) - coloured by PM10 exposure quantiles") +
  labs(legend="PM10 exposure (high vs.low)")+
    geom_point(size =5) +
    geom_text( aes( label=row.names(ARGcluster.aggl.noblanks.exp.half@sam_data),hjust=-0.3, vjust=-0.5 ) ) +
  theme(plot.title = element_text(size = 15))+ # adjusting size of title text
  theme(axis.text=element_text(size=10),
        axis.title=element_text(size=10)) + # adjusting size of axis text
  theme(legend.text=element_text(size=8),
        legend.title = element_text(size=8))+ # adjusting size of legend text
  stat_ellipse(geom = "polygon", alpha = 1/6, aes(fill = DISP_PM10CONC_AnnualAv_WP99.5))# adds ellipses to the PCoA, fills ellipses with colour according to copdcaco status, colour transparency=1/4 of strength 
ARGcluster.aggl.PCoA.exp.plot.DISPPM10.half
# Add ellipses 
ARGcluster.aggl.PCoA.exp.plot.DISPPM10.half + stat_ellipse()

# No clear clustering even when dispersion modelled PM10 or endotoxin are just stratified into halves (high and low exposure)...no effect of dispersion modelled PM10 or endotoxin on resistome composition. 
```
## Comparing beta diversity - exposure effects 
```{r}
# There are two tests that are widely used in beta diversity analyses: betadisper and adonis. 
# Betadisper tests whether two or more groups are homogeneously dispersed in relation to their species in studied samples (it is a multivariate analogue of Levene's test for homogeneity of variances). This test can be done to see if one group has more compositional variance than another. Moreover, homogeneity of dispersion among groups is very advisable to have if you want to test if two or more groups have different compositions, which is tested by adonis. So firstly I will perform betadisper: then move onto adonis
# Use betadisper to assess homogeneity of group dispersions - it first calculates the average distance of group members to the group centroid in multivariate space (generated by a distance matrix). Then, an ANOVA is done to test if the dispersions (variances) of groups are different.
#Extract abundance matrix from phyloseq object
ARGclusters.for.beta.div.exp <- as(otu_table(ARGcluster.aggl.noblanks.exp), "matrix")
# transform into a data frame
ARG.cluster.dataframe.exp <-as.data.frame(ARGclusters.for.beta.div.exp)
ARG.cluster.dataframe.exp

# add copd status to df 
ARG.cluster.dataframe.exp$copdcaco <- ARGcluster.aggl.noblanks@sam_data$copdcaco

BC.distance.exp <- vegdist(ARG.cluster.dataframe.exp, method =  "bray")
# All variables have already been split into quantiles or halves

# Dispersion modelled endotoxin
# Betadisper
# Add columns of the exposure proxies to the dataframe
ARG.cluster.dataframe.exp$MinDistAnyFarm.NEG <- ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.NEG
ARG.cluster.dataframe.exp$MinDistAnyFarm.INV <- ARGcluster.aggl.noblanks.exp.quant@sam_data$MinDistAnyFarm.INV
ARG.cluster.dataframe.exp$AllFarm.3000m <- ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.3000m
ARG.cluster.dataframe.exp$AllFarm.1000m <- ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.1000m
ARG.cluster.dataframe.exp$AllFarm.500m <- ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.500m
ARG.cluster.dataframe.exp$AllFarm.250m <- ARGcluster.aggl.noblanks.exp.quant@sam_data$AllFarm.250m
ARG.cluster.dataframe.exp$npigsWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.1000m.sum
ARG.cluster.dataframe.exp$npoultryWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.1000m.sum
ARG.cluster.dataframe.exp$ncowsWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.1000m.sum
ARG.cluster.dataframe.exp$nhorsesWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.1000m.sum
ARG.cluster.dataframe.exp$ngoatsWghtDist.1000m.sum<- ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.1000m.sum
ARG.cluster.dataframe.exp$nsheepWghtDist.1000m.sum<- ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.1000m.sum
ARG.cluster.dataframe.exp$nfuranimsWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nfuranimsWghtDist.1000m.sum
ARG.cluster.dataframe.exp$npigsWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$npigsWghtDist.3000m.sum
ARG.cluster.dataframe.exp$npoultryWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$npoultryWghtDist.3000m.sum
ARG.cluster.dataframe.exp$ncowsWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$ncowsWghtDist.3000m.sum
ARG.cluster.dataframe.exp$nhorsesWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nhorsesWghtDist.3000m.sum
ARG.cluster.dataframe.exp$ngoatsWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.3000m.sum
ARG.cluster.dataframe.exp$nsheepWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nsheepWghtDist.3000m.sum
ARG.cluster.dataframe.exp$nfuranimsWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nfuranimsWghtDist.3000m.sum
ARG.cluster.dataframe.exp$nAnyFarmWghtDist.1000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.1000m.sum
ARG.cluster.dataframe.exp$nAnyFarmWghtDist.3000m.sum <- ARGcluster.aggl.noblanks.exp.quant@sam_data$nAnyFarmWghtDist.3000m.sum
ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5 <- ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5
ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5 <- ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_PM10CONC_AnnualAv_WP99.5

head(ARG.cluster.dataframe.exp) # check these have been added - yes!

#DISP_EUinPM10_AnnualAv_WP99.5
Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5 <- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5)
Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5
# Perform an anova using the group dispersions.
anova(Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5)
# We  see that the ANOVA's p-value is not significant meaning that group dispersions are homogenous ("Null hypothesis of no difference in dispersion between the quartiles")
plot(Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5)
# Here we see that group dispersions (distances from centroids) are very similar, but also that compositions seem to be very similar too. Let's go to adonis to test whether these compositions are different or not. 

# Use adonis to assess compositional dissimilarity - The adonis2() function in the vegan package is an analysis of variance using distance matrices 
adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5, permutations = 999)
# we see that the groups do not have significantly different compositions (p>0.05). 
# We can conclude that our groups (quartiles of endotoxin exposure (dispersion modelled)) present homogeneity among group dispersions (compositions vary similarly) and also they have very similar compositions.

## DISP_PM10CONC_AnnualAv_WP99.5
# Betadisper
Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5 <- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5)
Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5
# Perform an anova using the group dispersions.
anova(Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5)
# We  see that the ANOVA's p-value is not significant meaning that group dispersions are homogenous("Null hypothesis of no difference in dispersion between the quartiles")
plot(Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5)
# Here we see that group dispersions (distances from centroids) are very similar, but also that compositions seem to be very similar too. Let's go to adonis to test whether these compositions are different or not. 

# Use adonis to assess compositional dissimilarity - The adonis2() function in the vegan package is an analysis of variance using distance matrices 
adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5, permutations = 999)
# we see that the 4 groups do not have significantly different compositions (p>0.05). 
# We can conclude that our groups (quartiles of PM10 exposure (dispersion modelled)) present homogeneity among group dispersions (compositions vary similarly) and also they have very similar compositions.

# Running betadisper on all exposure proxies 
Betadisper.ARGcluster.exp.MinDistAnyFarm.NEG<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$MinDistAnyFarm.NEG)
Betadisper.ARGcluster.exp.MinDistAnyFarm.INV<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$MinDistAnyFarm.INV)
Betadisper.ARGcluster.exp.AllFarm.3000m<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$AllFarm.3000m)
Betadisper.ARGcluster.exp.AllFarm.1000m<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$AllFarm.1000m)
Betadisper.ARGcluster.exp.AllFarm.500m<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$AllFarm.500m)
Betadisper.ARGcluster.exp.AllFarm.250m<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$AllFarm.250m)
Betadisper.ARGcluster.exp.npigsWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$npigsWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.npoultryWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$npoultryWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.ncowsWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$ncowsWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.nhorsesWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nhorsesWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.ngoatsWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$ngoatsWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.nsheepWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nsheepWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.nfuranimsWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nfuranimsWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.npigsWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$npigsWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.npoultryWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$npoultryWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.ncowsWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$ncowsWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.nhorsesWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nhorsesWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.ngoatsWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$ngoatsWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.nsheepWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nsheepWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.nfuranimsWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nfuranimsWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.nAnyFarmWghtDist.1000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nAnyFarmWghtDist.1000m.sum)
Betadisper.ARGcluster.exp.nAnyFarmWghtDist.3000m.sum<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$nAnyFarmWghtDist.3000m.sum)
Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5)
Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5<- betadisper(BC.distance.exp, ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5)

anova(Betadisper.ARGcluster.exp.MinDistAnyFarm.NEG)
anova(Betadisper.ARGcluster.exp.MinDistAnyFarm.INV)
anova(Betadisper.ARGcluster.exp.AllFarm.3000m)
anova(Betadisper.ARGcluster.exp.AllFarm.1000m)
anova(Betadisper.ARGcluster.exp.AllFarm.500m)
anova(Betadisper.ARGcluster.exp.AllFarm.250m)
anova(Betadisper.ARGcluster.exp.npigsWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.npoultryWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.ncowsWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.nhorsesWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.ngoatsWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.nsheepWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.nfuranimsWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.npigsWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.npoultryWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.ncowsWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.nhorsesWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.ngoatsWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.nsheepWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.nfuranimsWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.nAnyFarmWghtDist.1000m.sum)
anova(Betadisper.ARGcluster.exp.nAnyFarmWghtDist.3000m.sum)
anova(Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5)
anova(Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5)

plot(Betadisper.ARGcluster.exp.MinDistAnyFarm.NEG)
plot(Betadisper.ARGcluster.exp.MinDistAnyFarm.INV)
plot(Betadisper.ARGcluster.exp.AllFarm.3000m)
plot(Betadisper.ARGcluster.exp.AllFarm.1000m)
plot(Betadisper.ARGcluster.exp.AllFarm.500m)
plot(Betadisper.ARGcluster.exp.AllFarm.250m)
plot(Betadisper.ARGcluster.exp.npigsWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.npoultryWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.ncowsWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.nhorsesWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.ngoatsWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.nsheepWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.nfuranimsWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.npigsWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.npoultryWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.ncowsWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.nhorsesWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.ngoatsWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.nsheepWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.nfuranimsWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.nAnyFarmWghtDist.1000m.sum)
plot(Betadisper.ARGcluster.exp.nAnyFarmWghtDist.3000m.sum)
plot(Betadisper.ARGcluster.exp.DISP_EUinPM10_AnnualAv_WP99.5)
plot(Betadisper.ARGcluster.exp.DISP_PM10CONC_AnnualAv_WP99.5)

# Running adonis2() function on all exposure proxy variables
# set.seed as adonis() does a permutation test by selecting permutations at random
names(ARG.cluster.dataframe.exp)
set.seed(123)
adonis.MinDistAnyFarm.NEG <- adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$MinDistAnyFarm.NEG, permutations = 999)
set.seed(123)
adonis.MinDistAnyFarm.INV <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$MinDistAnyFarm.INV, permutations = 999)
set.seed(123)
adonis.AllFarm.3000m <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$AllFarm.3000m, permutations = 999)
set.seed(123)
adonis.AllFarm.1000m <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$AllFarm.1000m, permutations = 999)
set.seed(123)
adonis.AllFarm.500m <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$AllFarm.500m, permutations = 999)
set.seed(123)
adonis.AllFarm.250m <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$AllFarm.250m, permutations = 999)
set.seed(123)
adonis.npigsWghtDist.1000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$npigsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.npoultryWghtDist.1000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$npoultryWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.ncowsWghtDist.1000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$ncowsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nhorsesWghtDist.1000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nhorsesWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.ngoatsWghtDist.1000m.sum<-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$ngoatsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nsheepWghtDist.1000m.sum<-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nsheepWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.npigsWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$npigsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.npoultryWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$npoultryWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.ncowsWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$ncowsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nhorsesWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nhorsesWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.ngoatsWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$ngoatsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nsheepWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nsheepWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nAnyFarmWghtDist.1000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nAnyFarmWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nAnyFarmWghtDist.3000m.sum <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$nAnyFarmWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.DISP_EUinPM10_AnnualAv_WP99.5 <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5, permutations = 999)
set.seed(123)
adonis.DISP_PM10CONC_AnnualAv_WP99.5 <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5, permutations = 999)
# None come out as significant.
#print the p-values of each 
adonis.MinDistAnyFarm.NEG$`Pr(>F)`
adonis.MinDistAnyFarm.INV$`Pr(>F)`
adonis.AllFarm.3000m$`Pr(>F)`
adonis.AllFarm.1000m$`Pr(>F)`
adonis.AllFarm.500m $`Pr(>F)`
adonis.AllFarm.250m$`Pr(>F)`
adonis.npigsWghtDist.1000m.sum$`Pr(>F)`
adonis.npoultryWghtDist.1000m.sum$`Pr(>F)` 
adonis.ncowsWghtDist.1000m.sum$`Pr(>F)` 
adonis.nhorsesWghtDist.1000m.sum$`Pr(>F)`
adonis.ngoatsWghtDist.1000m.sum$`Pr(>F)`
adonis.nsheepWghtDist.1000m.sum$`Pr(>F)`
adonis.npigsWghtDist.3000m.sum$`Pr(>F)`
adonis.npoultryWghtDist.3000m.sum$`Pr(>F)` 
adonis.ncowsWghtDist.3000m.sum$`Pr(>F)` 
adonis.nhorsesWghtDist.3000m.sum$`Pr(>F)`
adonis.ngoatsWghtDist.3000m.sum$`Pr(>F)` 
adonis.nsheepWghtDist.3000m.sum$`Pr(>F)`
adonis.nAnyFarmWghtDist.1000m.sum$`Pr(>F)`
adonis.nAnyFarmWghtDist.3000m.sum$`Pr(>F)` 
adonis.DISP_EUinPM10_AnnualAv_WP99.5$`Pr(>F)`
adonis.DISP_PM10CONC_AnnualAv_WP99.5$`Pr(>F)` 

# Print the R^2 of each exposure proxy - see the first column for the R2 of the exposure proxy 
adonis.MinDistAnyFarm.NEG$R2
adonis.MinDistAnyFarm.INV$R2
adonis.AllFarm.3000m$R2
adonis.AllFarm.1000m$R2
adonis.AllFarm.500m $R2
adonis.AllFarm.250m$R2
adonis.npigsWghtDist.1000m.sum$R2
adonis.npoultryWghtDist.1000m.sum$R2 
adonis.ncowsWghtDist.1000m.sum$R2 
adonis.nhorsesWghtDist.1000m.sum$R2
adonis.ngoatsWghtDist.1000m.sum$R2
adonis.nsheepWghtDist.1000m.sum$R2
adonis.npigsWghtDist.3000m.sum$R2
adonis.npoultryWghtDist.3000m.sum$R2 
adonis.ncowsWghtDist.3000m.sum$R2 
adonis.nhorsesWghtDist.3000m.sum$R2
adonis.ngoatsWghtDist.3000m.sum$R2 
adonis.nsheepWghtDist.3000m.sum$R2
adonis.nAnyFarmWghtDist.1000m.sum$R2
adonis.nAnyFarmWghtDist.3000m.sum$R2 
adonis.DISP_EUinPM10_AnnualAv_WP99.5$R2
adonis.DISP_PM10CONC_AnnualAv_WP99.5$R2
# 2 variables came out as nearly significant: DISP_EUinPM10_AnnualAv_WP99.5 (p=0.078) & nhorsesWghtDist.3000m.sum (p=0.083) 


# Re-run adonis2 with COPD status in the formula - as a covariate  - TBC
# Maybe I need to add copd status to the PERMANOVA to adjust for differences?
ARG.cluster.dataframe.exp$copdcaco <- ARGcluster.aggl.noblanks.LUR@sam_data$copdcaco # add copd status into the dataframe
set.seed(123)
adonis.MinDistAnyFarm.NEG.copd <- adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$copdcaco + ARG.cluster.dataframe.exp$MinDistAnyFarm.NEG, permutations = 999)

adonis.MinDistAnyFarm.INV.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$MinDistAnyFarm.INV, permutations = 999)
set.seed(123)
adonis.AllFarm.3000m.copd <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$copdcaco + ARG.cluster.dataframe.exp$AllFarm.3000m, permutations = 999)
set.seed(123)
adonis.AllFarm.1000m.copd <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$copdcaco + ARG.cluster.dataframe.exp$AllFarm.1000m, permutations = 999)
set.seed(123)
adonis.AllFarm.500m.copd <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$copdcaco + ARG.cluster.dataframe.exp$AllFarm.500m, permutations = 999)
set.seed(123)
adonis.AllFarm.250m.copd <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$AllFarm.250m, permutations = 999)
set.seed(123)
adonis.npigsWghtDist.1000m.sum.copd <-adonis2(BC.distance.exp~ ARG.cluster.dataframe.exp$copdcaco + ARG.cluster.dataframe.exp$npigsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.npoultryWghtDist.1000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$npoultryWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.ncowsWghtDist.1000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$ncowsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nhorsesWghtDist.1000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nhorsesWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.ngoatsWghtDist.1000m.sum.copd<-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$ngoatsWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nsheepWghtDist.1000m.sum.copd<-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nsheepWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.npigsWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$npigsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.npoultryWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$npoultryWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.ncowsWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$ncowsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nhorsesWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nhorsesWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.ngoatsWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$ngoatsWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nsheepWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nsheepWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.nAnyFarmWghtDist.1000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nAnyFarmWghtDist.1000m.sum, permutations = 999)
set.seed(123)
adonis.nAnyFarmWghtDist.3000m.sum.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$nAnyFarmWghtDist.3000m.sum, permutations = 999)
set.seed(123)
adonis.DISP_EUinPM10_AnnualAv_WP99.5.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$DISP_EUinPM10_AnnualAv_WP99.5, permutations = 999)
set.seed(123)
adonis.DISP_PM10CONC_AnnualAv_WP99.5.copd <-adonis2(BC.distance.exp~ARG.cluster.dataframe.exp$copdcaco +  ARG.cluster.dataframe.exp$DISP_PM10CONC_AnnualAv_WP99.5, permutations = 999)

# None come out as significant.
#print the p-values of each 
adonis.MinDistAnyFarm.NEG.copd$`Pr(>F)`
adonis.MinDistAnyFarm.INV.copd$`Pr(>F)`
adonis.AllFarm.3000m.copd$`Pr(>F)`
adonis.AllFarm.1000m.copd$`Pr(>F)`
adonis.AllFarm.500m.copd $`Pr(>F)`
adonis.AllFarm.250m.copd$`Pr(>F)`
adonis.npigsWghtDist.1000m.sum.copd$`Pr(>F)`
adonis.npoultryWghtDist.1000m.sum.copd$`Pr(>F)` 
adonis.ncowsWghtDist.1000m.sum.copd$`Pr(>F)` 
adonis.nhorsesWghtDist.1000m.sum.copd$`Pr(>F)`
adonis.ngoatsWghtDist.1000m.sum.copd$`Pr(>F)`
adonis.nsheepWghtDist.1000m.sum.copd$`Pr(>F)`
adonis.npigsWghtDist.3000m.sum.copd$`Pr(>F)`
adonis.npoultryWghtDist.3000m.sum.copd$`Pr(>F)` 
adonis.ncowsWghtDist.3000m.sum.copd$`Pr(>F)` 
adonis.nhorsesWghtDist.3000m.sum.copd$`Pr(>F)`
adonis.ngoatsWghtDist.3000m.sum.copd$`Pr(>F)` 
adonis.nsheepWghtDist.3000m.sum.copd$`Pr(>F)`
adonis.nAnyFarmWghtDist.1000m.sum.copd$`Pr(>F)`
adonis.nAnyFarmWghtDist.3000m.sum.copd$`Pr(>F)` 
adonis.DISP_EUinPM10_AnnualAv_WP99.5.copd$`Pr(>F)`
adonis.DISP_PM10CONC_AnnualAv_WP99.5.copd$`Pr(>F)` 

# Now all p-values are adjusted for copd status and none come out as significantly associated with beta diversity

# Print the R^2 of each exposure proxy - see the first column for the R2 of the exposure proxy 

adonis.MinDistAnyFarm.NEG.copd$R2
adonis.MinDistAnyFarm.INV.copd$R2
adonis.AllFarm.3000m.copd$R2
adonis.AllFarm.1000m.copd$R2
adonis.AllFarm.500m.copd $R2
adonis.AllFarm.250m.copd$R2
adonis.npigsWghtDist.1000m.sum.copd$R2
adonis.npoultryWghtDist.1000m.sum.copd$R2 
adonis.ncowsWghtDist.1000m.sum.copd$R2 
adonis.nhorsesWghtDist.1000m.sum.copd$R2
adonis.ngoatsWghtDist.1000m.sum.copd$R2
adonis.nsheepWghtDist.1000m.sum.copd$R2
adonis.npigsWghtDist.3000m.sum.copd$R2
adonis.npoultryWghtDist.3000m.sum.copd$R2 
adonis.ncowsWghtDist.3000m.sum.copd$R2 
adonis.nhorsesWghtDist.3000m.sum.copd$R2
adonis.ngoatsWghtDist.3000m.sum.copd$R2 
adonis.nsheepWghtDist.3000m.sum.copd$R2
adonis.nAnyFarmWghtDist.1000m.sum.copd$R2
adonis.nAnyFarmWghtDist.3000m.sum.copd$R2 
adonis.DISP_EUinPM10_AnnualAv_WP99.5.copd$R2
adonis.DISP_PM10CONC_AnnualAv_WP99.5.copd$R2 


```
#### Multivariable PERMANOVA 
```{r}
#' Multiple Permanovas
#'
#'Function performs individual, \code{\link{adonis}}-style permanovas for every/selected elements of \code{sample_variables(physeq)}
#'
#' @param physeq phyloseq object. A distance matrix is created using \code{otu_table(physeq)}; permanovas are computed for each element of \code{sample_variables(physeq)}
#' @param distm choice of dissimilarity index according to \code{\link{vegdist}}. Options are "manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard",
#'              "gower", "altGower", "morisita", "horn", "mountford", "raup" , "binomial", "chao", "cao" or "mahalanobis".
#' @param perms the number of permutations to be performed. Default is 999
#' @param vars Empty=default tests all metadata variables (should be factor levels >=2). Otherwise give a vector c("var1","var2",...etc)
#' @param verbose boolean. Default is TRUE and prints progress.
#' @param terms character vector containing the variable names to be used in a multivariable model formula passed to adonis.
#'
#'
#' @return a dataframe containing p-values of each comparison in the first column, and the number of levels for that variable in the second.
#'
#' @export
#' @import vegan
#' @import phyloseq
#' @note if \code{sample_data(physeq)} contains NAs, the function will convert these to an additional level "none"
#' @note Added the complete adonis output in verbose mode (alex 20200515)
#' @note Added option to specifcy a formula which is passed to the adonis function.  
#'
#' @author Stephanie Jurburg and updated/extended by Alex Bossers \email{alex.bossers@wur.nl} and Warner van Kersen \email{w.vankersen@uu.nl}
#' @examples
#'
#' per <- permanovas(rarefied, "bray")
#'
#'
#'

permanovas_formula <- function(physeq, distm="bray", perms=999, vars="", verbose=TRUE,terms=NULL) {
  
  if( vars[1] == "" ) {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq))), factor))
  } else {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq)[,vars])), factor))
  }
  
  if (physeq@otu_table@taxa_are_rows ==TRUE ) {
    otutab <- t(otu_table(physeq))
  } else {
    otutab <- (otu_table(physeq))
  }
  
  distn <- vegdist(decostand(otutab, "hellinger"), method=distm)
  if(verbose){
    message( paste0("Using Hellinger transformed data and distance metric '",distm,"'") )
  }
  
  
  if(is.null(terms)){ # No terms specified, run univariate permanova's
  
  ADONIS <- matrix(nrow=length(vars), ncol=2)
  row.names(ADONIS) <- colnames(vars)
  colnames(ADONIS) <- c("P.value", "Levels")  
    
  for (i in seq_along (vars)) {
    if (anyNA(vars[[i]])){
      levels(vars[[i]]) <- c(levels(vars[[i]]), "none")
      vars[[i]][is.na(vars[[i]])] <-  "none"
    }
    if ( nlevels(vars[[i]]) == 1 ) {
      message( paste0( "Skipping variable '",rownames(ADONIS)[i],"', not enough levels!") )
      next
    }
    
    adonisResult <- adonis(distn~as.factor(na.pass(vars[[1]])), permutations=perms)
    if(verbose){
      cat(paste0("\nVariable: ",colnames(vars)[i]),"\n")
      print(adonisResult)
      cat("\n\n")
    }
    ADONIS[i,1] <- adonisResult$aov.tab$`Pr(>F)`[1]
    ADONIS[i,2] <- nlevels(vars[[i]])
  }
  return(ADONIS)
  }  
  else{
  # Define formula using terms
  form = reformulate(terms,"distn")
  adonisResult <- adonis(form,data=vars[,names(vars) %in% terms], permutations=perms)
  return(adonisResult)
  }
}

# Run the multivariate PERMANOVA 
# specify terms in the formula
names(ARGcluster.aggl.noblanks.exp@sam_data) # print names of metadata fields 
ARGcluster.aggl.noblanks.exp@sam_data$copdcaco <- ARGcluster.aggl.noblanks@sam_data$copdcaco # add in COPD status into metadata
names(ARGcluster.aggl.noblanks.exp@sam_data)
# "MinDistAnyFarm.NEG","MinDistAnyFarm.INV","AllFarm.3000m","AllFarm.1000m", "AllFarm.500m"                  "AllFarm.250m"                  "npigsWghtDist.1000m.sum","npoultryWghtDist.1000m.sum","ncowsWghtDist.1000m.sum","nhorsesWghtDist.1000m.sum","ngoatsWghtDist.1000m.sum","nsheepWghtDist.1000m.sum","nfuranimsWghtDist.1000m.sum","npigsWghtDist.3000m.sum","npoultryWghtDist.3000m.sum","ncowsWghtDist.3000m.sum","nhorsesWghtDist.3000m.sum","ngoatsWghtDist.3000m.sum","nsheepWghtDist.3000m.sum","nfuranimsWghtDist.3000m.sum","nAnyFarmWghtDist.1000m.sum","nAnyFarmWghtDist.3000m.sum","DISP_EUinPM10_AnnualAv_WP99.5","DISP_PM10CONC_AnnualAv_WP99.5","copdcaco"  
# I don't want to include the fur animals variables so not included in variables and terms parameters
variables_for_multivariable_permanova <- c("MinDistAnyFarm.NEG","MinDistAnyFarm.INV","AllFarm.3000m","AllFarm.1000m","AllFarm.500m","AllFarm.250m","npigsWghtDist.1000m.sum","npoultryWghtDist.1000m.sum","ncowsWghtDist.1000m.sum","nhorsesWghtDist.1000m.sum","ngoatsWghtDist.1000m.sum","nsheepWghtDist.1000m.sum","npigsWghtDist.3000m.sum","npoultryWghtDist.3000m.sum","ncowsWghtDist.3000m.sum","nhorsesWghtDist.3000m.sum","ngoatsWghtDist.3000m.sum","nsheepWghtDist.3000m.sum","nAnyFarmWghtDist.1000m.sum","nAnyFarmWghtDist.3000m.sum","DISP_EUinPM10_AnnualAv_WP99.5","DISP_PM10CONC_AnnualAv_WP99.5","copdcaco")
terms_vector <- c("MinDistAnyFarm.NEG","MinDistAnyFarm.INV","AllFarm.3000m","AllFarm.1000m","AllFarm.500m","AllFarm.250m","npigsWghtDist.1000m.sum","npoultryWghtDist.1000m.sum","ncowsWghtDist.1000m.sum","nhorsesWghtDist.1000m.sum","ngoatsWghtDist.1000m.sum","nsheepWghtDist.1000m.sum","npigsWghtDist.3000m.sum","npoultryWghtDist.3000m.sum","ncowsWghtDist.3000m.sum","nhorsesWghtDist.3000m.sum","ngoatsWghtDist.3000m.sum","nsheepWghtDist.3000m.sum","nAnyFarmWghtDist.1000m.sum","nAnyFarmWghtDist.3000m.sum","DISP_EUinPM10_AnnualAv_WP99.5","DISP_PM10CONC_AnnualAv_WP99.5","copdcaco")

Multivar.permanova <- permanovas_formula(ARGcluster.aggl.noblanks.exp, distm="bray", perms=999, vars=variables_for_multivariable_permanova, verbose=TRUE,terms=terms_vector)

Multivar.permanova <- permanovas_formula(ARGcluster.aggl.noblanks.exp, distm="bray", perms=999,terms=c("MinDistAnyFarm.NEG","MinDistAnyFarm.INV","AllFarm.3000m"))

saveRDS(ARGcluster.aggl.noblanks.exp, "PS_Object_BCH_PERMANOVA")

# Error about colnames received - researched online - try changing adonis function to adonis2 function...
permanovas_formula_adonis2 <- function(physeq, distm="bray", perms=999, vars="", verbose=TRUE,terms=NULL) {
  
  if( vars[1] == "" ) {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq))), factor))
  } else {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq)[,vars])), factor))
  }
  
  if (physeq@otu_table@taxa_are_rows ==TRUE ) {
    otutab <- t(otu_table(physeq))
  } else {
    otutab <- (otu_table(physeq))
  }
  
  distn <- vegdist(decostand(otutab, "hellinger"), method=distm)
  if(verbose){
    message( paste0("Using Hellinger transformed data and distance metric '",distm,"'") )
  }
  
  
  if(is.null(terms)){ # No terms specified, run univariate permanova's
  
  ADONIS <- matrix(nrow=length(vars), ncol=2)
  row.names(ADONIS) <- colnames(vars)
  colnames(ADONIS) <- c("P.value", "Levels")  
    
  for (i in seq_along (vars)) {
    if (anyNA(vars[[i]])){
      levels(vars[[i]]) <- c(levels(vars[[i]]), "none")
      vars[[i]][is.na(vars[[i]])] <-  "none"
    }
    if ( nlevels(vars[[i]]) == 1 ) {
      message( paste0( "Skipping variable '",rownames(ADONIS)[i],"', not enough levels!") )
      next
    }
    
    adonisResult <- adonis2(distn~as.factor(na.pass(vars[[1]])), permutations=perms)
    if(verbose){
      cat(paste0("\nVariable: ",colnames(vars)[i]),"\n")
      print(adonisResult)
      cat("\n\n")
    }
    ADONIS[i,1] <- adonisResult$aov.tab$`Pr(>F)`[1]
    ADONIS[i,2] <- nlevels(vars[[i]])
  }
  return(ADONIS)
  }  
  else{
  # Define formula using terms
  form = reformulate(terms,"distn")
  adonisResult <- adonis2(form,data=vars[,names(vars) %in% terms], permutations=perms)
  return(adonisResult)
  }
}


adonis2_multivar_perm <- permanovas_formula_adonis2(ARGcluster.aggl.noblanks.exp, distm="bray", perms=999, vars=variables_for_multivariable_permanova, verbose=TRUE,terms=terms_vector)
# I now get some output but not what I'm looking for...

# New try - 03/03/2022

factor(ARGcluster.aggl.noblanks.exp@sam_data$)

# make into factors 
# copdcaco and n farms 500m 
# make into factors 
# leave vars section blank
# form - enter formula 
# try new function first of all

permanovas_formula(ARGcluster.aggl.noblanks.exp, distm="bray",vars="copdcaco", perms=999, verbose=TRUE,form=distn~copdcaco)

ARGcluster.aggl.noblanks.exp@sam_data$MinDistAnyFarm.NEG
debug(permanovas_formula)
```
#### Bivariable PERMANOVA 
```{r}
#' Multiple Permanovas
#'
#'Function performs individual, \code{\link{adonis}}-style permanovas for every/selected elements of \code{sample_variables(physeq)}
#'
#' @param physeq phyloseq object. A distance matrix is created using \code{otu_table(physeq)}; permanovas are computed for each element of \code{sample_variables(physeq)}
#' @param distm choice of dissimilarity index according to \code{\link{vegdist}}. Options are "manhattan", "euclidean", "canberra", "bray", "kulczynski", "jaccard",
#'              "gower", "altGower", "morisita", "horn", "mountford", "raup" , "binomial", "chao", "cao" or "mahalanobis".
#' @param perms the number of permutations to be performed. Default is 999
#' @param vars Empty=default tests all metadata variables (should be factor levels >=2). Otherwise give a vector c("var1","var2",...etc)
#' @param verbose boolean. Default is TRUE and prints progress.
#' @param terms character vector containing the variable names to be used in a multivariable model formula passed to adonis.
#'
#'
#' @return a dataframe containing p-values of each comparison in the first column, and the number of levels for that variable in the second.
#'
#' @export
#' @import vegan
#' @import phyloseq
#' @note if \code{sample_data(physeq)} contains NAs, the function will convert these to an additional level "none"
#' @note Added the complete adonis output in verbose mode (alex 20200515)
#' @note Added option to specifcy a formula which is passed to the adonis function.  
#'
#' @author Stephanie Jurburg and updated/extended by Alex Bossers \email{alex.bossers@wur.nl} and Warner van Kersen \email{w.vankersen@uu.nl}
#' @examples
#'
#' per <- permanovas(rarefied, "bray")
#'
#'
#'

permanovas_formula <- function(physeq, distm="bray", perms=999, vars="", verbose=TRUE,terms=NULL) {
  
  if( vars[1] == "" ) {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq))), factor))
  } else {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq)[,vars])), factor))
  }
  
  if (physeq@otu_table@taxa_are_rows ==TRUE ) {
    otutab <- t(otu_table(physeq))
  } else {
    otutab <- (otu_table(physeq))
  }
  
  distn <- vegdist(decostand(otutab, "hellinger"), method=distm)
  if(verbose){
    message( paste0("Using Hellinger transformed data and distance metric '",distm,"'") )
  }
  
  
  if(is.null(terms)){ # No terms specified, run univariate permanova's
  
  ADONIS <- matrix(nrow=length(vars), ncol=2)
  row.names(ADONIS) <- colnames(vars)
  colnames(ADONIS) <- c("P.value", "Levels")  
    
  for (i in seq_along (vars)) {
    if (anyNA(vars[[i]])){
      levels(vars[[i]]) <- c(levels(vars[[i]]), "none")
      vars[[i]][is.na(vars[[i]])] <-  "none"
    }
    if ( nlevels(vars[[i]]) == 1 ) {
      message( paste0( "Skipping variable '",rownames(ADONIS)[i],"', not enough levels!") )
      next
    }
    
    adonisResult <- adonis(distn~as.factor(na.pass(vars[[1]])), permutations=perms)
    if(verbose){
      cat(paste0("\nVariable: ",colnames(vars)[i]),"\n")
      print(adonisResult)
      cat("\n\n")
    }
    ADONIS[i,1] <- adonisResult$aov.tab$`Pr(>F)`[1]
    ADONIS[i,2] <- nlevels(vars[[i]])
  }
  return(ADONIS)
  }  
  else{
  # Define formula using terms
  form = reformulate(terms,"distn")
  adonisResult <- adonis(form,data=vars[,names(vars) %in% terms], permutations=perms)
  return(adonisResult)
  }
}

# New version of function that WvK created following issues with the previous one - try with this version first
permanovas_formula2 <- function(physeq, distm="bray", perms=999, vars="", verbose=TRUE,form=NULL) {
  
  if( vars[1] == "" ) {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq))), factor))
  } else {
    vars <- as.data.frame(lapply(as.data.frame(as.matrix(sample_data(physeq)[,vars])), factor))
  }
  
  if (physeq@otu_table@taxa_are_rows ==TRUE ) {
    otutab <- t(otu_table(physeq))
  } else {
    otutab <- (otu_table(physeq))
  }
  
  distn <- vegdist(decostand(otutab, "hellinger"), method=distm)
  if(verbose){
    message( paste0("Using Hellinger transformed data and distance metric '",distm,"'") )
  }
  
  
  if(is.null(form)){ # No terms specified, run univariate permanova's
  
  ADONIS <- matrix(nrow=length(vars), ncol=2)
  row.names(ADONIS) <- colnames(vars)
  colnames(ADONIS) <- c("P.value", "Levels")  
    
  for (i in seq_along (vars)) {
    if (anyNA(vars[[i]])){
      levels(vars[[i]]) <- c(levels(vars[[i]]), "none")
      vars[[i]][is.na(vars[[i]])] <-  "none"
    }
    if ( nlevels(vars[[i]]) == 1 ) {
      message( paste0( "Skipping variable '",rownames(ADONIS)[i],"', not enough levels!") )
      next
    }
    
    adonisResult <- adonis(distn~as.factor(na.pass(vars[[1]])), permutations=perms)
    if(verbose){
      cat(paste0("\nVariable: ",colnames(vars)[i]),"\n")
      print(adonisResult)
      cat("\n\n")
    }
    ADONIS[i,1] <- adonisResult$aov.tab$`Pr(>F)`[1]
    ADONIS[i,2] <- nlevels(vars[[i]])
  }
  return(ADONIS)
  }  
  else{
  # Define formula using terms
  adonisResult <- adonis(form,data=vars[,names(vars) %in% terms], permutations=perms)
  return(adonisResult)
  }
}

# New try - 14/03/2022
ARGcluster.aggl.noblanks.exp.quant@sam_data$copdcaco <- Final.ps.noblanks@sam_data$copdcaco
factor(ARGcluster.aggl.noblanks.exp.quant@sam_data$copdcaco)

# make into factors 
# copdcaco and n farms 500m 
# make into factors 
# leave vars section blank
# form - enter formula 
# try new function first of all

permanovas_formula(ARGcluster.aggl.noblanks.exp.quant, distm="bray",vars="copdcaco", perms=999, verbose=TRUE,form=distn~copdcaco)

# make variable into factor
factor(ARGcluster.aggl.noblanks.exp.quant@sam_data$ngoatsWghtDist.3000m.sum)
2
permanovas_formula2(ARGcluster.aggl.noblanks.exp, distm="bray",vars=c("copdcaco", "ngoatsWghtDist.3000m.sum"), perms=999, verbose=TRUE,form=distn~copdcaco + ngoatsWghtDist.3000m.sum)

permanovas_formula2(ARGcluster.aggl.noblanks.exp, distm="bray", perms=999, verbose=TRUE,form=distn~copdcaco + ngoatsWghtDist.3000m.sum)

undebug(permanovas_formula2)

# All attempts not working

# Will do manually - see chunk above for all adonis functions with copd status included as variable

```

### DA analysis
#### DESeq2 (unfiltered data)
```{r}
# Since the DESeq2 function requires count data which has not been corrected for gene length, I need to create a new ps object with only corrections for volume input and qPCR bacterial count
Countdata.DESeq2 # This is the ps object to be used for DESeq2 analysis (not gene length corrected but corrected for input volumes and qPCR bacterial counts). BUT this is not at the 90% cluster level so I should agglomerate this ps object (using previously created tax_glom2 function from AB - see below too)

# Use function tax_glom2 - 6 is the ARGCluster level in the tax_table of Final.ps object. This creates a new ps object with ARGcluster as default 
ARGcluster.aggl.DESeq2 <- tax_glom2(Countdata.DESeq2, taxrank=rank_names(Countdata.DESeq2)[6], NArm=TRUE)
taxa_names(ARGcluster.aggl.DESeq2)
# Confused why the 'taxa_names' are not the arg cluster names? AB responded saying: "What you see is perfectly normal. If you checkout the tax_table you will see that the lowest level is cluster. The lower ranks should be NA. That is why you use explaining Ps object names to realise some aggregation happened. The data is underneath still linked on indiv accno level but incomplete. Only one per arg cluster should be present.  If you now directly ask the taxon name it is indeed "wrong" the accession name. Just use the arg cluster name corresponding to it and you should be fine. Most functions suited for Ps object allow to mention the tax rank or the fill rank to use. They handle it for you underneath in the same way.No worries, just check above and if that matches. Just continue and be sure to use the name of the tax rank. 
# Check tax-table of ps object
tax_table(ARGcluster.aggl.DESeq2)

# Add new sample data (with exposure proxies) to the new ps object 
ARGcluster.aggl.DESeq2.exp <- ARGcluster.aggl.DESeq2
sample_data(ARGcluster.aggl.DESeq2.exp)
sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(ARGcluster.aggl.DESeq2.exp) <- sam.data.new
ARGcluster.aggl.DESeq2.exp@sam_data

# I will start with continuous exposure covariates in the DESeq2 analysis as a constant fold change is possible for each unit of increase of the variables 
# column names are (specific) ARG names, agglomeration is correct (lowest level is ARG cluster) but col names are not correct - I will rename the column names to the AMR classes for presentation in the heatmap
colnames(ARGcluster.aggl.DESeq2.exp@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

#Inspect the new ps object
ARGcluster.aggl.DESeq2.exp
ARGcluster.aggl.DESeq2.exp@otu_table # clusters labelled correctly 
ARGcluster.aggl.DESeq2.exp@sam_data # exposure proxy sample data added 
ARGcluster.aggl.DESeq2.exp@sam_data$DISP_EUinPM10_AnnualAv_WP99.5 # exposure proxy data is continuous 
str(ARGcluster.aggl.DESeq2.exp)

# Now ready to run the deseq2 analysis using the phyloseq_to_deseq2 function
# Start with assessing endotoxin concentration effects
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp,~DISP_EUinPM10_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont <- DESeq(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont <- results(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont
#summary of differential gene expression
summary(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
head(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.ordered <- deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont[order(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont$pvalue),]
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.ordered #tet(O/W/32/O/W/O) has the greatest difference for each fold change in endotoxin concentration

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.ordered$padj < 0.1, na.rm=TRUE) # 5 ARG clusters are <0.1

# Plot to show the log2 fold changes attributable to a given variable over the mean of normalized counts for all the samples in the DESeqDataSet. Points will be colored red if the adjusted p value is less than 0.1. Points which fall out of the window are plotted as open triangles pointing either up or down.
plotMA(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont, ylim=c(-2,2))

# Shrink the log2fold changes as these provide better LFC estimates
resultsNames(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
BiocManager::install("apeglm")
library(apeglm)
resLFCshrink.exp.cont <- lfcShrink(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont, coef="DISP_EUinPM10_AnnualAv_WP99.5", type="apeglm")
resLFCshrink.exp.cont

#It is more useful visualize the MA-plot for the shrunken log2 fold changes, which remove the noise associated with log2 fold changes from low count genes without requiring arbitrary filtering thresholds.
plotMA(resLFCshrink.exp.cont, ylim=c(-2,2)) # plot looks odd...?

# Using ggplot2 to create an MA plot
# Load libraries
# install.packages(c("ggplot2", "scales", "viridis"))
library(ggplot2)
library(scales) # needed for oob parameter
library(viridis)
# Coerce to a data frame
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF <- as.data.frame(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont)
# Examine this data frame
head(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF)
# Set a boolean column for significance
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF$significant <- ifelse(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF$padj < .1, "Significant", NA)
# Plot the results similar to DEseq2
ggplot(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF, aes(baseMean, log2FoldChange, colour=significant)) + geom_point(size=1) + scale_y_continuous(limits=c(-3, 3)) + scale_x_log10() + geom_hline(yintercept = 0, colour="tomato1", size=2) + labs(x="mean of normalized counts", y="log fold change") + scale_colour_manual(name="q-value", values=("Significant"="red"), na.value="grey50") + theme_bw()
# Let's add some more detail
ggplot(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont.DF, aes(baseMean, log2FoldChange, colour=padj)) + geom_point(size=1) + scale_y_continuous(limits=c(-3, 3)) + scale_x_log10() + geom_hline(yintercept = 0, colour="darkorchid4", size=1, linetype="longdash") + labs(x="mean of normalized counts", y="log fold change") + scale_colour_viridis(direction=-1, trans='sqrt') + theme_bw() + geom_density_2d(colour="black", size=2)

# volcano plot
#reset par
par(mfrow=c(1,1))
# Make a basic volcano plot
with(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont, plot(log2FoldChange, -log10(pvalue), pch=20, main="Volcano plot", xlim=c(-3,3)))

# Add colored points: blue if padj<0.01, red if log2FC>1 and padj<0.05)
with(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont, plot(log2FoldChange, -log10(pvalue), pch=20, main="Volcano plot", xlim=c(-3,3)))
with(subset(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.cont, padj<.1 ), points(log2FoldChange, -log10(pvalue), pch=20, col="red"))

# Now look at PM10 concentration effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp,~DISP_PM10CONC_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont <- DESeq(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont <- results(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont
#summary of differential gene expression
summary(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont)
head(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont.ordered <- deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont[order(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont$pvalue),]
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont.ordered #erm(T)_4_AJ488494 has the greatest difference for each fold change in endotoxin concentration

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.cont.ordered$padj < 0.1, na.rm=TRUE) # 2 ARG clusters have p-value <0.1


# Now look at nhorsesWghtDist.3000m.sum effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.nhorsesWghtDist.3000m.sum.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp,~nhorsesWghtDist.3000m.sum)

# RUN  DESeq function 
deseq2.exp.nhorsesWghtDist.3000m.sum.cont <- DESeq(deseq2.exp.nhorsesWghtDist.3000m.sum.cont)
deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont <- results(deseq2.exp.nhorsesWghtDist.3000m.sum.cont)
deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont
#summary of differential gene expression
summary(deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont)
head(deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont)$description

# Order the results table by the smallest p value:
deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont.ordered <- deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont[order(deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont$pvalue),]
deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont.ordered #erm(T)_4_AJ488494 has the greatest difference for each fold change in endotoxin concentration

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.nhorsesWghtDist.3000m.sum.cont$padj < 0.1, na.rm=TRUE) # 3 ARG clusters have p-value <0.1



# Now look at ngoatsWghtDist.3000m.sum effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.ngoatsWghtDist.3000m.sum.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp,~ngoatsWghtDist.3000m.sum)

# RUN  DESeq function 
deseq2.exp.ngoatsWghtDist.3000m.sum.cont <- DESeq(deseq2.exp.ngoatsWghtDist.3000m.sum.cont)
deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont <- results(deseq2.exp.ngoatsWghtDist.3000m.sum.cont)
deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont
#summary of differential gene expression
summary(deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont)
head(deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont)$description

# Order the results table by the smallest p value:
deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont.ordered <- deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont[order(deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont$pvalue),]
deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont.ordered #erm(T)_4_AJ488494 has the greatest difference for each fold change in endotoxin concentration

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.ngoatsWghtDist.3000m.sum.cont$padj < 0.1, na.rm=TRUE) # 3 ARG clusters have p-value <0.1


# DESeq2 with exposure quantiles
# Now I need to categorise my exposure proxy variables which are currently continuous numeric to quartiles (factor variables)
# I will use the cut() function 
?cut
# MinDistAnyFarm.NEG
ARGcluster.aggl.DESeq2.exp.quart <- ARGcluster.aggl.DESeq2.exp
ARGcluster.aggl.DESeq2.exp.quart@sam_data$MinDistAnyFarm.NEG<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$MinDistAnyFarm.NEG, breaks=4, labels =c("1st quartile (minDistAnyFarm.NEG)", "2nd quartile (minDistAnyFarm.NEG)", "3rd quartile (minDistAnyFarm.NEG)", "4th quartile (minDistAnyFarm.NEG)"))
# MinDistAnyFarm.INV
ARGcluster.aggl.DESeq2.exp.quart@sam_data$MinDistAnyFarm.INV<- cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$MinDistAnyFarm.INV, breaks=4, labels =c("1st quartile (MinDistAnyFarm.INV)", "2nd quartile (MinDistAnyFarm.INV)", "3rd quartile (MinDistAnyFarm.INV)", "4th quartile (MinDistAnyFarm.INV)"))
# AllFarm.3000m
ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.3000m<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.3000m, breaks=4, labels =c("1st quartile (AllFarm.3000m)", "2nd quartile (AllFarm.3000m)", "3rd quartile (AllFarm.3000m)", "4th quartile (AllFarm.3000m)"))
#AllFarm.1000m
ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.1000m<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.1000m, breaks=4, labels =c("1st quartile (AllFarm.1000m)", "2nd quartile (AllFarm.1000m)", "3rd quartile (AllFarm.1000m)", "4th quartile (AllFarm.1000m)"))
#AllFarm.500m
ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.500m<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.500m, breaks=4, labels =c("1st quartile (AllFarm.500m)", "2nd quartile (AllFarm.500m)", "3rd quartile (AllFarm.500m)", "4th quartile (AllFarm.500m)"))
#AllFarm.250m
ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.250m<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$AllFarm.250m, breaks=4, labels =c("1st quartile (AllFarm.250m)", "2nd quartile (AllFarm.250m)", "3rd quartile (AllFarm.250m)", "4th quartile (AllFarm.250m)"))
#npigsWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$npigsWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$npigsWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (npigsWghtDist.1000m.sum)", "2nd quartile (npigsWghtDist.1000m.sum)", "3rd quartile (npigsWghtDist.1000m.sum)", "4th quartile (npigsWghtDist.1000m.sum)"))
#npoultryWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$npoultryWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$npoultryWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (npoultryWghtDist.1000m.sum)", "2nd quartile (npoultryWghtDist.1000m.sum)", "3rd quartile (npoultryWghtDist.1000m.sum)", "4th quartile (npoultryWghtDist.1000m.sum)"))
#ncowsWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$ncowsWghtDist.1000m.sum<- cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$ncowsWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (ncowsWghtDist.1000m.sum)", "2nd quartile (ncowsWghtDist.1000m.sum)", "3rd quartile (ncowsWghtDist.1000m.sum)", "4th quartile (ncowsWghtDist.1000m.sum)"))
#nhorsesWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nhorsesWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nhorsesWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (nhorsesWghtDist.1000m.sum)", "2nd quartile (nhorsesWghtDist.1000m.sum)", "3rd quartile (nhorsesWghtDist.1000m.sum)", "4th quartile (nhorsesWghtDist.1000m.sum)"))
#ngoatsWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$ngoatsWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$ngoatsWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (ngoatsWghtDist.1000m.sum)", "2nd quartile (ngoatsWghtDist.1000m.sum)", "3rd quartile (ngoatsWghtDist.1000m.sum)", "4th quartile (ngoatsWghtDist.1000m.sum)"))
#nsheepWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nsheepWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nsheepWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (nsheepWghtDist.1000m.sum)", "2nd quartile (nsheepWghtDist.1000m.sum)", "3rd quartile (nsheepWghtDist.1000m.sum)", "4th quartile (nsheepWghtDist.1000m.sum)"))
#nfuranimsWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nfuranimsWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nfuranimsWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (nfuranimsWghtDist.1000m.sum)", "2nd quartile (nfuranimsWghtDist.1000m.sum)", "3rd quartile (nfuranimsWghtDist.1000m.sum)", "4th quartile (nfuranimsWghtDist.1000m.sum)"))
#npigsWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$npigsWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$npigsWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (npigsWghtDist.3000m.sum)", "2nd quartile (npigsWghtDist.3000m.sum)", "3rd quartile (npigsWghtDist.3000m.sum)", "4th quartile (npigsWghtDist.3000m.sum)"))
#npoultryWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$npoultryWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$npoultryWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (npoultryWghtDist.3000m.sum)", "2nd quartile (npoultryWghtDist.3000m.sum)", "3rd quartile (npoultryWghtDist.3000m.sum)", "4th quartile (npoultryWghtDist.3000m.sum)"))
#ncowsWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$ncowsWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$ncowsWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (ncowsWghtDist.3000m.sum)", "2nd quartile (ncowsWghtDist.3000m.sum)", "3rd quartile (ncowsWghtDist.3000m.sum)", "4th quartile (ncowsWghtDist.3000m.sum)"))
#nhorsesWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nhorsesWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nhorsesWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (nhorsesWghtDist.3000m.sum)", "2nd quartile (nhorsesWghtDist.3000m.sum)", "3rd quartile (nhorsesWghtDist.3000m.sum)", "4th quartile (nhorsesWghtDist.3000m.sum)"))
#ngoatsWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$ngoatsWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$ngoatsWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (ngoatsWghtDist.3000m.sum)", "2nd quartile (ngoatsWghtDist.3000m.sum)", "3rd quartile (ngoatsWghtDist.3000m.sum)", "4th quartile (ngoatsWghtDist.3000m.sum)"))
#nsheepWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nsheepWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nsheepWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (nsheepWghtDist.3000m.sum)", "2nd quartile (nsheepWghtDist.3000m.sum)", "3rd quartile (nsheepWghtDist.3000m.sum)", "4th quartile (nsheepWghtDist.3000m.sum)"))
#nfuranimsWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nfuranimsWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nfuranimsWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (nfuranimsWghtDist.3000m.sum)", "2nd quartile (nfuranimsWghtDist.3000m.sum)", "3rd quartile (nfuranimsWghtDist.3000m.sum)", "4th quartile (nfuranimsWghtDist.3000m.sum)"))
#nAnyFarmWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nAnyFarmWghtDist.1000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nAnyFarmWghtDist.1000m.sum, breaks=4, labels =c("1st quartile (nAnyFarmWghtDist.1000m.sum)", "2nd quartile (nAnyFarmWghtDist.1000m.sum)", "3rd quartile (nAnyFarmWghtDist.1000m.sum)", "4th quartile (nAnyFarmWghtDist.1000m.sum)"))
#nAnyFarmWghtDist.3000m.sum
ARGcluster.aggl.DESeq2.exp.quart@sam_data$nAnyFarmWghtDist.3000m.sum<-cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$nAnyFarmWghtDist.3000m.sum, breaks=4, labels =c("1st quartile (nAnyFarmWghtDist.3000m.sum)", "2nd quartile (nAnyFarmWghtDist.3000m.sum)", "3rd quartile (nAnyFarmWghtDist.3000m.sum)", "4th quartile (nAnyFarmWghtDist.3000m.sum)"))
#DISP_EUinPM10_AnnualAv_WP99.5
ARGcluster.aggl.DESeq2.exp.quart@sam_data$DISP_EUinPM10_AnnualAv_WP99.5<- cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, breaks=4, labels =c("1st quartile (DISP_EUinPM10_AnnualAv_WP99.5)", "2nd quartile (DISP_EUinPM10_AnnualAv_WP99.5)", "3rd quartile (DISP_EUinPM10_AnnualAv_WP99.5)", "4th quartile (DISP_EUinPM10_AnnualAv_WP99.5)"))
#DISP_PM10CONC_AnnualAv_WP99.5
ARGcluster.aggl.DESeq2.exp.quart@sam_data$DISP_PM10CONC_AnnualAv_WP99.5<- cut(ARGcluster.aggl.DESeq2.exp.quart@sam_data$DISP_PM10CONC_AnnualAv_WP99.5, breaks=4, labels =c("1st quartile (DISP_PM10CONC_AnnualAv_WP99.5)", "2nd quartile (DISP_PM10CONC_AnnualAv_WP99.5)", "3rd quartile (DISP_PM10CONC_AnnualAv_WP99.5)", "4th quartile (DISP_PM10CONC_AnnualAv_WP99.5)"))


# column names are (specific) ARG names, agglomeration is correct (lowest level is ARG cluster) but col names are not correct - I will rename the column names to the AMR classes for presentation in the heatmap
colnames(ARGcluster.aggl.DESeq2.exp.quart@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

#Inspect the new ps object
ARGcluster.aggl.DESeq2.exp.quart
ARGcluster.aggl.DESeq2.exp.quart@otu_table # clusters labelled correctly 
ARGcluster.aggl.DESeq2.exp.quart@sam_data # exposure proxy sample data added 
ARGcluster.aggl.DESeq2.exp.quart@sam_data$MinDistAnyFarm.NEG # exposure proxy data is in quartiles 

# Now ready to run the deseq2 analysis using the phyloseq_to_deseq2 function
# Convert phyloseq data to DESeq2 dataset object - start with endotoxin concentration
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp.quart,~DISP_EUinPM10_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart <- DESeq(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart <- results(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart
#summary of differential gene expression
summary(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart)
head(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart.ordered <- deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart[order(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart$pvalue),]
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.ordered # No ARGs have padj <0.1 

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have padj <0.1


#PM10 concentration 
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp.quart,~DISP_PM10CONC_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart <- DESeq(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart <- results(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart
#summary of differential gene expression
summary(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart)
head(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart.ordered <- deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart[order(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart$pvalue),]
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.quart.ordered # No ARGs have padj <0.1 

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.quart$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have padj <0.1


# convert exposure proxies into top and bottom halves 
#DISP_EUinPM10_AnnualAv_WP99.5
ARGcluster.aggl.DESeq2.exp.half <- ARGcluster.aggl.DESeq2.exp

colnames(ARGcluster.aggl.DESeq2.exp.half@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")

ARGcluster.aggl.DESeq2.exp.half@sam_data$DISP_EUinPM10_AnnualAv_WP99.5<- cut(ARGcluster.aggl.DESeq2.exp.half@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, breaks=2, labels =c("Top half (DISP_EUinPM10_AnnualAv_WP99.5)", "Bottom half (DISP_EUinPM10_AnnualAv_WP99.5)"))
#DISP_PM10CONC_AnnualAv_WP99.5
ARGcluster.aggl.DESeq2.exp.half@sam_data$DISP_PM10CONC_AnnualAv_WP99.5<- cut(ARGcluster.aggl.DESeq2.exp.half@sam_data$DISP_PM10CONC_AnnualAv_WP99.5, breaks=2, labels =c("Top half (DISP_PM10CONC_AnnualAv_WP99.5)", "Bottom half (DISP_PM10CONC_AnnualAv_WP99.5)"))

#Re-run Deseq2 with 'halved' data rather than in quartiles 
# Convert phyloseq data to DESeq2 dataset object - start with endotoxin concentration
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.half <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp.half,~DISP_EUinPM10_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.half <- DESeq(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.half)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half <- results(deseq2.exp.DISP_EUinPM10_AnnualAv_WP99.5.half)
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half
#summary of differential gene expression
summary(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half)
head(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half.ordered <- deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half[order(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half$pvalue),]
deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half.ordered # No ARGs have padj <0.1 

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_EUinPM10_AnnualAv_WP99.5.half.ordered$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have padj <0.1


#PM10 concentration 
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.half <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp.half,~DISP_PM10CONC_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.half <- DESeq(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.half)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half <- results(deseq2.exp.DISP_PM10CONC_AnnualAv_WP99.5.half)
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half
#summary of differential gene expression
summary(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half)
head(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half)

# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half)$description

# Order the results table by the smallest p value:
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half.ordered <- deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half[order(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half$pvalue),]
deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half.ordered # No ARGs have padj <0.1 

# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.DISP_PM10CONC_AnnualAv_WP99.5.half$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have padj <0.1

# Now I will look at some of the other exposure variables 
#nhorsesWghtDist.1000m.sum
ARGcluster.aggl.DESeq2.exp.half <- ARGcluster.aggl.DESeq2.exp
colnames(ARGcluster.aggl.DESeq2.exp.half@otu_table) <- c("aac(3)-II_clust","aac(3)-Iva","aac(6')-aph(2'')_clust","aadA_ant(3'')-Ia_clust","aadA_clust1","aadD","ant(6)-Ia_clust2","aph(3'')-Ib","aph(3')-Ia_aph(3')-Ic","aph(3')-Ib","aph(3')-III","aph(4)-Ia","aph(6)-Id","str","blaACT_clust","blaACT_CMG_MIR_clust","blaBRO","blaCARB_clust2","blaCTX-M_clust1","blaOXA-22","blaOXA-395_clust","blaOXA-60_clust","blaOXA-85","blaOXA_clust19","blaOXA_clust3","blaOXA_clust8","blaOXA_clust9","blaOXY_clust1","blaSPU-1","blaTEM_clust","blaZ_clust","cfxA_clust","mecA_clust","mecA1","penA","fusB","lsa(A)","lsa(C)","erm(A)_2_AF002716","erm(B)_clust","erm(C)_clust","erm(F)_clust","erm(T)_4_AJ488494","erm(X)_clust","lnu(C)","mdf(A)","mef(A)-3","mef(A)_clust","mph(A)","mph(C)","msr(A)","msr(D)","msr(E)","vga(A)_clust","cat(pC194)","cat_2","catA1","catQ","catS","cml_clust","cmx","cfr(C)","sul1","sul2","tet(32)","tet(33)","tet(37)","tet(39)","tet(40)","tet(44)","tet(A)","tet(B)","tet(C)","tet(G)","tet(K)","tet(L)_clust1","tet(M)","tet(O)","tet(O/32/O)","tet(O/W/32/O/W/O)","tet(O/W/O)-1","tet(Q)","tet(T)","tet(W)","dfrA15_clust")
ARGcluster.aggl.DESeq2.exp.half@sam_data$nhorsesWghtDist.1000m.sum<- cut(ARGcluster.aggl.DESeq2.exp.half@sam_data$nhorsesWghtDist.1000m.sum, breaks=2, labels =c("Top half (nhorsesWghtDist.1000m.sum)", "Bottom half (nhorsesWghtDist.1000m.sum)"))
deseq2.exp.nhorsesWghtDist.1000m.sum.half <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.exp.half,~nhorsesWghtDist.1000m.sum)
# RUN  DESeq function 
deseq2.exp.nhorsesWghtDist.1000m.sum.half <- DESeq(deseq2.exp.nhorsesWghtDist.1000m.sum.half)
deseq2.results.exp.nhorsesWghtDist.1000m.sum.half <- results(deseq2.exp.nhorsesWghtDist.1000m.sum.half)
deseq2.results.exp.nhorsesWghtDist.1000m.sum.half
#summary of differential gene expression
summary(deseq2.results.exp.nhorsesWghtDist.1000m.sum.half)
head(deseq2.results.exp.nhorsesWghtDist.1000m.sum.half)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.nhorsesWghtDist.1000m.sum.half)$description
# Order the results table by the smallest p value:
deseq2.results.exp.nhorsesWghtDist.1000m.sum.half.ordered <- deseq2.results.exp.nhorsesWghtDist.1000m.sum.half[order(deseq2.results.exp.nhorsesWghtDist.1000m.sum.half$pvalue),]
deseq2.results.exp.nhorsesWghtDist.1000m.sum.half.ordered 
# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.nhorsesWghtDist.1000m.sum.half.ordered$padj < 0.1, na.rm=TRUE) # 4 ARG clusters have padj <0.1

barchart(ARGcluster.aggl.DESeq2.exp.half@sam_data$nhorsesWghtDist.1000m.sum)
```
<!-- ##### DESeq plots for poster -->
<!-- ```{r} -->
<!-- # Creating plots for poster presentation Jan 2022  -->
<!-- # I will use dispersion modelled endotoxin as explanatory variable and tet(w) as outcome as this came out as signifcantly increased in deseq2 analysis with disp modelled endotoxin and PM10 -->
<!-- endotox.dispmodel<- ARGcluster.aggl.DESeq2.exp@sam_data$DISP_EUinPM10_AnnualAv_WP99.5 # exposure proxy data is continuous  -->
<!-- ARGcluster.aggl.DESeq2.exp@otu_table$tet(W) -->
<!-- otu_table(ARGcluster.aggl.DESeq2.exp)[,"tet(W)"] -->
<!-- endotox.dispmodel.df <- as.data.frame(ARGcluster.aggl.DESeq2.exp@sam_data$DISP_EUinPM10_AnnualAv_WP99.5) -->
<!-- tetW.df <-as.data.frame(otu_table(ARGcluster.aggl.DESeq2.exp)[,"tet(W)"]) -->
<!-- rownames(tetW.df) <- NULL -->

<!-- combined.df <- data.frame(endotox.dispmodel.df, tetW.df) # Put them in the same data.frame -->
<!-- plot(combined.df$ARGcluster.aggl.DESeq2.exp.sam_data.DISP_EUinPM10_AnnualAv_WP99.5, combined.df$tet.W.,  main="Scatterplot dispersion modelled endotoxin vs tet(W)", xlab="Disp. modelled endotoxin concentration", ylab="tet(W) concentratio", pch=19) -->
<!-- # Looks like outliers are probably influencing this result  -->

<!-- # Find another exposure proxy to present in poster -->
<!-- # Try quantiles of endotoxin exposure -->
<!-- ARGcluster.aggl.DESeq2.exp.quart@sam_data$DISP_EUinPM10_AnnualAv_WP99.5 # exposure proxy data is in quartiles  -->
<!-- hist(sample_data(ARGcluster.aggl.DESeq2.exp.quart)[,"DISP_EUinPM10_AnnualAv_WP99.5"])  -->

<!-- plot(ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, tetW.df) -->
<!-- combined2.df <- data.frame(ARGcluster.aggl.noblanks.exp.quant@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, tetW.df)  -->
<!-- plot(combined2.df$ARGcluster.aggl.noblanks.exp.quant.sam_data.DISP_EUinPM10_AnnualAv_WP99.5, combined2.df$tet.W.) -->


<!-- # Split endotoxin exposure in half instead of in quantiles -->
<!-- Endotoxin.splithalf <- ARGcluster.aggl.noblanks.exp -->
<!-- Endotoxin.splithalf@sam_data$DISP_EUinPM10_AnnualAv_WP99.5 <- quantcut(Endotoxin.splithalf@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, q=2, labels =c("Low", "High")) -->
<!-- combined3.df <- data.frame(Endotoxin.splithalf@sam_data$DISP_EUinPM10_AnnualAv_WP99.5, tetW.df)  -->
<!-- boxplot.endotox <- plot(combined3.df$Endotoxin.splithalf.sam_data.DISP_EUinPM10_AnnualAv_WP99.5, combined3.df$tet.W., main="Comparison of tet(W) gene counts between high and low endotoxin exposed individuals", xlab="Endotoxin exposure level", ylab="tet(W) gene counts", pch=20, cex.main= 2, cex.lab= 2, cex.axis= 2) -->

<!-- ggplot(combined3.df) -->

<!-- geom_boxplot(outlier.colour="black", outlier.shape=16, -->
<!--              outlier.size=2, notch=FALSE) -->

<!-- p <- ggplot(combined3.df, aes(x="Endotoxin.splithalf.sam_data.DISP_EUinPM10_AnnualAv_WP99.5", y="tet.W.")) +  -->
<!--   geom_boxplot() -->

<!-- ggplot(aes(x=combined3.df$Endotoxin.splithalf.sam_data.DISP_EUinPM10_AnnualAv_WP99.5, y=combined3.df$tet.W., fill=combined3.df$Endotoxin.splithalf.sam_data.DISP_EUinPM10_AnnualAv_WP99.5)) +  -->
<!--   geom_boxplot(width=0.5,lwd=1)+ -->
<!--   labs(subtitle="Filling Boxplot with Colors by a Variable") -->

<!-- boxplot(combined3.df$Endotoxin.splithalf.sam_data.DISP_EUinPM10_AnnualAv_WP99.5, combined3.df$tet.W., main="Comparison of tet(W) gene counts between high and low endotoxin exposed individuals", xlab="Endotoxin exposure level", ylab="tet(W) gene counts", pch=20) -->

<!-- ``` -->

#### DESeq2 (filtered - 0.1% relative abundance, 15% prevalent)
```{r}
# copy over the previously agglomerated and filtered ps object from my above DESeq2 analysis
ARGcluster.aggl.DESeq2.filtered.exp <- ARGcluster.aggl.DESeq2.filtered.relative
ARGcluster.aggl.DESeq2.filtered.exp
tax_table(ARGcluster.aggl.DESeq2.filtered.exp)
# 30 ARG Clusters 
ARGcluster.aggl.DESeq2.filtered.exp@sam_data # But the sample data in this ps object does not include the exposure proxy data so I need to add this in
sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(ARGcluster.aggl.DESeq2.filtered.exp) <- sam.data.new
ARGcluster.aggl.DESeq2.filtered.exp@sam_data

# I will start with continuous exposure covariates in the DESeq2 analysis as a constant fold change is possible for each unit of increase of the variables 
# Now ready to run the deseq2 analysis using the phyloseq_to_deseq2 function
# Start with assessing endotoxin concentration effects
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.filtered.endotoxin <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.filtered.exp,~DISP_EUinPM10_AnnualAv_WP99.5)

# RUN  DESeq function 
deseq2.exp.filtered.endotoxin <- DESeq(deseq2.exp.filtered.endotoxin)
deseq2.results.exp.filtered.endotoxin<- results(deseq2.exp.filtered.endotoxin)
deseq2.results.exp.filtered.endotoxin
#summary of differential gene expression
summary(deseq2.results.exp.filtered.endotoxin)
head(deseq2.results.exp.filtered.endotoxin)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.filtered.endotoxin)$description
# Order the results table by the smallest p value:
deseq2.results.exp.filtered.endotoxin.ordered <- deseq2.results.exp.filtered.endotoxin[order(deseq2.results.exp.filtered.endotoxin$pvalue),]
deseq2.results.exp.filtered.endotoxin.ordered 
# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.filtered.endotoxin.ordered$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have p-value <0.1

# Now look at PM10 concentration effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.filtered.PM10 <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.filtered.exp,~DISP_PM10CONC_AnnualAv_WP99.5)
# RUN  DESeq function 
deseq2.exp.filtered.PM10 <- DESeq(deseq2.exp.filtered.PM10)
deseq2.results.exp.filtered.PM10 <- results(deseq2.exp.filtered.PM10)
deseq2.results.exp.filtered.PM10
#summary of differential gene expression
summary(deseq2.results.exp.filtered.PM10)
head(deseq2.results.exp.filtered.PM10)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.filtered.PM10)$description
# Order the results table by the smallest p value:
deseq2.results.exp.filtered.PM10.ordered <- deseq2.results.exp.filtered.PM10[order(deseq2.results.exp.filtered.PM10$pvalue),]
deseq2.results.exp.filtered.PM10.ordered #erm(T)_4_AJ488494 has the greatest difference for each fold change in endotoxin concentration
# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.filtered.PM10.ordered$padj < 0.1, na.rm=TRUE) # 0 ARG clusters have p-value <0.1


# Now look at nhorsesWghtDist.3000m.sum effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.filtered.nhorsesWghtDist.3000m.sum.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.filtered.exp,~nhorsesWghtDist.3000m.sum)
# RUN  DESeq function 
deseq2.exp.filtered.nhorsesWghtDist.3000m.sum.cont <- DESeq(deseq2.exp.filtered.nhorsesWghtDist.3000m.sum.cont)
deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont <- results(deseq2.exp.filtered.nhorsesWghtDist.3000m.sum.cont)
deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont
#summary of differential gene expression
summary(deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont)
head(deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont)$description
# Order the results table by the smallest p value:
deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont.ordered <- deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont[order(deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont$pvalue),]
deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont.ordered
# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.filtered.nhorsesWghtDist.3000m.sum.cont$padj < 0.1, na.rm=TRUE) # 3 ARG clusters have p-value <0.1

# Now look at ngoatsWghtDist.3000m.sum effects on differential abundance
# Convert phyloseq data to DESeq2 dataset object
deseq2.exp.filtered.ngoatsWghtDist.3000m.sum.cont <- phyloseq_to_deseq2(ARGcluster.aggl.DESeq2.filtered.exp,~ngoatsWghtDist.3000m.sum)
# RUN  DESeq function 
deseq2.exp.filtered.ngoatsWghtDist.3000m.sum.cont <- DESeq(deseq2.exp.filtered.ngoatsWghtDist.3000m.sum.cont)
deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont <- results(deseq2.exp.filtered.ngoatsWghtDist.3000m.sum.cont)
deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont
#summary of differential gene expression
summary(deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont)
head(deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont)
# Information about which variables and tests were used for the results is given by this function
mcols(deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont)$description
# Order the results table by the smallest p value:
deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont.ordered <- deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont[order(deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont$pvalue),]
deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont.ordered 
# How many adjusted p-values are < 0.1? 
sum(deseq2.results.exp.filtered.ngoatsWghtDist.3000m.sum.cont$padj < 0.1, na.rm=TRUE) # 3 ARG clusters have p-value <0.1
```
#### ALDEx2
```{r}
BiocManager::install("ALDEx2")
library(ALDEx2)

# Use the previous ps object created above for the deseq2 analysis
ARGcluster.aggl.DESeq2.exp
ARGcluster.aggl.DESeq2.exp@otu_table # clusters labelled correctly 
ARGcluster.aggl.DESeq2.exp@sam_data # exposure proxy sample data added 
ARGcluster.aggl.DESeq2.exp@sam_data$DISP_EUinPM10_AnnualAv_WP99.5 # exposure proxy data is continuous 
str(ARGcluster.aggl.DESeq2.exp)

# create otu table for aldex2 analysis - this needs to be "A non-negative, integer-only data.frame or matrix with unique names for all rows and columns. Rows should contain genes and columns should contain sequencing read counts (i.e., sample vectors). Rows with 0 reads in each sample are deleted prior to analysis
ALDEx2.exp.otutable.trans <- t(otu_table(ARGcluster.aggl.DESeq2.exp))
ALDEx2.exp.otutable.trans.bill.round <- round(ALDEx2.exp.otutable.trans * 1000000000,0) # I multiply by billion as aldex2 requires that reads are all integers 
ALDEx2.exp.otutable.df <- data.frame(ALDEx2.exp.otutable.trans.bill.round)
# Then we set the comparison groups. This must be a vector of conditions in the same order as the samples in the input counts table.
# I firstly test dispersion modelled endotoxin as the comparison variable: 
ALDEx2.exp.DISPendotoxin <- sample_data(ARGcluster.aggl.DESeq2.exp)$DISP_EUinPM10_AnnualAv_WP99.5
# Instead of using the simple aldex() function, I need to use aldex.corr as I am using a continuous variable (e.g. endotoxin concentration to begin with)
# Firstly create the aldex.clr object
ALDEx2.exp.clr <- aldex.clr(ALDEx2.exp.otutable.df, mc.samples = 128, verbose = FALSE, useMC=FALSE)
# Now run the correlation assessment 
# aldex.corr calculates the expected values for the correlation between each feature and a continuous variable, using data returned returned by aldex.clr and a vector of the continuous variable.It returns results of Pearson, Spearman and Kendall tests.
aldex2_endotoxin_continuous<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.DISPendotoxin)
aldex2_endotoxin_continuous
# aldex.corr calculates the expected values for the correlation between each feature and a continuous variable (dispersion modelled endotoxin in this case), using data returned returned by aldex.clr and a vector of the continuous variable. Returns results of Pearson, Spearman and Kendall tests.
# I will look at the BH corrected pearson correlation coefficient p-value (this is the column named: "pearson.eBH" (expected Benjamini-Hochberg corrected P value of the Pearson Product moment value for each feature)
aldex2_endotoxin_continuous_reordered <- aldex2_endotoxin_continuous[order(aldex2_endotoxin_continuous$pearson.eBH),]
# check the ARGs that were previously identified as differentially abundant to see whether there are significant correlations identified by ALDEx2
count(aldex2_endotoxin_continuous_reordered$pearson.eBH < 0.1) 
# 3 ARG clusters have an expected Benjamini-Hochberg corrected P-value of the Pearson Product moment value - these are the following ARG clusters: tet(K), blaSPU-1, msr(E)

# Test for correlation with dispersion modelled PM10 now
ALDEx2.exp.DISPPM10 <- sample_data(ARGcluster.aggl.DESeq2.exp)$DISP_PM10CONC_AnnualAv_WP99.5
aldex2_PM10_continuous<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.DISPPM10)
aldex2_PM10_continuous
count(aldex2_PM10_continuous$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to dispersion modelled PM10
aldex2_PM10_continuous_reordered <- aldex2_PM10_continuous[order(aldex2_PM10_continuous$pearson.eBH),]
aldex2_PM10_continuous_reordered

# Test for correlation with Number of horses weighted to distance in a 3000m buffer
ALDEx2.exp.nhorsesWghtDist.3000m.sum <- ARGcluster.aggl.DESeq2.exp@sam_data$nhorsesWghtDist.3000m.sum
aldex2_nhorsesWghtDist.3000m.sum<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.nhorsesWghtDist.3000m.sum)
aldex2_nhorsesWghtDist.3000m.sum
count(aldex2_nhorsesWghtDist.3000m.sum$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to nhorsesWghtDist.3000m.sum

# Test for correlation with Number of goats weighted to distance in a 3000m buffer
ALDEx2.exp.ngoatsWghtDist.3000m.sum <- ARGcluster.aggl.DESeq2.exp@sam_data$ngoatsWghtDist.3000m.sum
aldex2_ngoatsWghtDist.3000m.sum<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.ngoatsWghtDist.3000m.sum)
aldex2_ngoatsWghtDist.3000m.sum
count(aldex2_ngoatsWghtDist.3000m.sum$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to nhorsesWghtDist.3000m.sum
# list of all exposure proxy variables 
names( ARGcluster.aggl.DESeq2.exp@sam_data)

# Test for correlation with MinDistAnyFarm.NEG
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) #No ARG clusters are correlated

# Test for correlation with MinDistAnyFarm.INV
ALDEx2.exp.MinDistAnyFarm.INV <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.INV
aldex2_MinDistAnyFarm.INV<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.INV)
aldex2_MinDistAnyFarm.INV
count(aldex2_MinDistAnyFarm.INV$pearson.eBH < 0.1) #No ARG clusters are correlated

# AllFarm.3000m
ALDEx2.exp.AllFarm.3000m <- ARGcluster.aggl.DESeq2.exp@sam_data$AllFarm.3000m
aldex2_AllFarm.3000m<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.AllFarm.3000m)
aldex2_AllFarm.3000m
count(aldex2_AllFarm.3000m$pearson.eBH < 0.1) #No ARG clusters are correlated

#AllFarm.1000m
ALDEx2.exp.AllFarm.1000m <- ARGcluster.aggl.DESeq2.exp@sam_data$AllFarm.1000m
aldex2_AllFarm.1000m<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.AllFarm.1000m)
aldex2_AllFarm.1000m
count(aldex2_AllFarm.1000m$pearson.eBH < 0.1) 

# AllFarm.500m
ALDEx2.exp.AllFarm.500m <- ARGcluster.aggl.DESeq2.exp@sam_data$AllFarm.500m
aldex2_AllFarm.500m<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.AllFarm.500m)
aldex2_AllFarm.500m
count(aldex2_AllFarm.500m$pearson.eBH < 0.1) 

# AllFarm.250m
ALDEx2.exp.AllFarm.250m <- ARGcluster.aggl.DESeq2.exp@sam_data$AllFarm.250m
aldex2_AllFarm.250m<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.AllFarm.250m)
aldex2_AllFarm.250m
count(aldex2_AllFarm.250m$pearson.eBH < 0.1) 
# npigsWghtDist.1000m.sum
ALDEx2.exp.npigsWghtDist.1000m.sum <- ARGcluster.aggl.DESeq2.exp@sam_data$npigsWghtDist.1000m.sum
aldex2_npigsWghtDist.1000m.sum<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.npigsWghtDist.1000m.sum)
aldex2_npigsWghtDist.1000m.sum
count(aldex2_npigsWghtDist.1000m.sum$pearson.eBH < 0.1) 
# npoultryWghtDist.1000m.sum
ALDEx2.exp.npoultryWghtDist.1000m.sum <- ARGcluster.aggl.DESeq2.exp@sam_data$npoultryWghtDist.1000m.sum
aldex2_npoultryWghtDist.1000m.sum<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.npoultryWghtDist.1000m.sum)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# ncowsWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nhorsesWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# ngoatsWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nsheepWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nfuranimsWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# npigsWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# npoultryWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# ncowsWghtDist.3000m.sum    
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nhorsesWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# ngoatsWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nsheepWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nfuranimsWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nAnyFarmWghtDist.1000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# nAnyFarmWghtDist.3000m.sum
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# DISP_EUinPM10_AnnualAv_WP99.5
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 
# DISP_PM10CONC_AnnualAv_WP99.5
ALDEx2.exp.MinDistAnyFarm.NEG <- ARGcluster.aggl.DESeq2.exp@sam_data$MinDistAnyFarm.NEG
aldex2_MinDistAnyFarm.NEG<- aldex.corr(ALDEx2.exp.clr,ALDEx2.exp.MinDistAnyFarm.NEG)
aldex2_MinDistAnyFarm.NEG
count(aldex2_MinDistAnyFarm.NEG$pearson.eBH < 0.1) 

```

#### ALDEx2 (filtered - 0.1% relative abundance, 15% prevalent)
```{r}
library(ALDEx2)

# copy over the previously agglomerated and filtered ps object from my above DESeq2 analysis
ARGcluster.aggl.ALDEx2.filtered.exp <- ARGcluster.aggl.DESeq2.filtered.relative
ARGcluster.aggl.ALDEx2.filtered.exp
tax_table(ARGcluster.aggl.ALDEx2.filtered.exp)
# 30 ARG Clusters 
ARGcluster.aggl.ALDEx2.filtered.exp@sam_data # But the sample data in this ps object does not include the exposure proxy data so I need to add this in
sam.data.new <- as.data.frame(sample_data(livestock.exp.df.noblanks.df))
sample_data(ARGcluster.aggl.ALDEx2.filtered.exp) <- sam.data.new
ARGcluster.aggl.ALDEx2.filtered.exp@sam_data

# create otu table for aldex2 analysis - this needs to be "A non-negative, integer-only data.frame or matrix with unique names for all rows and columns. Rows should contain genes and columns should contain sequencing read counts (i.e., sample vectors). Rows with 0 reads in each sample are deleted prior to analysis
ALDEx2.filtered.exp.otutable.trans <- t(otu_table(ARGcluster.aggl.ALDEx2.filtered.exp))
ALDEx2.filtered.exp.otutable.trans.bill.round <- round(ALDEx2.filtered.exp.otutable.trans * 1000000000,0) # I multiply by billion as aldex2 requires that reads are all integers 
otutab.for.ALDEx2.exp.filtered <- data.frame(ALDEx2.filtered.exp.otutable.trans.bill.round)

# Firstly create the aldex.clr object
ALDEx2.exp.filtered.clr <- aldex.clr(otutab.for.ALDEx2.exp.filtered, mc.samples = 128, verbose = FALSE, useMC=FALSE)
# Then we set the comparison groups. This must be a vector of conditions in the same order as the samples in the input counts table.
ALDEx2.exp.filtered.DISPendotoxin <- sample_data(ARGcluster.aggl.ALDEx2.filtered.exp)$DISP_EUinPM10_AnnualAv_WP99.5
# Instead of using the simple aldex() function, I need to use aldex.corr as I am using a continuous variable (e.g. endotoxin concentration to begin with)
# Now run the correlation assessment 
# aldex.corr calculates the expected values for the correlation between each feature and a continuous variable, using data returned returned by aldex.clr and a vector of the continuous variable.It returns results of Pearson, Spearman and Kendall tests.
aldex2.filtered.endotoxin<- aldex.corr(ALDEx2.exp.filtered.clr,ALDEx2.exp.filtered.DISPendotoxin)
aldex2.filtered.endotoxin
# aldex.corr calculates the expected values for the correlation between each feature and a continuous variable (dispersion modelled endotoxin in this case), using data returned returned by aldex.clr and a vector of the continuous variable. Returns results of Pearson, Spearman and Kendall tests.
# I will look at the BH corrected pearson correlation coefficient p-value (this is the column named: "pearson.eBH" (expected Benjamini-Hochberg corrected P value of the Pearson Product moment value for each feature)
aldex2.filtered.endotoxin_reordered <- aldex2.filtered.endotoxin[order(aldex2.filtered.endotoxin$pearson.eBH),]
# check the ARGs that were previously identified as differentially abundant to see whether there are significant correlations identified by ALDEx2
count(aldex2.filtered.endotoxin$pearson.eBH < 0.1) 
# 1 ARG cluster has an expected Benjamini-Hochberg corrected P-value of the Pearson Product moment value < 0.1: blaSPU-1

# Test for correlation with dispersion modelled PM10 now
ALDEx2.exp.DISPPM10 <- sample_data(ARGcluster.aggl.ALDEx2.filtered.exp)$DISP_PM10CONC_AnnualAv_WP99.5
aldex2.filtered.PM10<- aldex.corr(ALDEx2.exp.filtered.clr,ALDEx2.exp.DISPPM10)
aldex2.filtered.PM10
count(aldex2.filtered.PM10$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to dispersion modelled PM10

# Test for correlation with Number of horses weighted to distance in a 3000m buffer
ALDEx2.exp.nhorsesWghtDist.3000m.sum <- ARGcluster.aggl.ALDEx2.filtered.exp@sam_data$nhorsesWghtDist.3000m.sum
aldex2.filtered.nhorsesWghtDist.3000m.sum<- aldex.corr(ALDEx2.exp.filtered.clr,ALDEx2.exp.nhorsesWghtDist.3000m.sum)
aldex2.filtered.nhorsesWghtDist.3000m.sum
count(aldex2.filtered.nhorsesWghtDist.3000m.sum$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to nhorsesWghtDist.3000m.sum

# Test for correlation with Number of goats weighted to distance in a 3000m buffer
ALDEx2.exp.ngoatsWghtDist.3000m.sum <- ARGcluster.aggl.ALDEx2.filtered.exp@sam_data$ngoatsWghtDist.3000m.sum
aldex2.filtered.ngoatsWghtDist.3000m.sum<- aldex.corr(ALDEx2.exp.filtered.clr,ALDEx2.exp.ngoatsWghtDist.3000m.sum)
aldex2.filtered.ngoatsWghtDist.3000m.sum
count(aldex2.filtered.ngoatsWghtDist.3000m.sum$pearson.eBH < 0.1) # No ARG clusters are correlated significantly to nhorsesWghtDist.3000m.sum

# Could try with other exposure proxies but have chosen these based on previous beta-diversity results and logic.
```



